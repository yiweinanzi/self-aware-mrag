#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Self-Aware Multimodal RAG Pipeline - Qwen3-VLç‰ˆæœ¬

âœ… ç»Ÿä¸€ä½¿ç”¨Qwen3-VL-8B-Instructï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”

ä¸LLaVAç‰ˆæœ¬çš„åŒºåˆ«ï¼š
1. æ¨¡å‹ï¼šQwen3-VL-8B-Instructï¼ˆ2024ï¼‰vs LLaVA-1.5ï¼ˆ2023ï¼‰
2. å¤šå›¾åƒæ”¯æŒï¼šæœ€å¤š20å¼  vs å•å›¾åƒ
3. é«˜åˆ†è¾¨ç‡ï¼šæ”¯æŒ vs æœ‰é™
4. æŒ‡ä»¤è·Ÿéšï¼šæ›´å¼º vs ä¸€èˆ¬

æ ¸å¿ƒåˆ›æ–°ä¿æŒä¸å˜ï¼š
- è·¨æ¨¡æ€ä¸ç¡®å®šæ€§ä¼°è®¡
- ä½ç½®æ„ŸçŸ¥èåˆ
- ç»†ç²’åº¦å½’å› 
- å¤šæ¨¡æ€è¾“å‡ºï¼ˆå¯é€‰ï¼‰
"""

import torch
import warnings
from typing import List, Dict, Any, Optional, Tuple
import numpy as np

class SelfAwarePipelineQwen3VL:
    """
    Self-Aware Multimodal RAG Pipelineï¼ˆQwen3-VLç‰ˆæœ¬ï¼‰
    
    âœ… P0ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨Qwen3-VLç¡®ä¿å…¬å¹³å¯¹æ¯”
    
    æ ¸å¿ƒæµç¨‹ï¼š
    1. ä¸ç¡®å®šæ€§ä¼°è®¡ â†’ å†³å®šæ˜¯å¦æ£€ç´¢
    2. è‡ªé€‚åº”æ£€ç´¢ â†’ ä½ç½®æ„ŸçŸ¥èåˆ
    3. ç”Ÿæˆç­”æ¡ˆï¼ˆQwen3-VLï¼‰
    4. ç»†ç²’åº¦å½’å› 
    5. å¤šæ¨¡æ€è¾“å‡ºå¢å¼ºï¼ˆå¯é€‰ï¼‰
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    ```python
    from flashrag.modules.qwen3_vl import create_qwen3_vl_wrapper
    
    qwen3_vl = create_qwen3_vl_wrapper()
    
    pipeline = SelfAwarePipelineQwen3VL(
        qwen3_vl_wrapper=qwen3_vl,
        retriever=retriever,
        config={
            'uncertainty_threshold': 0.35,
            'use_position_fusion': True,
            'use_attribution': True
        }
    )
    
    results = pipeline.run(dataset)
    ```
    """
    
    def __init__(self, qwen3_vl_wrapper, retriever, config=None):
        """
        åˆå§‹åŒ–Pipelineï¼ˆQwen3-VLç‰ˆæœ¬ï¼‰
        
        Args:
            qwen3_vl_wrapper: Qwen3-VLæ¨¡å‹å°è£…å™¨
            retriever: æ£€ç´¢å™¨
            config: é…ç½®å­—å…¸
        """
        self.qwen3_vl = qwen3_vl_wrapper  # âœ… ä½¿ç”¨Qwen3-VL
        self.retriever = retriever
        self.config = config or {}
        
        # é…ç½®å‚æ•°
        # ä¼˜åŒ–ï¼šé™ä½é˜ˆå€¼ï¼Œè®©æ›´å¤šæ ·æœ¬è§¦å‘æ£€ç´¢
        self.uncertainty_threshold = self.config.get('uncertainty_threshold', 0.30)  # 0.35 â†’ 0.30
        self.top_k = self.config.get('retrieval_topk', 5)
        self.use_position_fusion = self.config.get('use_position_fusion', True)
        self.use_attribution = self.config.get('use_attribution', True)
        self.use_multimodal_output = self.config.get('enable_multimodal_output', False)
        
        # Qwen3-VLç‰¹å®šé…ç½®
        self.max_images = min(self.config.get('max_images', 20), 20)  # æœ€å¤š20å¼ 
        self.use_thinking = self.config.get('thinking', False)  # P0-2: ç¡®ä¿thinking=false
        
        # åˆå§‹åŒ–æ¨¡å—
        self._init_modules()
        
        print("âœ… SelfAwarePipelineQwen3VLåˆå§‹åŒ–å®Œæˆ")
        print(f"  - æ¨¡å‹: Qwen3-VL-8B-Instruct")
        print(f"  - Uncertainty threshold (Ï„): {self.uncertainty_threshold:.3f}")
        print(f"  - Max images: {self.max_images}")
        print(f"  - Thinking mode: {self.use_thinking} (æ¨èFalse)")
        print(f"  - Position fusion: {self.use_position_fusion}")
        print(f"  - Attribution: {self.use_attribution}")
        print(f"  - Multimodal output: {self.use_multimodal_output}")
    
    def should_retrieve(self, u: float, tau: Optional[float] = None) -> bool:
        """
        âœ… P0-6: é˜ˆå€¼æ¥å£åŒ– - æ£€ç´¢å†³ç­–å‡½æ•°
        
        Args:
            u: ä¸ç¡®å®šæ€§åˆ†æ•°
            tau: ä¸ç¡®å®šæ€§é˜ˆå€¼ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            bool: Trueè¡¨ç¤ºéœ€è¦æ£€ç´¢
        """
        threshold = tau if tau is not None else self.uncertainty_threshold
        return u > threshold
    
    def _init_modules(self):
        """åˆå§‹åŒ–å„ä¸ªæ¨¡å—"""
        from flashrag.modules.position_aware_fusion import PositionAwareCrossModalFusion
        from flashrag.modules.attribution import FineGrainedMultimodalAttribution
        from flashrag.modules.modality_selector import ModalitySelector
        from flashrag.modules.query_reformulation import QueryReformulator
        
        # 1. ä¸ç¡®å®šæ€§ä¼°è®¡å™¨ - æ”¯æŒé€‰æ‹©åŸå§‹ç‰ˆæˆ–æ”¹è¿›ç‰ˆ
        use_improved = self.config.get('use_improved_estimator', False)
        
        if use_improved:
            print("  âœ… ä½¿ç”¨æ”¹è¿›ç‰ˆä¸ç¡®å®šæ€§ä¼°è®¡å™¨ (ImprovedUncertaintyEstimator)")
            from flashrag.modules.uncertainty_estimator_improved import ImprovedUncertaintyEstimator
            self.uncertainty_estimator = ImprovedUncertaintyEstimator(
                config={
                    'clip_model_path': self.config.get('clip_model_path', 
                        '/root/autodl-tmp/models/clip-vit-large-patch14-336'),
                    'text_weight': 0.5,
                    'visual_weight': 0.3,
                    'alignment_weight': 0.2
                }
            )
        else:
            print("  â„¹ï¸  ä½¿ç”¨åŸå§‹ä¸ç¡®å®šæ€§ä¼°è®¡å™¨ (CrossModalUncertaintyEstimator)")
            from flashrag.modules.uncertainty_estimator import CrossModalUncertaintyEstimator
            self.uncertainty_estimator = CrossModalUncertaintyEstimator(
                mllm_model=None,
                config={
                    'eigen_threshold': -6.0,
                    'use_clip_for_alignment': True,
                    'clip_model_path': self.config.get('clip_model_path', 
                        '/root/autodl-tmp/models/clip-vit-large-patch14-336'),
                    'text_weight': 0.4,
                    'visual_weight': 0.3,
                    'alignment_weight': 0.3
                }
            )
        
        # 2. æ¨¡æ€é€‰æ‹©å™¨
        self.modality_selector = ModalitySelector()
        
        # 3. æŸ¥è¯¢é‡æ„å™¨
        self.query_reformulator = QueryReformulator()
        
        # 4. ä½ç½®æ„ŸçŸ¥èåˆ
        if self.use_position_fusion:
            self.position_aware_fusion = PositionAwareCrossModalFusion(
                d_model=768, num_heads=12, device='cpu'
            )
        
        # 5. å½’å› æ¨¡å—
        if self.use_attribution:
            self.attribution_module = FineGrainedMultimodalAttribution(
                mllm_model=None
            )
        
        # 6. å¤šæ¨¡æ€è¾“å‡ºï¼ˆå¯é€‰ï¼‰
        if self.use_multimodal_output:
            try:
                from flashrag.modules.multimodal_output import MultimodalOutputComposition
                self.multimodal_output = MultimodalOutputComposition()
                print("  âš ï¸  å¤šæ¨¡æ€è¾“å‡ºå·²å¯ç”¨ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰")
            except Exception as e:
                warnings.warn(f"Multimodal outputæ¨¡å—åŠ è½½å¤±è´¥: {e}")
                self.use_multimodal_output = False
        else:
            self.multimodal_output = None
    
    # =========================================================================
    # æ ¸å¿ƒPipelineæµç¨‹
    # =========================================================================
    
    def run_single(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ ·æœ¬ï¼ˆQwen3-VLç‰ˆæœ¬ï¼‰
        
        Args:
            sample: æ ·æœ¬å­—å…¸
        
        Returns:
            Dict: ç»“æœå­—å…¸
        """
        question = sample['question']
        image = sample.get('image', None)
        
        # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
        position_bias_stats = None
        attribution_stats = None
        
        # âœ… MRAG-Benchå¤šé€‰é¢˜æ ¼å¼æ”¯æŒ
        has_choices = ('A' in sample and sample.get('A'))
        original_question = question
        
        if has_choices:
            # æ„é€ å¤šé€‰é¢˜æ ¼å¼çš„é—®é¢˜
            question = f"""{original_question}

Options:
A. {sample.get('A', '')}
B. {sample.get('B', '')}
C. {sample.get('C', '')}
D. {sample.get('D', '')}

Answer with the letter only (A/B/C/D):"""
        
        # ========== é˜¶æ®µ1: ä¸ç¡®å®šæ€§ä¼°è®¡ ==========
        uncertainty = self.uncertainty_estimator.estimate(question, image)
        
        if isinstance(uncertainty, dict):
            total_unc = uncertainty.get('total', 0.5)
            uncertainty_info = uncertainty
        else:
            total_unc = uncertainty
            uncertainty_info = {'total': total_unc}
        
        # ğŸ” DEBUG: è¾“å‡ºä¸ç¡®å®šæ€§å€¼
        print(f"[DEBUG] uncertainty={total_unc:.4f}, threshold={self.uncertainty_threshold:.4f}, should_retrieve={total_unc > self.uncertainty_threshold}")
        
        # ========== é˜¶æ®µ2: è‡ªé€‚åº”æ£€ç´¢ ==========
        retrieved_docs = []
        retrieval_scores = []
        fused_docs = []
        fused_scores = []
        
        if self.should_retrieve(total_unc):
            should_retrieve = True
            
            # æ¨¡æ€é€‰æ‹©
            modality = self.modality_selector.select(uncertainty_info)
            print(f"[DEBUG] modality={modality}")
            
            # æŸ¥è¯¢é‡æ„
            enhanced_query = self.query_reformulator.reformulate(
                query=question,
                uncertainty_scores=uncertainty_info,
                modality=modality
            )
            print(f"[DEBUG] enhanced_query={enhanced_query[:80] if enhanced_query else 'None'}...")
            
            # æ£€ç´¢ï¼ˆæ”¯æŒä¸åŒçš„æ£€ç´¢å™¨æ¥å£ï¼‰
            print(f"[DEBUG] self.retriever is not None: {self.retriever is not None}")
            if self.retriever:
                print(f"[DEBUG] è¿›å…¥æ£€ç´¢åˆ†æ”¯")
                modality_weights = self.modality_selector.get_modality_weights(modality)
                
                # FlashRAGçš„DenseRetrieverä½¿ç”¨searchæ–¹æ³•
                if hasattr(self.retriever, 'search'):
                    print(f"[DEBUG] è°ƒç”¨retriever.search(), top_k={self.top_k}")
                    # DenseRetriever.search(query, num, return_score) è¿”å› list[dict] æˆ– (list[dict], list[float])
                    search_results = self.retriever.search(enhanced_query, num=self.top_k, return_score=True)
                    print(f"[DEBUG] search_resultsç±»å‹: {type(search_results)}, æ˜¯å¦ä¸ºtuple: {isinstance(search_results, tuple)}")
                    if isinstance(search_results, tuple):
                        retrieved_docs, retrieval_scores = search_results
                        print(f"[DEBUG] retrieved_docsæ•°é‡: {len(retrieved_docs) if retrieved_docs else 0}")
                    else:
                        retrieved_docs = search_results if search_results else []
                        retrieval_scores = [1.0] * len(retrieved_docs) if retrieved_docs else []
                        print(f"[DEBUG] retrieved_docsæ•°é‡: {len(retrieved_docs)}")
                elif hasattr(self.retriever, 'retrieve'):
                    # è‡ªå®šä¹‰æ£€ç´¢å™¨ä½¿ç”¨retrieveæ–¹æ³• (å¦‚SelfAwareMultimodalRetriever)
                    result = self.retriever.retrieve(
                        query_text=enhanced_query,
                        query_image=image,
                        top_k=self.top_k,
                        return_score=True  # ç¡®ä¿è¿”å›åˆ†æ•°
                    )
                    if isinstance(result, tuple):
                        retrieved_docs, retrieval_scores = result
                    else:
                        retrieved_docs = result if result else []
                        retrieval_scores = [1.0] * len(retrieved_docs) if retrieved_docs else []
                else:
                    retrieved_docs = []
                    retrieval_scores = []
                
                uncertainty_info['original_query'] = question
                uncertainty_info['enhanced_query'] = enhanced_query if enhanced_query != question else None
                uncertainty_info['selected_modality'] = modality
                uncertainty_info['modality_weights'] = modality_weights
            else:
                retrieved_docs, retrieval_scores = [], []
                modality = 'both'
            
            # ä½ç½®æ„ŸçŸ¥èåˆ
            position_bias_stats = None
            if self.use_position_fusion and retrieved_docs:
                fused_docs, fused_scores, position_bias_stats = self._apply_position_fusion(
                    retrieved_docs, retrieval_scores, question
                )
            else:
                fused_docs = retrieved_docs[:3] if retrieved_docs else []
                fused_scores = retrieval_scores[:3] if retrieval_scores else []
        
        else:
            should_retrieve = False
        
        # ========== é˜¶æ®µ3: ç”Ÿæˆç­”æ¡ˆï¼ˆQwen3-VLï¼‰==========
        if fused_docs:
            context = self._format_context_with_attribution_preview(
                fused_docs, fused_scores, attributions=None
            )
        else:
            context = ""
        
        # âœ… ä½¿ç”¨Qwen3-VLç”Ÿæˆ
        text_answer = self._generate_answer_qwen3vl(question, context, image)
        
        # ========== é˜¶æ®µ4: ç»†ç²’åº¦å½’å›  ==========
        attributions = None
        
        if self.use_attribution and fused_docs:
            try:
                retrieved_texts = [doc.get('text', '') for doc in fused_docs]
                attributions = self.attribution_module.attribute_text_evidence(
                    generated_text=text_answer,
                    retrieved_texts=retrieved_texts
                )
                
                if attributions and isinstance(attributions, list):
                    high_conf_count = sum(
                        1 for attr in attributions 
                        if isinstance(attr, dict) and attr.get('confidence', 0) > 0.7
                    )
                    
                    attribution_stats = {
                        'total_sources': len(attributions),
                        'high_confidence': high_conf_count,
                        'avg_confidence': np.mean([
                            attr.get('confidence', 0) 
                            for attr in attributions 
                            if isinstance(attr, dict)
                        ]) if attributions else 0
                    }
                else:
                    attribution_stats = None
                    
            except Exception as e:
                warnings.warn(f"å½’å› è®¡ç®—å¤±è´¥: {e}")
                attributions = None
                attribution_stats = None
        
        # ========== é˜¶æ®µ5: å¤šæ¨¡æ€è¾“å‡ºå¢å¼ºï¼ˆå¯é€‰ï¼‰==========
        final_answer = text_answer
        if self.use_multimodal_output and retrieved_docs and self.multimodal_output is not None:
            try:
                final_answer = self.multimodal_output.generate_multimodal_answer(
                    text_answer, retrieved_docs, attributions
                )
            except Exception as e:
                warnings.warn(f"å¤šæ¨¡æ€è¾“å‡ºå¢å¼ºå¤±è´¥: {e}")
                final_answer = text_answer
        
        # âœ… å¤šé€‰é¢˜ç­”æ¡ˆæ˜ å°„
        if has_choices:
            # æå–é€‰é¡¹å­—æ¯ï¼ˆA/B/C/Dï¼‰
            answer_letter = final_answer.strip().upper()
            if answer_letter and answer_letter[0] in ['A', 'B', 'C', 'D']:
                choice_letter = answer_letter[0]
                # æ˜ å°„å›å…·ä½“ç­”æ¡ˆ
                mapped_answer = sample.get(choice_letter, final_answer)
                final_answer = mapped_answer
        
        # ========== è¿”å›ç»“æœ ==========
        # å¤„ç†æ ‡å‡†ç­”æ¡ˆå­—æ®µï¼ˆå…¼å®¹ä¸åŒæ•°æ®é›†æ ¼å¼ï¼‰
        golden_answers = sample.get('golden_answers', [])
        if not golden_answers and 'answer' in sample:
            # MRAG-Benchç­‰æ•°æ®é›†ä½¿ç”¨'answer'å­—æ®µ
            golden = sample['answer']
            golden_answers = [golden] if golden else []
        
        result = {
            'question': question,
            'answer': final_answer,
            'uncertainty': uncertainty_info,
            'retrieved': should_retrieve,
            'retrieved_docs': retrieved_docs if should_retrieve else [],
            'n_retrieved_docs': len(retrieved_docs) if should_retrieve else 0,
            'n_fused_docs': len(fused_docs),
            'attributions': attributions,
            'golden_answers': golden_answers
        }
        
        if should_retrieve:
            result['selected_modality'] = uncertainty_info.get('selected_modality', 'both')
            result['modality_weights'] = uncertainty_info.get('modality_weights', {'text': 0.5, 'image': 0.5})
            result['query_enhanced'] = uncertainty_info.get('enhanced_query') is not None
        
        # æ·»åŠ å½’å› ç»Ÿè®¡ä¿¡æ¯
        if attributions:
            result['attribution_stats'] = attribution_stats if 'attribution_stats' in locals() else None
        
        # æ·»åŠ ä½ç½®åå·®ç»Ÿè®¡ä¿¡æ¯
        if position_bias_stats is not None:
            result['position_bias_stats'] = position_bias_stats
        
        return result
    
    # =========================================================================
    # è¾…åŠ©æ–¹æ³•
    # =========================================================================
    
    def _apply_position_fusion(self, docs: List[str], scores: List[float], 
                               query: str) -> Tuple[List[str], List[float], Dict]:
        """
        åº”ç”¨ä½ç½®æ„ŸçŸ¥èåˆ
        
        Returns:
            fused_docs: èåˆåçš„æ–‡æ¡£
            fused_scores: èåˆåçš„åˆ†æ•°
            position_bias_stats: ä½ç½®åå·®ç»Ÿè®¡ä¿¡æ¯
        """
        if not docs:
            return [], [], None
        
        k = len(docs)
        
        # è®¡ç®—ä½ç½®æƒé‡
        position_weights = np.exp(np.arange(k) * 0.5)
        position_weights = position_weights / position_weights.sum()
        
        # ç»¼åˆæƒé‡
        scores_norm = np.array(scores) / (np.sum(scores) + 1e-10)
        combined_weights = scores_norm * position_weights
        
        # æ’åº
        sorted_indices = np.argsort(combined_weights)[::-1]
        
        reordered_docs = [docs[i] for i in sorted_indices]
        reordered_scores = [combined_weights[i] for i in sorted_indices]
        
        # è®¡ç®—ä½ç½®åå·®ç»Ÿè®¡ä¿¡æ¯
        position_bias_stats = {
            'original_positions': list(range(k)),
            'reordered_positions': sorted_indices.tolist(),
            'position_weights': position_weights.tolist(),
            'original_scores': scores,
            'combined_scores': combined_weights.tolist(),
            'reordering_magnitude': float(np.mean(np.abs(np.array(sorted_indices) - np.arange(k)))),
            'top1_changed': int(sorted_indices[0] != 0) if len(sorted_indices) > 0 else 0,
        }
        
        return reordered_docs[:3], reordered_scores[:3], position_bias_stats
    
    def _format_context_with_attribution_preview(self, docs: List[str], 
                                                  scores: List[float],
                                                  attributions: Optional[List] = None) -> str:
        """ä½¿ç”¨å½’å› ä¿¡æ¯æ ¼å¼åŒ–ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        for i, (doc, score) in enumerate(zip(docs, scores)):
            if attributions and i < len(attributions):
                attr = attributions[i]
                attr_conf = attr.get('confidence', 0) if isinstance(attr, dict) else 0
                combined_score = (score + attr_conf) / 2
            else:
                combined_score = score
            
            if combined_score > 0.6:
                importance = "**HIGH RELEVANCE**"
            elif combined_score > 0.3:
                importance = "**RELEVANT**"
            else:
                importance = "**REFERENCE**"
            
            doc_text = doc[:300] if len(doc) > 300 else doc
            citation = f" [Confidence: {combined_score:.2f}]" if attributions else ""
            
            context_parts.append(
                f"[Evidence {i+1}] {importance}{citation}\n{doc_text}"
            )
        
        return "\n\n".join(context_parts)
    
    def _generate_answer_qwen3vl(self, question: str, context: str, image=None) -> str:
        """
        âœ… ä½¿ç”¨Qwen3-VLç”Ÿæˆç­”æ¡ˆ
        
        æ”¯æŒï¼š
        - å•å›¾åƒç”Ÿæˆ
        - å¤šå›¾åƒç”Ÿæˆï¼ˆå¦‚æœcontextåŒ…å«å¤šå›¾åƒï¼‰
        - é«˜åˆ†è¾¨ç‡å›¾åƒ
        """
        # æ„å»ºprompt
        if context:
            prompt = f"""Based on the following evidence, answer the question concisely.

{context}

Question: {question}

Answer:"""
        else:
            prompt = f"""Question: {question}

Answer:"""
        
        try:
            # âœ… ä½¿ç”¨Qwen3-VLç”Ÿæˆ
            # æ³¨æ„ï¼šQwen3-VLä¸æ¥å—thinkingå‚æ•°ï¼Œå·²åœ¨promptä¸­æ§åˆ¶è¾“å‡ºæ ¼å¼
            # ä¼˜åŒ–ï¼šé™ä½æ¸©åº¦æé«˜ç¡®å®šæ€§ï¼Œå‡å°‘tokenæ•°åŠ å¿«é€Ÿåº¦
            answer = self.qwen3_vl.generate(
                text=prompt,
                image=image,
                max_new_tokens=10,  # å¤šé€‰é¢˜åªéœ€è¦1ä¸ªå­—æ¯ï¼Œå‡å°‘ç”Ÿæˆé•¿åº¦
                temperature=0.01  # æ¥è¿‘0ï¼Œæé«˜ç¡®å®šæ€§
            )
            return answer.strip()
            
        except Exception as e:
            warnings.warn(f"Qwen3-VLç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    # =========================================================================
    # æ‰¹é‡å¤„ç†
    # =========================================================================
    
    def run(self, dataset, verbose: bool = True) -> List[Dict[str, Any]]:
        """åœ¨æ•°æ®é›†ä¸Šè¿è¡ŒPipelineï¼ˆQwen3-VLç‰ˆæœ¬ï¼‰"""
        results = []
        
        if verbose:
            from tqdm import tqdm
            iterator = tqdm(dataset, desc="Self-Aware Pipeline (Qwen3-VL)")
        else:
            iterator = dataset
        
        retrieval_triggered = 0
        
        for sample in iterator:
            try:
                result = self.run_single(sample)
                
                if result['retrieved']:
                    retrieval_triggered += 1
                
                # è¯„ä¼°
                answer = result['answer'].lower().strip()
                golden = result.get('golden_answers', [])
                correct = any(g.lower().strip() in answer for g in golden)
                result['correct'] = correct
                
                results.append(result)
                
                if verbose and len(results) % 50 == 0:
                    acc = sum(r['correct'] for r in results) / len(results)
                    ret_rate = retrieval_triggered / len(results)
                    if hasattr(iterator, 'set_postfix'):
                        iterator.set_postfix({
                            'Acc': f'{acc*100:.1f}%',
                            'Ret': f'{ret_rate*100:.0f}%'
                        })
            
            except Exception as e:
                warnings.warn(f"å¤„ç†æ ·æœ¬å¤±è´¥: {e}")
                continue
        
        if verbose:
            acc = sum(r['correct'] for r in results) / len(results) if results else 0
            ret_rate = retrieval_triggered / len(results) if results else 0
            print(f"\nâœ… Pipelineå®Œæˆ:")
            print(f"  å‡†ç¡®ç‡: {acc*100:.2f}%")
            print(f"  æ£€ç´¢ç‡: {ret_rate*100:.1f}%")
            print(f"  å¤„ç†æ ·æœ¬: {len(results)}")
        
        return results


# å·¥å‚å‡½æ•°
def create_self_aware_pipeline_qwen3vl(qwen3_vl_wrapper, retriever, **kwargs):
    """
    åˆ›å»ºSelf-Aware Pipelineï¼ˆQwen3-VLç‰ˆæœ¬ï¼‰
    
    Args:
        qwen3_vl_wrapper: Qwen3-VLå°è£…å™¨
        retriever: æ£€ç´¢å™¨
        **kwargs: é…ç½®å‚æ•°
    
    Returns:
        SelfAwarePipelineQwen3VLå®ä¾‹
    """
    return SelfAwarePipelineQwen3VL(qwen3_vl_wrapper, retriever, kwargs)


if __name__ == '__main__':
    print("=" * 80)
    print("Self-Aware Multimodal RAG Pipeline - Qwen3-VLç‰ˆæœ¬")
    print("=" * 80)
    print()
    print("âœ… P0ä¿®å¤: ç»Ÿä¸€ä½¿ç”¨Qwen3-VL")
    print()
    print("ä¼˜åŠ¿ï¼š")
    print("  1. å…¬å¹³å¯¹æ¯”ï¼ˆæ‰€æœ‰æ–¹æ³•ç»Ÿä¸€æ¨¡å‹ï¼‰")
    print("  2. å¤šå›¾åƒæ”¯æŒï¼ˆæœ€å¤š20å¼ ï¼‰")
    print("  3. é«˜åˆ†è¾¨ç‡å¤„ç†")
    print("  4. æ›´å¼ºæŒ‡ä»¤è·Ÿéš")
    print()
    print("ä½¿ç”¨æ–¹æ³•:")
    print("  from flashrag.pipeline.self_aware_pipeline_qwen3vl import create_self_aware_pipeline_qwen3vl")
    print("  from flashrag.modules.qwen3_vl import create_qwen3_vl_wrapper")
    print()
    print("  qwen3_vl = create_qwen3_vl_wrapper()")
    print("  pipeline = create_self_aware_pipeline_qwen3vl(qwen3_vl, retriever)")
    print("  results = pipeline.run(dataset)")
    print("=" * 80)

