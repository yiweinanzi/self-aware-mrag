#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Self-Aware Multimodal RAG Pipeline - Qwen3-VLç‰ˆæœ¬

âœ… ç»Ÿä¸€ä½¿ç”¨Qwen3-VL-8B-Instructï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”

ä¸LLaVAç‰ˆæœ¬çš„åŒºåˆ«ï¼š
1. æ¨¡å‹ï¼šQwen3-VL-8B-Instructï¼ˆ2024ï¼‰vs LLaVA-1.5ï¼ˆ2023ï¼‰
2. å¤šå›¾åƒæ”¯æŒï¼šæœ€å¤š20å¼  vs å•å›¾åƒ
3. é«˜åˆ†è¾¨ç‡ï¼šæ”¯æŒ vs æœ‰é™
4. æŒ‡ä»¤è·Ÿéšï¼šæ›´å¼º vs ä¸€èˆ¬

æ ¸å¿ƒåˆ›æ–°ä¿æŒä¸å˜ï¼š
- è·¨æ¨¡æ€ä¸ç¡®å®šæ€§ä¼°è®¡
- ä½ç½®æ„ŸçŸ¥èåˆ
- ç»†ç²’åº¦å½’å› 
- å¤šæ¨¡æ€è¾“å‡ºï¼ˆå¯é€‰ï¼‰
"""

import torch
import warnings
from typing import List, Dict, Any, Optional, Tuple
import numpy as np

class SelfAwarePipelineQwen3VL:
    """
    Self-Aware Multimodal RAG Pipelineï¼ˆQwen3-VLç‰ˆæœ¬ï¼‰
    
    âœ… P0ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨Qwen3-VLç¡®ä¿å…¬å¹³å¯¹æ¯”
    
    æ ¸å¿ƒæµç¨‹ï¼š
    1. ä¸ç¡®å®šæ€§ä¼°è®¡ â†’ å†³å®šæ˜¯å¦æ£€ç´¢
    2. è‡ªé€‚åº”æ£€ç´¢ â†’ ä½ç½®æ„ŸçŸ¥èåˆ
    3. ç”Ÿæˆç­”æ¡ˆï¼ˆQwen3-VLï¼‰
    4. ç»†ç²’åº¦å½’å› 
    5. å¤šæ¨¡æ€è¾“å‡ºå¢å¼ºï¼ˆå¯é€‰ï¼‰
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    ```python
    from flashrag.modules.qwen3_vl import create_qwen3_vl_wrapper
    
    qwen3_vl = create_qwen3_vl_wrapper()
    
    pipeline = SelfAwarePipelineQwen3VL(
        qwen3_vl_wrapper=qwen3_vl,
        retriever=retriever,
        config={
            'uncertainty_threshold': 0.35,
            'use_position_fusion': True,
            'use_attribution': True
        }
    )
    
    results = pipeline.run(dataset)
    ```
    """
    
    def __init__(self, qwen3_vl_wrapper, retriever, config=None):
        """
        åˆå§‹åŒ–Pipelineï¼ˆQwen3-VLç‰ˆæœ¬ï¼‰
        
        Args:
            qwen3_vl_wrapper: Qwen3-VLæ¨¡å‹å°è£…å™¨
            retriever: æ£€ç´¢å™¨
            config: é…ç½®å­—å…¸
        """
        self.qwen3_vl = qwen3_vl_wrapper  # âœ… ä½¿ç”¨Qwen3-VL
        self.retriever = retriever
        self.config = config or {}
        
        # é…ç½®å‚æ•°
        # ä¼˜åŒ–ï¼šé™ä½é˜ˆå€¼ï¼Œè®©æ›´å¤šæ ·æœ¬è§¦å‘æ£€ç´¢
        self.uncertainty_threshold = self.config.get('uncertainty_threshold', 0.30)  # 0.35 â†’ 0.30
        self.top_k = self.config.get('retrieval_topk', 5)
        self.use_position_fusion = self.config.get('use_position_fusion', True)
        self.use_attribution = self.config.get('use_attribution', True)
        self.use_multimodal_output = self.config.get('enable_multimodal_output', False)
        
        # Qwen3-VLç‰¹å®šé…ç½®
        self.max_images = min(self.config.get('max_images', 20), 20)  # æœ€å¤š20å¼ 
        self.use_thinking = self.config.get('thinking', False)  # P0-2: ç¡®ä¿thinking=false
        
        # åˆå§‹åŒ–æ¨¡å—
        self._init_modules()
        
        print("âœ… SelfAwarePipelineQwen3VLåˆå§‹åŒ–å®Œæˆ")
        print(f"  - æ¨¡å‹: Qwen3-VL-8B-Instruct")
        print(f"  - Uncertainty threshold (Ï„): {self.uncertainty_threshold:.3f}")
        print(f"  - Max images: {self.max_images}")
        print(f"  - Thinking mode: {self.use_thinking} (æ¨èFalse)")
        print(f"  - Position fusion: {self.use_position_fusion}")
        print(f"  - Attribution: {self.use_attribution}")
        print(f"  - Multimodal output: {self.use_multimodal_output}")
    
    def should_retrieve(self, u: float, tau: Optional[float] = None) -> bool:
        """
        âœ… P0-6: é˜ˆå€¼æ¥å£åŒ– - æ£€ç´¢å†³ç­–å‡½æ•°
        
        Args:
            u: ä¸ç¡®å®šæ€§åˆ†æ•°
            tau: ä¸ç¡®å®šæ€§é˜ˆå€¼ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            bool: Trueè¡¨ç¤ºéœ€è¦æ£€ç´¢
        """
        threshold = tau if tau is not None else self.uncertainty_threshold
        return u > threshold
    
    def _relevance_judgment(self, question: str, document: str, image=None) -> bool:
        """
        âœ… ä¼˜åŒ–C-Step1: åˆ¤æ–­æ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³ï¼ˆå€Ÿé‰´Self-RAGï¼‰
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            document: æ–‡æ¡£å†…å®¹
            image: å›¾åƒï¼ˆå¯é€‰ï¼‰
        
        Returns:
            bool: Trueè¡¨ç¤ºç›¸å…³
        """
        doc_preview = document[:300] + "..." if len(document) > 300 else document
        
        prompt = f"""Task: Is this document relevant to answering the question?

Question: {question}

Document: {doc_preview}

Answer ONLY 'RELEVANT' or 'IRRELEVANT':"""
        
        try:
            response = self.qwen3_vl.generate(
                text=prompt,
                image=None,
                max_new_tokens=5,
                temperature=0.05
            )
            
            response_upper = response.strip().upper()
            is_relevant = 'RELEVANT' in response_upper and 'IRRELEVANT' not in response_upper[:15]
            return is_relevant
        except Exception as e:
            print(f"[WARN] Relevance judgment failed: {e}, defaulting to True")
            return True
    
    def _verify_answer_support(self, question: str, answer: str, documents: list, image=None) -> float:
        """
        âœ… æœ€ç»ˆä¼˜åŒ–-Step4: éªŒè¯ç­”æ¡ˆçš„æ”¯æŒåº¦ï¼ˆå€Ÿé‰´Self-RAGï¼‰
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            documents: æ£€ç´¢çš„æ–‡æ¡£åˆ—è¡¨
            image: å›¾åƒï¼ˆå¯é€‰ï¼‰
        
        Returns:
            float: æ”¯æŒåº¦åˆ†æ•° [0, 1]
        """
        # æå–æ–‡æ¡£å†…å®¹
        doc_texts = []
        for doc in documents[:3]:  # åªç”¨å‰3ä¸ª
            if isinstance(doc, dict):
                doc_texts.append(doc.get('contents', doc.get('text', '')))
            else:
                doc_texts.append(str(doc))
        
        combined_docs = " ".join(doc_texts)[:500]  # é™åˆ¶é•¿åº¦
        
        prompt = f"""Task: Is the answer supported by the provided documents?

Question: {question}

Answer: {answer}

Documents: {combined_docs}

Rate the support level:
- FULLY_SUPPORTED: Answer is directly supported by documents
- PARTIALLY_SUPPORTED: Answer is somewhat related to documents  
- NOT_SUPPORTED: Answer is not supported by documents

Answer with ONE word only:"""
        
        try:
            response = self.qwen3_vl.generate(
                text=prompt,
                image=None,
                max_new_tokens=10,
                temperature=0.05
            )
            
            response_upper = response.strip().upper()
            
            # æ˜ å°„åˆ°åˆ†æ•°
            if 'FULLY' in response_upper or 'FULL' in response_upper:
                return 0.9
            elif 'PARTIALLY' in response_upper or 'PARTIAL' in response_upper:
                return 0.6
            elif 'NOT' in response_upper:
                return 0.2
            else:
                return 0.5  # é»˜è®¤ä¸­ç­‰æ”¯æŒåº¦
        
        except Exception as e:
            print(f"[WARN] Support verification failed: {e}, defaulting to 0.5")
            return 0.5
    
    def _init_modules(self):
        """åˆå§‹åŒ–å„ä¸ªæ¨¡å—"""
        from flashrag.modules.position_aware_fusion import PositionAwareCrossModalFusion
        from flashrag.modules.attribution import FineGrainedMultimodalAttribution
        from flashrag.modules.modality_selector import ModalitySelector
        from flashrag.modules.query_reformulation import QueryReformulator
        
        # 1. ä¸ç¡®å®šæ€§ä¼°è®¡å™¨ - æ”¯æŒé€‰æ‹©åŸå§‹ç‰ˆæˆ–æ”¹è¿›ç‰ˆ
        use_improved = self.config.get('use_improved_estimator', False)
        
        if use_improved:
            print("  âœ… ä½¿ç”¨æ”¹è¿›ç‰ˆä¸ç¡®å®šæ€§ä¼°è®¡å™¨ (ImprovedUncertaintyEstimator)")
            from flashrag.modules.uncertainty_estimator_improved import ImprovedUncertaintyEstimator
            self.uncertainty_estimator = ImprovedUncertaintyEstimator(
                config={
                    'clip_model_path': self.config.get('clip_model_path', 
                        '/root/autodl-tmp/models/clip-vit-large-patch14-336'),
                    'text_weight': 0.5,
                    'visual_weight': 0.3,
                    'alignment_weight': 0.2
                }
            )
        else:
            print("  â„¹ï¸  ä½¿ç”¨åŸå§‹ä¸ç¡®å®šæ€§ä¼°è®¡å™¨ (CrossModalUncertaintyEstimator)")
            from flashrag.modules.uncertainty_estimator import CrossModalUncertaintyEstimator
            self.uncertainty_estimator = CrossModalUncertaintyEstimator(
                mllm_model=self.qwen3_vl,  # âœ… ä¿®å¤ï¼šä¼ å…¥qwen3_vlå®Œæ•´wrapper
                config={
                    'eigen_threshold': -6.0,
                    'use_clip_for_alignment': True,
                    'clip_model_path': self.config.get('clip_model_path', 
                        '/root/autodl-tmp/models/clip-vit-large-patch14-336'),
                    'text_weight': 0.4,
                    'visual_weight': 0.3,
                    'alignment_weight': 0.3
                }
            )
        
        # 2. æ¨¡æ€é€‰æ‹©å™¨
        self.modality_selector = ModalitySelector()
        
        # 3. æŸ¥è¯¢é‡æ„å™¨
        self.query_reformulator = QueryReformulator()
        
        # 4. ä½ç½®æ„ŸçŸ¥èåˆ
        if self.use_position_fusion:
            self.position_aware_fusion = PositionAwareCrossModalFusion(
                d_model=768, num_heads=12, device='cpu'
            )
        
        # 5. å½’å› æ¨¡å—
        if self.use_attribution:
            self.attribution_module = FineGrainedMultimodalAttribution(
                mllm_model=None
            )
        
        # 6. å¤šæ¨¡æ€è¾“å‡ºï¼ˆå¯é€‰ï¼‰
        if self.use_multimodal_output:
            try:
                from flashrag.modules.multimodal_output import MultimodalOutputComposition
                self.multimodal_output = MultimodalOutputComposition()
                print("  âš ï¸  å¤šæ¨¡æ€è¾“å‡ºå·²å¯ç”¨ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰")
            except Exception as e:
                warnings.warn(f"Multimodal outputæ¨¡å—åŠ è½½å¤±è´¥: {e}")
                self.use_multimodal_output = False
        else:
            self.multimodal_output = None
    
    # =========================================================================
    # æ ¸å¿ƒPipelineæµç¨‹
    # =========================================================================
    
    def run_single(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ ·æœ¬ï¼ˆQwen3-VLç‰ˆæœ¬ï¼‰
        
        Args:
            sample: æ ·æœ¬å­—å…¸
        
        Returns:
            Dict: ç»“æœå­—å…¸
        """
        question = sample['question']
        image = sample.get('image', None)
        
        # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
        position_bias_stats = None
        attribution_stats = None
        
        # âœ… MRAG-Benchå¤šé€‰é¢˜æ ¼å¼æ”¯æŒ
        has_choices = ('A' in sample and sample.get('A'))
        original_question = question
        
        if has_choices:
            # æ„é€ å¤šé€‰é¢˜æ ¼å¼çš„é—®é¢˜
            question = f"""{original_question}

Options:
A. {sample.get('A', '')}
B. {sample.get('B', '')}
C. {sample.get('C', '')}
D. {sample.get('D', '')}

Answer with the letter only (A/B/C/D):"""
        
        # âš ï¸ ä¿å­˜åŸå§‹questionç”¨äºç”Ÿæˆï¼ˆé¿å…è¢«queryæ”¹å†™ç ´åOptionsæ ¼å¼ï¼‰
        question_for_generation = question
        
        # ========== é˜¶æ®µ1: ä¸ç¡®å®šæ€§ä¼°è®¡ ==========
        uncertainty = self.uncertainty_estimator.estimate(question, image)
        
        if isinstance(uncertainty, dict):
            total_unc = uncertainty.get('total', 0.5)
            uncertainty_info = uncertainty
        else:
            total_unc = uncertainty
            uncertainty_info = {'total': total_unc}
        
        # ğŸ” DEBUG: è¾“å‡ºä¸ç¡®å®šæ€§å€¼ï¼ˆåŒ…å«ä¸‰ä¸ªåˆ†é‡ï¼‰
        text_unc = uncertainty_info.get('text', 0.0)
        visual_unc = uncertainty_info.get('visual', 0.0)
        align_unc = uncertainty_info.get('alignment', 0.0)
        print(f"[DEBUG] uncertainty={total_unc:.4f} [text={text_unc:.4f}, visual={visual_unc:.4f}, align={align_unc:.4f}], threshold={self.uncertainty_threshold:.4f}, should_retrieve={total_unc > self.uncertainty_threshold}")
        
        # ========== é˜¶æ®µ2: è‡ªé€‚åº”æ£€ç´¢ ==========
        retrieved_docs = []
        retrieval_scores = []
        fused_docs = []
        fused_scores = []
        
        if self.should_retrieve(total_unc):
            should_retrieve = True
            
            # æ¨¡æ€é€‰æ‹©
            modality = self.modality_selector.select(uncertainty_info)
            print(f"[DEBUG] modality={modality}")
            
            # æŸ¥è¯¢é‡æ„
            enhanced_query = self.query_reformulator.reformulate(
                query=question,
                uncertainty_scores=uncertainty_info,
                modality=modality
            )
            print(f"[DEBUG] enhanced_query={enhanced_query[:80] if enhanced_query else 'None'}...")
            
            # æ£€ç´¢ï¼ˆæ”¯æŒä¸åŒçš„æ£€ç´¢å™¨æ¥å£ï¼‰
            print(f"[DEBUG] self.retriever is not None: {self.retriever is not None}")
            if self.retriever:
                print(f"[DEBUG] è¿›å…¥æ£€ç´¢åˆ†æ”¯")
                modality_weights = self.modality_selector.get_modality_weights(modality)
                
                # FlashRAGçš„DenseRetrieverä½¿ç”¨searchæ–¹æ³•
                if hasattr(self.retriever, 'search'):
                    print(f"[DEBUG] è°ƒç”¨retriever.search(), top_k={self.top_k}")
                    # DenseRetriever.search(query, num, return_score) è¿”å› list[dict] æˆ– (list[dict], list[float])
                    search_results = self.retriever.search(enhanced_query, num=self.top_k, return_score=True)
                    print(f"[DEBUG] search_resultsç±»å‹: {type(search_results)}, æ˜¯å¦ä¸ºtuple: {isinstance(search_results, tuple)}")
                    if isinstance(search_results, tuple):
                        retrieved_docs, retrieval_scores = search_results
                        print(f"[DEBUG] retrieved_docsæ•°é‡: {len(retrieved_docs) if retrieved_docs else 0}")
                    else:
                        retrieved_docs = search_results if search_results else []
                        retrieval_scores = [1.0] * len(retrieved_docs) if retrieved_docs else []
                        print(f"[DEBUG] retrieved_docsæ•°é‡: {len(retrieved_docs)}")
                elif hasattr(self.retriever, 'retrieve'):
                    # è‡ªå®šä¹‰æ£€ç´¢å™¨ä½¿ç”¨retrieveæ–¹æ³• (å¦‚SelfAwareMultimodalRetriever)
                    result = self.retriever.retrieve(
                        query_text=enhanced_query,
                        query_image=image,
                        top_k=self.top_k,
                        return_score=True  # ç¡®ä¿è¿”å›åˆ†æ•°
                    )
                    if isinstance(result, tuple):
                        retrieved_docs, retrieval_scores = result
                    else:
                        retrieved_docs = result if result else []
                        retrieval_scores = [1.0] * len(retrieved_docs) if retrieved_docs else []
                else:
                    retrieved_docs = []
                    retrieval_scores = []
                
                uncertainty_info['original_query'] = question
                uncertainty_info['enhanced_query'] = enhanced_query if enhanced_query != question else None
                uncertainty_info['selected_modality'] = modality
                uncertainty_info['modality_weights'] = modality_weights
            else:
                retrieved_docs, retrieval_scores = [], []
                modality = 'both'
            
            # âœ… ä¼˜åŒ–C-Step1: æ–‡æ¡£ç›¸å…³æ€§è¿‡æ»¤ï¼ˆå€Ÿé‰´Self-RAGï¼‰
            if retrieved_docs:
                relevant_docs = []
                relevant_scores = []
                
                print(f"[FILTER] å¼€å§‹è¿‡æ»¤{len(retrieved_docs)}ä¸ªæ£€ç´¢æ–‡æ¡£...")
                for idx, doc in enumerate(retrieved_docs):
                    doc_text = doc.get('contents', '') if isinstance(doc, dict) else str(doc)
                    is_relevant = self._relevance_judgment(question, doc_text, image)
                    
                    if is_relevant:
                        relevant_docs.append(doc)
                        relevant_scores.append(retrieval_scores[idx] if idx < len(retrieval_scores) else 1.0)
                        print(f"[FILTER] æ–‡æ¡£{idx+1}: âœ… RELEVANT")
                    else:
                        print(f"[FILTER] æ–‡æ¡£{idx+1}: âŒ IRRELEVANT (è¿‡æ»¤)")
                
                print(f"[FILTER] è¿‡æ»¤å®Œæˆ: {len(retrieved_docs)} â†’ {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
                
                # å¦‚æœæ²¡æœ‰ç›¸å…³æ–‡æ¡£ï¼Œå›é€€åˆ°ç›´æ¥å›ç­”ï¼ˆé¿å…ä½¿ç”¨å™ªå£°ï¼‰
                if not relevant_docs:
                    print(f"[FILTER] âš ï¸  æ— ç›¸å…³æ–‡æ¡£ï¼Œå›é€€åˆ°ç›´æ¥å›ç­”")
                    should_retrieve = False
                    retrieved_docs, retrieval_scores = [], []
                else:
                    # ä½¿ç”¨è¿‡æ»¤åçš„æ–‡æ¡£
                    retrieved_docs = relevant_docs
                    retrieval_scores = relevant_scores
            
            # ä½ç½®æ„ŸçŸ¥èåˆ
            position_bias_stats = None
            if self.use_position_fusion and retrieved_docs:
                # âœ… ä¿®å¤P0-2: ä¼ é€’ä¸ç¡®å®šæ€§åˆ°ä½ç½®èåˆï¼ˆåˆ›æ–°ç‚¹1å’Œ2çš„å…³è”ï¼‰
                fused_docs, fused_scores, position_bias_stats = self._apply_position_fusion(
                    retrieved_docs, retrieval_scores, question,
                    uncertainty_scores=uncertainty_info  # âœ… ä¼ å…¥ä¸ç¡®å®šæ€§
                )
            else:
                fused_docs = retrieved_docs[:3] if retrieved_docs else []
                fused_scores = retrieval_scores[:3] if retrieval_scores else []
        
        else:
            should_retrieve = False
        
        # ========== é˜¶æ®µ3: ç”Ÿæˆç­”æ¡ˆï¼ˆQwen3-VLï¼‰==========
        if fused_docs:
            context = self._format_context_with_attribution_preview(
                fused_docs, fused_scores, attributions=None
            )
        else:
            context = ""
        
        # âœ… ä½¿ç”¨Qwen3-VLç”Ÿæˆï¼ˆä¼ å…¥sampleä»¥è·å–é€‰é¡¹ï¼‰
        text_answer = self._generate_answer_qwen3vl(question_for_generation, context, image, sample)
        
        # ========== æ–°å¢ï¼šç­”æ¡ˆæ”¯æŒåº¦éªŒè¯ï¼ˆå€Ÿé‰´Self-RAGï¼‰==========
        support_score = None
        if fused_docs and text_answer:
            try:
                support_score = self._verify_answer_support(question_for_generation, text_answer, fused_docs, image)
                
                # å¦‚æœæ”¯æŒåº¦è¿‡ä½ï¼Œå›é€€åˆ°ç›´æ¥å›ç­”ï¼ˆä¸ä½¿ç”¨æ£€ç´¢ï¼‰
                if support_score < 0.4:  # æ”¯æŒåº¦é˜ˆå€¼
                    print(f"[SUPPORT] âš ï¸  ç­”æ¡ˆæ”¯æŒåº¦è¿‡ä½ ({support_score:.2f})ï¼Œå›é€€åˆ°ç›´æ¥å›ç­”")
                    # é‡æ–°ç”Ÿæˆï¼ˆä¸ä½¿ç”¨æ£€ç´¢ç»“æœï¼‰
                    text_answer = self._generate_answer_qwen3vl(question_for_generation, "", image, sample)
                    fused_docs = []  # æ¸…ç©ºæ£€ç´¢ç»“æœ
            except Exception as e:
                print(f"[SUPPORT] éªŒè¯å¤±è´¥: {e}")
        
        # ========== é˜¶æ®µ4: ç»†ç²’åº¦å½’å›  ==========
        attributions = None
        
        if self.use_attribution and fused_docs:
            try:
                retrieved_texts = [doc.get('text', '') for doc in fused_docs]
                attributions = self.attribution_module.attribute_text_evidence(
                    generated_text=text_answer,
                    retrieved_texts=retrieved_texts
                )
                
                if attributions and isinstance(attributions, list):
                    high_conf_count = sum(
                        1 for attr in attributions 
                        if isinstance(attr, dict) and attr.get('confidence', 0) > 0.7
                    )
                    
                    attribution_stats = {
                        'total_sources': len(attributions),
                        'high_confidence': high_conf_count,
                        'avg_confidence': np.mean([
                            attr.get('confidence', 0) 
                            for attr in attributions 
                            if isinstance(attr, dict)
                        ]) if attributions else 0
                    }
                else:
                    attribution_stats = None
                    
            except Exception as e:
                warnings.warn(f"å½’å› è®¡ç®—å¤±è´¥: {e}")
                attributions = None
                attribution_stats = None
        
        # ========== é˜¶æ®µ5: å¤šæ¨¡æ€è¾“å‡ºå¢å¼ºï¼ˆå¯é€‰ï¼‰==========
        final_answer = text_answer
        if self.use_multimodal_output and retrieved_docs and self.multimodal_output is not None:
            try:
                final_answer = self.multimodal_output.generate_multimodal_answer(
                    text_answer, retrieved_docs, attributions
                )
            except Exception as e:
                warnings.warn(f"å¤šæ¨¡æ€è¾“å‡ºå¢å¼ºå¤±è´¥: {e}")
                final_answer = text_answer
        
        # âœ… å¤šé€‰é¢˜ç­”æ¡ˆæ˜ å°„
        if has_choices:
            # æå–é€‰é¡¹å­—æ¯ï¼ˆA/B/C/Dï¼‰
            answer_letter = final_answer.strip().upper()
            if answer_letter and answer_letter[0] in ['A', 'B', 'C', 'D']:
                choice_letter = answer_letter[0]
                # æ˜ å°„å›å…·ä½“ç­”æ¡ˆ
                mapped_answer = sample.get(choice_letter, final_answer)
                final_answer = mapped_answer
        
        # ========== è¿”å›ç»“æœ ==========
        # å¤„ç†æ ‡å‡†ç­”æ¡ˆå­—æ®µï¼ˆå…¼å®¹ä¸åŒæ•°æ®é›†æ ¼å¼ï¼‰
        golden_answers = sample.get('golden_answers', [])
        if not golden_answers and 'answer' in sample:
            # MRAG-Benchç­‰æ•°æ®é›†ä½¿ç”¨'answer'å­—æ®µ
            golden = sample['answer']
            golden_answers = [golden] if golden else []
        
        result = {
            'question': question,
            'answer': final_answer,
            'uncertainty': uncertainty_info,
            'retrieved': should_retrieve,
            'retrieved_docs': retrieved_docs if should_retrieve else [],
            'n_retrieved_docs': len(retrieved_docs) if should_retrieve else 0,
            'n_fused_docs': len(fused_docs),
            'attributions': attributions,
            'golden_answers': golden_answers,
            
            # âœ… Task 1: æ·»åŠ evaluatoréœ€è¦çš„å­—æ®µ
            # 1. retrieval_result - ç”¨äºFaithfulnessè®¡ç®—
            'retrieval_result': [{
                'retrieved_docs': retrieved_docs if should_retrieve else [],
                'retrieval_scores': [1.0] * len(retrieved_docs) if should_retrieve else [],
                'retrieval_used': should_retrieve
            }],
            
            # 2. attributions - ç¡®ä¿æ ¼å¼æ­£ç¡®ï¼ˆå·²æœ‰ï¼Œä½†å¯èƒ½éœ€è¦è°ƒæ•´æ ¼å¼ï¼‰
            # attributionså­—æ®µå·²åœ¨ä¸Šé¢å®šä¹‰
            
            # 3. position_bias_results - ç”¨äºPosition Bias Scoreè®¡ç®—
            'position_bias_results': {
                'average_bias': position_bias_stats.get('bias_score', 0.0) if position_bias_stats else 0.0,
                'individual_scores': [position_bias_stats.get('bias_score', 0.0)] if position_bias_stats else [0.0],
                'position_weights': position_bias_stats.get('position_weights', []) if position_bias_stats else []
            }
        }
        
        if should_retrieve:
            result['selected_modality'] = uncertainty_info.get('selected_modality', 'both')
            result['modality_weights'] = uncertainty_info.get('modality_weights', {'text': 0.5, 'image': 0.5})
            result['query_enhanced'] = uncertainty_info.get('enhanced_query') is not None
        
        # æ·»åŠ å½’å› ç»Ÿè®¡ä¿¡æ¯
        if attributions:
            result['attribution_stats'] = attribution_stats if 'attribution_stats' in locals() else None
        
        # æ·»åŠ ä½ç½®åå·®ç»Ÿè®¡ä¿¡æ¯
        if position_bias_stats is not None:
            result['position_bias_stats'] = position_bias_stats
        
        return result
    
    # =========================================================================
    # è¾…åŠ©æ–¹æ³•
    # =========================================================================
    
    def _apply_position_fusion(self, docs: List[str], scores: List[float],
                               query: str,
                               uncertainty_scores: Optional[Dict] = None) -> Tuple[List[str], List[float], Dict]:
        """
        åº”ç”¨ä½ç½®æ„ŸçŸ¥èåˆï¼ˆä¸ç¡®å®šæ€§è°ƒåˆ¶ç‰ˆï¼‰

        âœ… ä¿®å¤P0-3: å®ç°ä¸ç¡®å®šæ€§é©±åŠ¨çš„ä½ç½®æƒé‡è°ƒåˆ¶

        ç†è®ºä¾æ®ï¼š
        - é«˜ä¸ç¡®å®šæ€§ â†’ æ¨¡å‹ä¸ç¡®å®š â†’ å¢å¼ºä½ç½®åå·®ç¼“è§£
        - ä½ä¸ç¡®å®šæ€§ â†’ æ¨¡å‹æœ‰ä¿¡å¿ƒ â†’ ä¿æŒæ£€ç´¢å™¨åŸåº

        Args:
            docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£
            scores: æ£€ç´¢åˆ†æ•°
            query: æŸ¥è¯¢
            uncertainty_scores: ä¸ç¡®å®šæ€§åˆ†æ•°å­—å…¸ï¼ˆåŒ…å«total, text, visual, alignmentï¼‰

        Returns:
            fused_docs: èåˆåçš„æ–‡æ¡£
            fused_scores: èåˆåçš„åˆ†æ•°
            position_bias_stats: ä½ç½®åå·®ç»Ÿè®¡ä¿¡æ¯
        """
        if not docs:
            return [], [], None

        k = len(docs)

        # åŸºç¡€ä½ç½®æƒé‡ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
        base_position_weights = np.exp(-np.arange(k) * 0.5)
        base_position_weights = base_position_weights / base_position_weights.sum()

        # âœ… æ ¸å¿ƒåˆ›æ–°ï¼šä¸ç¡®å®šæ€§è°ƒåˆ¶ä½ç½®æƒé‡
        if uncertainty_scores is not None:
            total_unc = uncertainty_scores.get('total', 0.5)

            # è°ƒåˆ¶å› å­ï¼šä¸ç¡®å®šæ€§è¶Šé«˜ï¼Œä½ç½®åå·®ç¼“è§£è¶Šå¼º
            # total_unc âˆˆ [0, 1]
            # modulation âˆˆ [0.75, 1.25]
            # å…¬å¼: modulation = 1.0 + (U_total - 0.5) Ã— Î±
            # å…¶ä¸­ Î±=0.5 æ˜¯è°ƒåˆ¶å¼ºåº¦è¶…å‚æ•°
            modulation = 1.0 + (total_unc - 0.5) * 0.5

            # åº”ç”¨è°ƒåˆ¶
            position_weights = base_position_weights * modulation
            position_weights = position_weights / position_weights.sum()

            print(f"[DEBUG] ä½ç½®èåˆï¼ˆä¸ç¡®å®šæ€§è°ƒåˆ¶ï¼‰: total_unc={total_unc:.4f}, "
                  f"modulation={modulation:.4f}, "
                  f"weights_range=[{position_weights.min():.4f}, {position_weights.max():.4f}]")
        else:
            position_weights = base_position_weights
            modulation = 1.0
            print(f"[DEBUG] ä½ç½®èåˆï¼ˆæ— è°ƒåˆ¶ï¼‰: ä½¿ç”¨åŸºç¡€æƒé‡")

        # ç»¼åˆæƒé‡
        scores_norm = np.array(scores) / (np.sum(scores) + 1e-10)
        combined_weights = scores_norm * position_weights

        # æ’åº
        sorted_indices = np.argsort(combined_weights)[::-1]

        reordered_docs = [docs[i] for i in sorted_indices]
        reordered_scores = [combined_weights[i] for i in sorted_indices]

        # è®¡ç®—ä½ç½®åå·®ç»Ÿè®¡ä¿¡æ¯
        position_bias_stats = {
            'original_positions': list(range(k)),
            'reordered_positions': sorted_indices.tolist(),
            'position_weights': position_weights.tolist(),
            'base_position_weights': base_position_weights.tolist(),  # âœ… æ–°å¢ï¼šåŸºç¡€æƒé‡
            'uncertainty_modulation': float(modulation),  # âœ… æ–°å¢ï¼šè°ƒåˆ¶å› å­
            'total_uncertainty': uncertainty_scores.get('total', 0.0) if uncertainty_scores else 0.0,  # âœ… æ–°å¢
            'original_scores': scores,
            'combined_scores': combined_weights.tolist(),
            'reordering_magnitude': float(np.mean(np.abs(np.array(sorted_indices) - np.arange(k)))),
            'top1_changed': int(sorted_indices[0] != 0) if len(sorted_indices) > 0 else 0,
        }

        return reordered_docs[:3], reordered_scores[:3], position_bias_stats  # ä¼˜åŒ–ï¼šä½¿ç”¨top3å‡å°‘å™ªå£°
    
    def _format_context_with_attribution_preview(self, docs: List[str], 
                                                  scores: List[float],
                                                  attributions: Optional[List] = None) -> str:
        """
        âœ… ä¼˜åŒ–ï¼šç®€åŒ–Contextæ ¼å¼ï¼Œé¿å…å¤æ‚æ ‡ç­¾å¹²æ‰°LLMç†è§£
        
        ä¿®æ”¹å‰: [Evidence 1] **HIGHLY RELEVANT** [Confidence: 0.95]\ntext...
        ä¿®æ”¹å: Document 1:\ntext...
        
        æ•ˆæœ: ä¸baselineä¿æŒä¸€è‡´çš„ç®€æ´æ ¼å¼
        """
        context_parts = []
        
        for i, doc in enumerate(docs):
            # ç®€åŒ–æ ¼å¼ï¼šåªä¿ç•™Documentç¼–å·å’Œå†…å®¹
            doc_text = doc[:512] if len(doc) > 512 else doc  # ä¼˜åŒ–ï¼š512å­—ç¬¦å¹³è¡¡ä¿¡æ¯ä¸å™ªå£°
            context_parts.append(
                f"Document {i+1}:\n{doc_text}"
            )
        
        return "\n\n".join(context_parts)
    
    def _generate_answer_qwen3vl(self, question: str, context: str, image=None, sample: Dict = None) -> str:
        """
        âœ… ä½¿ç”¨Qwen3-VLç”Ÿæˆç­”æ¡ˆ
        
        æ”¯æŒï¼š
        - å•å›¾åƒç”Ÿæˆ
        - å¤šå›¾åƒç”Ÿæˆï¼ˆå¦‚æœcontextåŒ…å«å¤šå›¾åƒï¼‰
        - é«˜åˆ†è¾¨ç‡å›¾åƒ
        - å¤šé€‰é¢˜æ ¼å¼ï¼ˆä¸baselineå®Œå…¨ä¸€è‡´ï¼‰
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šé€‰é¢˜
        has_choices = sample and all(k in sample and sample.get(k) for k in ['A', 'B', 'C', 'D'])
        
        # æ„å»ºprompt
        if context:
            if has_choices:
                # å¤šé€‰é¢˜æ ¼å¼ - ä¸baselineå®Œå…¨ä¸€è‡´ï¼
                # æå–çº¯é—®é¢˜ï¼ˆå»é™¤Optionséƒ¨åˆ†ï¼‰
                core_question = question.split('\nOptions:')[0] if '\nOptions:' in question else question.split('\n')[0]
                
                prompt = f"""Based on the following evidence, answer the question.

{context}

Question: {core_question}

Choices:
A. {sample['A']}
B. {sample['B']}
C. {sample['C']}
D. {sample['D']}

Answer with the letter only (A/B/C/D):"""
            else:
                # æ™®é€šé—®é¢˜æ ¼å¼
                prompt = f"""Based on the following evidence, answer the question concisely.

{context}

Question: {question}

Answer:"""
        else:
            if has_choices:
                core_question = question.split('\nOptions:')[0] if '\nOptions:' in question else question.split('\n')[0]
                prompt = f"""Question: {core_question}

Choices:
A. {sample['A']}
B. {sample['B']}
C. {sample['C']}
D. {sample['D']}

Answer with the letter only (A/B/C/D):"""
            else:
                prompt = f"""Question: {question}

Answer:"""
        
        try:
            # âœ… ä½¿ç”¨Qwen3-VLç”Ÿæˆ
            # æ³¨æ„ï¼šQwen3-VLä¸æ¥å—thinkingå‚æ•°ï¼Œå·²åœ¨promptä¸­æ§åˆ¶è¾“å‡ºæ ¼å¼
            # ä¼˜åŒ–ï¼šé™ä½æ¸©åº¦æé«˜ç¡®å®šæ€§ï¼Œå‡å°‘tokenæ•°åŠ å¿«é€Ÿåº¦
            answer = self.qwen3_vl.generate(
                text=prompt,
                image=image,
                max_new_tokens=10,  # å¤šé€‰é¢˜åªéœ€è¦1ä¸ªå­—æ¯ï¼Œå‡å°‘ç”Ÿæˆé•¿åº¦
                temperature=0.01  # æ¥è¿‘0ï¼Œæé«˜ç¡®å®šæ€§
            )
            return answer.strip()
            
        except Exception as e:
            warnings.warn(f"Qwen3-VLç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    # =========================================================================
    # æ‰¹é‡å¤„ç†
    # =========================================================================
    
    def run(self, dataset, verbose: bool = True) -> List[Dict[str, Any]]:
        """åœ¨æ•°æ®é›†ä¸Šè¿è¡ŒPipelineï¼ˆQwen3-VLç‰ˆæœ¬ï¼‰"""
        results = []
        
        if verbose:
            from tqdm import tqdm
            iterator = tqdm(dataset, desc="Self-Aware Pipeline (Qwen3-VL)")
        else:
            iterator = dataset
        
        retrieval_triggered = 0
        
        for sample in iterator:
            try:
                result = self.run_single(sample)
                
                if result['retrieved']:
                    retrieval_triggered += 1
                
                # è¯„ä¼°
                answer = result['answer'].lower().strip()
                golden = result.get('golden_answers', [])
                correct = any(g.lower().strip() in answer for g in golden)
                result['correct'] = correct
                
                results.append(result)
                
                if verbose and len(results) % 50 == 0:
                    acc = sum(r['correct'] for r in results) / len(results)
                    ret_rate = retrieval_triggered / len(results)
                    if hasattr(iterator, 'set_postfix'):
                        iterator.set_postfix({
                            'Acc': f'{acc*100:.1f}%',
                            'Ret': f'{ret_rate*100:.0f}%'
                        })
            
            except Exception as e:
                warnings.warn(f"å¤„ç†æ ·æœ¬å¤±è´¥: {e}")
                continue
        
        if verbose:
            acc = sum(r['correct'] for r in results) / len(results) if results else 0
            ret_rate = retrieval_triggered / len(results) if results else 0
            print(f"\nâœ… Pipelineå®Œæˆ:")
            print(f"  å‡†ç¡®ç‡: {acc*100:.2f}%")
            print(f"  æ£€ç´¢ç‡: {ret_rate*100:.1f}%")
            print(f"  å¤„ç†æ ·æœ¬: {len(results)}")
        
        return results


# å·¥å‚å‡½æ•°
def create_self_aware_pipeline_qwen3vl(qwen3_vl_wrapper, retriever, **kwargs):
    """
    åˆ›å»ºSelf-Aware Pipelineï¼ˆQwen3-VLç‰ˆæœ¬ï¼‰
    
    Args:
        qwen3_vl_wrapper: Qwen3-VLå°è£…å™¨
        retriever: æ£€ç´¢å™¨
        **kwargs: é…ç½®å‚æ•°
    
    Returns:
        SelfAwarePipelineQwen3VLå®ä¾‹
    """
    return SelfAwarePipelineQwen3VL(qwen3_vl_wrapper, retriever, kwargs)


if __name__ == '__main__':
    print("=" * 80)
    print("Self-Aware Multimodal RAG Pipeline - Qwen3-VLç‰ˆæœ¬")
    print("=" * 80)
    print()
    print("âœ… P0ä¿®å¤: ç»Ÿä¸€ä½¿ç”¨Qwen3-VL")
    print()
    print("ä¼˜åŠ¿ï¼š")
    print("  1. å…¬å¹³å¯¹æ¯”ï¼ˆæ‰€æœ‰æ–¹æ³•ç»Ÿä¸€æ¨¡å‹ï¼‰")
    print("  2. å¤šå›¾åƒæ”¯æŒï¼ˆæœ€å¤š20å¼ ï¼‰")
    print("  3. é«˜åˆ†è¾¨ç‡å¤„ç†")
    print("  4. æ›´å¼ºæŒ‡ä»¤è·Ÿéš")
    print()
    print("ä½¿ç”¨æ–¹æ³•:")
    print("  from flashrag.pipeline.self_aware_pipeline_qwen3vl import create_self_aware_pipeline_qwen3vl")
    print("  from flashrag.modules.qwen3_vl import create_qwen3_vl_wrapper")
    print()
    print("  qwen3_vl = create_qwen3_vl_wrapper()")
    print("  pipeline = create_self_aware_pipeline_qwen3vl(qwen3_vl, retriever)")
    print("  results = pipeline.run(dataset)")
    print("=" * 80)

