#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
BGE Reranker å°è£…
ç”¨äºæ–‡æ¡£é‡æ’
"""

import torch
from typing import List, Tuple
import warnings


class BGEReranker:
    """
    BGE Rerankerå°è£…
    ç”¨äºé‡æ’æ£€ç´¢æ–‡æ¡£ï¼Œæå‡ç›¸å…³æ€§
    """
    
    def __init__(self, model_name='BAAI/bge-reranker-v2-m3', device='cuda'):
        """
        åˆå§‹åŒ–BGE Reranker
        
        Args:
            model_name: æ¨¡å‹åç§°
            device: è®¾å¤‡
        """
        self.device = device
        self.model_name = model_name
        
        try:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            import os
            
            # âœ… ä½¿ç”¨HFé•œåƒåŠ é€Ÿä¸‹è½½
            os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
            
            # æ£€æŸ¥æœ¬åœ°è·¯å¾„
            local_path = f'/root/autodl-tmp/models/{model_name.split("/")[-1]}'
            if os.path.exists(local_path):
                print(f"âœ… ä½¿ç”¨æœ¬åœ°BGE Reranker: {local_path}")
                model_path = local_path
            else:
                print(f"ğŸ“¥ ä»HFé•œåƒä¸‹è½½BGE Reranker: {model_name}")
                model_path = model_name
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
            self.model = self.model.to(device)
            self.model.eval()
            print(f"âœ… BGE RerankeråŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âš ï¸  BGE RerankeråŠ è½½å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨mockæ¨¡å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰")
            self.model = None
            self.tokenizer = None
    
    def rerank(self, query: str, documents: List[str], top_k: int = 5) -> List[str]:
        """
        é‡æ’æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›top-kæ–‡æ¡£
            
        Returns:
            é‡æ’åçš„æ–‡æ¡£åˆ—è¡¨ï¼ˆtop-kï¼‰
        """
        if self.model is None:
            # Mockæ¨¡å¼ï¼šè¿”å›åŸå§‹æ–‡æ¡£
            return documents[:top_k]
        
        if not documents:
            return []
        
        try:
            # æ„é€ æŸ¥è¯¢-æ–‡æ¡£å¯¹
            pairs = [[query, doc] for doc in documents]
            
            # åˆ†æ‰¹å¤„ç†ï¼ˆé¿å…OOMï¼‰
            batch_size = 32
            all_scores = []
            
            for i in range(0, len(pairs), batch_size):
                batch_pairs = pairs[i:i+batch_size]
                
                with torch.no_grad():
                    inputs = self.tokenizer(
                        batch_pairs,
                        padding=True,
                        truncation=True,
                        return_tensors='pt',
                        max_length=512
                    ).to(self.device)
                    
                    scores = self.model(**inputs, return_dict=True).logits.squeeze(-1)
                    all_scores.extend(scores.cpu().tolist())
            
            # æ’åº
            scored_docs = list(zip(documents, all_scores))
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            
            # è¿”å›top-k
            return [doc for doc, score in scored_docs[:top_k]]
        
        except Exception as e:
            warnings.warn(f"é‡æ’å¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹é¡ºåº")
            return documents[:top_k]
    
    def score(self, query: str, document: str) -> float:
        """
        è®¡ç®—å•ä¸ªæ–‡æ¡£çš„ç›¸å…³æ€§åˆ†æ•°
        
        Args:
            query: æŸ¥è¯¢
            document: æ–‡æ¡£
            
        Returns:
            ç›¸å…³æ€§åˆ†æ•°
        """
        if self.model is None:
            return 0.5
        
        try:
            with torch.no_grad():
                inputs = self.tokenizer(
                    [[query, document]],
                    padding=True,
                    truncation=True,
                    return_tensors='pt',
                    max_length=512
                ).to(self.device)
                
                score = self.model(**inputs, return_dict=True).logits.squeeze(-1)
                return float(score.cpu().item())
        
        except Exception as e:
            warnings.warn(f"æ‰“åˆ†å¤±è´¥: {e}")
            return 0.5


def create_bge_reranker(model_name='BAAI/bge-reranker-v2-m3', device='cuda'):
    """åˆ›å»ºBGE Rerankerï¼ˆæ”¯æŒHFé•œåƒï¼‰"""
    return BGEReranker(model_name, device)


if __name__ == '__main__':
    # æµ‹è¯•
    print("æµ‹è¯•BGE Reranker")
    
    reranker = create_bge_reranker()
    
    query = "What is the capital of France?"
    docs = [
        "Paris is the capital and most populous city of France.",
        "London is the capital of the United Kingdom.",
        "The Eiffel Tower is located in Paris.",
        "France is a country in Western Europe."
    ]
    
    print(f"\næŸ¥è¯¢: {query}")
    print(f"åŸå§‹æ–‡æ¡£: {len(docs)}ä¸ª")
    
    reranked = reranker.rerank(query, docs, top_k=2)
    print(f"\né‡æ’åTop-2:")
    for i, doc in enumerate(reranked, 1):
        print(f"  {i}. {doc}")

