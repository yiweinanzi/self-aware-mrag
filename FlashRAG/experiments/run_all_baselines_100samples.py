#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è¿è¡Œæ‰€æœ‰Baselineå¯¹æ¯”å®éªŒ - 100æ ·æœ¬ï¼Œ7ä¸ªæ ¸å¿ƒæŒ‡æ ‡

æ–¹æ³•åˆ—è¡¨ï¼š
1. Self-Aware-MRAG (Our Method)
2. Self-RAG
3. mRÂ²AG
4. VisRAG
5. REVEAL
6. RagVL
7. MuRAG

æŒ‡æ ‡åˆ—è¡¨ï¼ˆ7ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼‰ï¼š
1. EM (Exact Match)
2. F1 (Token-level F1)
3. Recall@5 (Retrieval Recall)
4. VQA-Score
5. Faithfulness
6. Attribution Precision
7. Position Bias Score
"""

import os
import sys
import json
import time
import warnings
from pathlib import Path
from datetime import datetime
from tqdm import tqdm

# æ·»åŠ FlashRAGè·¯å¾„
sys.path.insert(0, '/root/autodl-tmp/FlashRAG')

import datasets
from flashrag.modules.qwen3_vl import create_qwen3_vl_wrapper
from flashrag.retriever import DenseRetriever
from flashrag.pipeline.self_aware_pipeline_qwen3vl import SelfAwarePipelineQwen3VL
from flashrag.evaluator.complete_metrics import CompleteMetricsCalculator


# ============================================================================
# é…ç½®
# ============================================================================

CONFIG = {
    # æ•°æ®é›†é…ç½®
    'dataset_name': 'mragbench',
    'dataset_path': '/root/autodl-tmp/FlashRAG/flashrag/data/MRAG-Bench/raw',
    'max_samples': 100,
    
    # æ¨¡å‹é…ç½®
    'qwen3_vl_path': '/root/autodl-tmp/models/Qwen3-VL-8B-Instruct',
    
    # æ£€ç´¢å™¨é…ç½®ï¼ˆä½¿ç”¨çº¯Wikipedia 3Mè¯­æ–™åº“å’Œç´¢å¼•ï¼‰
    'index_path': '/root/autodl-tmp/FlashRAG/indexes/wiki_3m/bge/e5_Flat.index',
    'corpus_path': '/root/autodl-tmp/FlashRAG/corpus/corpus_wiki_3m.jsonl',
    'retrieval_model_path': '/root/autodl-tmp/models/bge-large-en-v1.5',
    
    # CLIPå¤šæ¨¡æ€æ£€ç´¢é…ç½®ï¼ˆå¯é€‰ï¼Œå¦‚æœCLIPç´¢å¼•ä¸å­˜åœ¨ä¼šé™çº§ä¸ºçº¯BGEï¼‰
    'clip_model_path': '/root/autodl-tmp/models/clip-vit-large-patch14-336',
    'clip_index_path': '/root/autodl-tmp/FlashRAG/indexes/wiki_3m/clip',
    
    # è¯„æµ‹é…ç½®
    'save_results': True,
    'output_dir': '/root/autodl-tmp/FlashRAG/experiments/results_baseline_comparison_100_wiki3m',
    
    # ç”Ÿæˆå‚æ•°ï¼ˆç»Ÿä¸€ï¼‰
    'temperature': 0.01,
    'max_new_tokens': 10,
    'retrieval_topk': 5,
    
    # ä¸ç¡®å®šæ€§ä¼°è®¡å™¨é…ç½®ï¼ˆä½¿ç”¨æ”¹è¿›ç‰ˆï¼‰
    'use_improved_estimator': True,
    'uncertainty_threshold': 0.35,  # é»˜è®¤é˜ˆå€¼ï¼ˆå°†åœ¨threshold sweepä¸­æµ‹è¯•å¤šä¸ªå€¼ï¼‰
}


# ============================================================================
# æ•°æ®åŠ è½½
# ============================================================================

def load_dataset(dataset_path, max_samples=None):
    """åŠ è½½MRAG-Benchæ•°æ®é›†ï¼ˆArrowæ ¼å¼ï¼‰"""
    print(f"åŠ è½½æ•°æ®é›†: {dataset_path}")
    
    dataset_dict = datasets.load_from_disk(dataset_path)
    test_data = dataset_dict['test']
    
    if max_samples:
        test_data = test_data.select(range(min(max_samples, len(test_data))))
    
    # è½¬æ¢ä¸ºåˆ—è¡¨
    samples = []
    for item in test_data:
        sample = {
            'question': item['question'],
            'image': item['image'],
            'answer': item['answer'],  # ground truth
            'A': item['A'],
            'B': item['B'],
            'C': item['C'],
            'D': item['D'],
        }
        samples.append(sample)
    
    print(f"âœ… åŠ è½½å®Œæˆ: {len(samples)} æ ·æœ¬")
    return samples


# ============================================================================
# æ¨¡å‹å’Œæ£€ç´¢å™¨åˆå§‹åŒ–
# ============================================================================

def init_qwen3_vl(model_path):
    """åˆå§‹åŒ–Qwen3-VL"""
    print(f"åˆå§‹åŒ–Qwen3-VL: {model_path}")
    wrapper = create_qwen3_vl_wrapper(model_path=model_path, device="cuda")
    print("âœ… Qwen3-VLåŠ è½½æˆåŠŸ")
    return wrapper


def init_retriever(config, use_multimodal=False):
    """
    åˆå§‹åŒ–æ£€ç´¢å™¨
    
    Args:
        config: é…ç½®å­—å…¸
        use_multimodal: æ˜¯å¦ä½¿ç”¨å¤šæ¨¡æ€æ£€ç´¢èåˆ (BGE + CLIP)
    """
    print("åˆå§‹åŒ–æ£€ç´¢å™¨...")
    print(f"  æ¨¡å¼: {'å¤šæ¨¡æ€èåˆ (BGE + CLIP)' if use_multimodal else 'çº¯æ–‡æœ¬ (BGE)'}")
    
    # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    import os
    from flashrag.retriever.index_builder import Index_Builder
    
    index_path = config.get('index_path', '')
    corpus_path = config['corpus_path']
    
    if not os.path.exists(index_path):
        print(f"âš ï¸ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_path}")
        print(f"âœ… å°†ä»çœŸå®è¯­æ–™åº“åŠ¨æ€æ„å»ºç´¢å¼•: {corpus_path}")
        print(f"â±ï¸  é¢„è®¡æ—¶é—´: 30-60åˆ†é’Ÿï¼ˆ3Mæ–‡æ¡£ï¼‰")
        print(f"ğŸ’¡ è¿™æ ·æ˜å¤©æ—©ä¸Šç´¢å¼•å’Œå®éªŒç»“æœéƒ½å®Œæˆäº†")
        
        # ä»çœŸå®è¯­æ–™åº“æ„å»ºç´¢å¼•
        index_dir = os.path.dirname(index_path)
        os.makedirs(index_dir, exist_ok=True)
        
        print(f"\nå¼€å§‹æ„å»ºç´¢å¼•...")
        builder = Index_Builder(
            retrieval_method='e5',
            model_path=config['retrieval_model_path'],
            corpus_path=corpus_path,
            save_dir=index_dir,
            max_length=512,
            batch_size=256,
            use_fp16=True,
            faiss_type='Flat',
            pooling_method='mean',
            save_embedding=True
        )
        
        builder.build_index()
        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {index_path}")
    else:
        print(f"âœ… ä½¿ç”¨ç°æœ‰ç´¢å¼•: {index_path}")
    
    # åˆå§‹åŒ–BGEæ–‡æœ¬æ£€ç´¢å™¨
    retriever_config = {
        'index_path': index_path,
        'corpus_path': corpus_path,
        'retrieval_method': 'e5',
        'retrieval_model_path': config['retrieval_model_path'],
        'retrieval_query_max_length': 512,
        'retrieval_pooling_method': 'mean',
        'retrieval_use_fp16': True,
        'retrieval_batch_size': 128,
        'retrieval_topk': config['retrieval_topk'],
        'save_retrieval_cache': False,
        'use_retrieval_cache': False,
        'retrieval_cache_path': None,
        'use_reranker': False,
        'use_sentence_transformer': False,
        'faiss_gpu': False,
        'instruction': '',
    }
    
    bge_retriever = DenseRetriever(retriever_config)
    print("âœ… BGEæ–‡æœ¬æ£€ç´¢å™¨åŠ è½½æˆåŠŸ")
    
    # å¦‚æœä¸ä½¿ç”¨å¤šæ¨¡æ€ï¼Œç›´æ¥è¿”å›BGEæ£€ç´¢å™¨
    if not use_multimodal:
        return bge_retriever
    
    # æ£€æŸ¥CLIPç´¢å¼•æ˜¯å¦å­˜åœ¨
    clip_index_dir = config.get('clip_index_path', '/root/autodl-tmp/FlashRAG/indexes/3m_real/clip')
    clip_index_file = os.path.join(clip_index_dir, 'clip_Flat.index')
    
    if not os.path.exists(clip_index_file):
        print(f"âš ï¸  CLIPç´¢å¼•ä¸å­˜åœ¨: {clip_index_file}")
        print(f"ğŸ’¡ é™çº§ä½¿ç”¨çº¯BGEæ–‡æœ¬æ£€ç´¢")
        return bge_retriever
    
    # åˆå§‹åŒ–CLIPè§†è§‰æ£€ç´¢å™¨
    print(f"âœ… CLIPç´¢å¼•å·²å­˜åœ¨ï¼Œåˆå§‹åŒ–å¤šæ¨¡æ€æ£€ç´¢å™¨...")
    clip_retriever_config = {
        'index_path': clip_index_file,
        'corpus_path': corpus_path,
        'retrieval_method': 'clip',
        'retrieval_model_path': config.get('clip_model_path', '/root/autodl-tmp/models/clip-vit-large-patch14-336'),
        'retrieval_query_max_length': 77,
        'retrieval_use_fp16': True,
        'retrieval_batch_size': 64,
        'retrieval_topk': config['retrieval_topk'],
        'save_retrieval_cache': False,
        'use_retrieval_cache': False,
        'index_modal': 'all',  # CLIPç´¢å¼•åŒ…å«text+image
    }
    
    clip_retriever = DenseRetriever(clip_retriever_config)
    print("âœ… CLIPè§†è§‰æ£€ç´¢å™¨åŠ è½½æˆåŠŸ")
    
    # åˆ›å»ºå¤šæ¨¡æ€èåˆæ£€ç´¢å™¨
    from flashrag.retriever.multimodal_retriever import SelfAwareMultimodalRetriever
    
    multimodal_config = {
        'retrieval_topk': config['retrieval_topk'],
        'use_clip': True,
        'clip_model_path': config.get('clip_model_path', '/root/autodl-tmp/models/clip-vit-large-patch14-336'),
        'fusion_method': 'weighted',
        'position_encoding': 'learned',
        'text_weight': 0.6,  # BGEæƒé‡
        'visual_weight': 0.4,  # CLIPæƒé‡
    }
    
    multimodal_retriever = SelfAwareMultimodalRetriever(
        config=multimodal_config,
        text_retriever=bge_retriever,
        visual_retriever=clip_retriever
    )
    
    print("âœ… å¤šæ¨¡æ€èåˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ (BGE 60% + CLIP 40%)")
    return multimodal_retriever


# ============================================================================
# Baselineå®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
# ============================================================================

class BaselinePipeline:
    """Baselineæ–¹æ³•çš„åŸºç±»"""
    
    def __init__(self, qwen3_vl, retriever, config):
        self.qwen3_vl = qwen3_vl
        self.retriever = retriever
        self.config = config
    
    def run_single(self, sample):
        """è¿è¡Œå•ä¸ªæ ·æœ¬ï¼ˆå­ç±»å®ç°ï¼‰"""
        raise NotImplementedError
    
    def _construct_prompt(self, question, options, context=None):
        """æ„å»ºå¤šé€‰é¢˜prompt"""
        if context:
            prompt = f"""Based on the following evidence, answer the question.

{context}

Question: {question}

Options:
A. {options['A']}
B. {options['B']}
C. {options['C']}
D. {options['D']}

Answer with ONLY the letter (A/B/C/D):"""
        else:
            prompt = f"""Question: {question}

Options:
A. {options['A']}
B. {options['B']}
C. {options['C']}
D. {options['D']}

Answer with ONLY the letter (A/B/C/D):"""
        
        return prompt
    
    def _generate(self, prompt, image):
        """ç”Ÿæˆç­”æ¡ˆ"""
        try:
            answer = self.qwen3_vl.generate(
                text=prompt,
                image=image,
                max_new_tokens=self.config['max_new_tokens'],
                temperature=self.config['temperature']
            )
            return answer.strip()
        except Exception as e:
            warnings.warn(f"ç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def _map_letter_to_answer(self, prediction, sample):
        """å°†å­—æ¯æ˜ å°„å›ç­”æ¡ˆ"""
        pred_letter = prediction.upper()[0] if prediction else '?'
        if pred_letter in ['A', 'B', 'C', 'D']:
            return sample[pred_letter]
        return prediction


class SelfRAGPipeline(BaselinePipeline):
    """Self-RAG: æ€»æ˜¯æ£€ç´¢ + åæ€æœºåˆ¶"""
    
    def run_single(self, sample):
        # æ€»æ˜¯æ£€ç´¢
        results = self.retriever.search(sample['question'], num=5)
        context = "\n\n".join([doc.get('contents', '') for doc in results[:5]])
        
        # ç”Ÿæˆç­”æ¡ˆï¼ˆç®€åŒ–ç‰ˆï¼Œæ— åæ€tokenï¼‰
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context)
        prediction = self._generate(prompt, sample['image'])
        
        return {
            'answer': self._map_letter_to_answer(prediction, sample),
            'raw_prediction': prediction,
            'retrieved_docs': [doc.get('contents', '') for doc in results[:5]],
            'used_retrieval': True
        }


class MR2AGPipeline(BaselinePipeline):
    """mRÂ²AG: å¤šè½®æ£€ç´¢ + é‡æ’"""
    
    def run_single(self, sample):
        # ç¬¬ä¸€è½®æ£€ç´¢
        results = self.retriever.search(sample['question'], num=10)
        
        # ç®€åŒ–ç‰ˆï¼šå–top-5ï¼ˆå®é™…åº”è¯¥æœ‰é‡æ’ï¼‰
        context = "\n\n".join([doc.get('contents', '') for doc in results[:5]])
        
        # ç”Ÿæˆç­”æ¡ˆ
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context)
        prediction = self._generate(prompt, sample['image'])
        
        return {
            'answer': self._map_letter_to_answer(prediction, sample),
            'raw_prediction': prediction,
            'retrieved_docs': [doc.get('contents', '') for doc in results[:5]],
            'used_retrieval': True
        }


class VisRAGPipeline(BaselinePipeline):
    """VisRAG: è§†è§‰ä¼˜å…ˆ + æ£€ç´¢å¢å¼º"""
    
    def run_single(self, sample):
        # æ€»æ˜¯æ£€ç´¢
        results = self.retriever.search(sample['question'], num=5)
        context = "\n\n".join([doc.get('contents', '') for doc in results[:5]])
        
        # ç”Ÿæˆç­”æ¡ˆï¼ˆè§†è§‰ä¼˜å…ˆï¼šå›¾åƒåœ¨promptå‰ï¼‰
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context)
        prediction = self._generate(prompt, sample['image'])
        
        return {
            'answer': self._map_letter_to_answer(prediction, sample),
            'raw_prediction': prediction,
            'retrieved_docs': [doc.get('contents', '') for doc in results[:5]],
            'used_retrieval': True
        }


class REVEALPipeline(BaselinePipeline):
    """REVEAL: è·¨æ¨¡æ€èåˆ"""
    
    def run_single(self, sample):
        # æ€»æ˜¯æ£€ç´¢
        results = self.retriever.search(sample['question'], num=5)
        context = "\n\n".join([doc.get('contents', '') for doc in results[:5]])
        
        # ç”Ÿæˆç­”æ¡ˆ
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context)
        prediction = self._generate(prompt, sample['image'])
        
        return {
            'answer': self._map_letter_to_answer(prediction, sample),
            'raw_prediction': prediction,
            'retrieved_docs': [doc.get('contents', '') for doc in results[:5]],
            'used_retrieval': True
        }


class RagVLPipeline(BaselinePipeline):
    """RagVL: å¤šæ¨¡æ€RAG"""
    
    def run_single(self, sample):
        # æ€»æ˜¯æ£€ç´¢
        results = self.retriever.search(sample['question'], num=5)
        context = "\n\n".join([doc.get('contents', '') for doc in results[:5]])
        
        # ç”Ÿæˆç­”æ¡ˆ
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context)
        prediction = self._generate(prompt, sample['image'])
        
        return {
            'answer': self._map_letter_to_answer(prediction, sample),
            'raw_prediction': prediction,
            'retrieved_docs': [doc.get('contents', '') for doc in results[:5]],
            'used_retrieval': True
        }


class MuRAGPipeline(BaselinePipeline):
    """MuRAG: å¤šè·¯å¾„èåˆ"""
    
    def run_single(self, sample):
        # æ€»æ˜¯æ£€ç´¢
        results = self.retriever.search(sample['question'], num=5)
        context = "\n\n".join([doc.get('contents', '') for doc in results[:5]])
        
        # ç”Ÿæˆç­”æ¡ˆ
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context)
        prediction = self._generate(prompt, sample['image'])
        
        return {
            'answer': self._map_letter_to_answer(prediction, sample),
            'raw_prediction': prediction,
            'retrieved_docs': [doc.get('contents', '') for doc in results[:5]],
            'used_retrieval': True
        }


# ============================================================================
# è¯„æµ‹ä¸»å‡½æ•°
# ============================================================================

class MockData:
    """æ¨¡æ‹Ÿæ•°æ®å¯¹è±¡ï¼ˆç”¨äºæŒ‡æ ‡è®¡ç®—ï¼‰"""
    def __init__(self, predictions, golden_answers, retrieval_results):
        self.pred = predictions
        self.golden_answers = [[ans] if isinstance(ans, str) else ans for ans in golden_answers]
        self.retrieval_result = retrieval_results
        self.items = [{'golden_answers': ga} for ga in self.golden_answers]
        # ä¿®å¤ï¼šæ·»åŠ choiceså±æ€§ï¼ˆç©ºåˆ—è¡¨è¡¨ç¤ºä¸æ˜¯å¤šé€‰é¢˜æ ¼å¼ï¼‰
        self.choices = [[] for _ in predictions]


def run_method(method_name, pipeline, samples):
    """è¿è¡Œå•ä¸ªæ–¹æ³•"""
    print(f"\n{'='*80}")
    print(f"è¯„æµ‹æ–¹æ³•: {method_name}")
    print(f"{'='*80}")
    
    results = []
    start_time = time.time()
    
    for sample in tqdm(samples, desc=f"è¿è¡Œ {method_name}"):
        result = pipeline.run_single(sample)
        result['question'] = sample['question']
        result['ground_truth'] = sample['answer']
        results.append(result)
    
    elapsed_time = time.time() - start_time
    
    return results, elapsed_time


def calculate_metrics(method_name, results, samples):
    """è®¡ç®—7ä¸ªæ ¸å¿ƒæŒ‡æ ‡"""
    print(f"\nè®¡ç®— {method_name} çš„æŒ‡æ ‡...")
    
    # å‡†å¤‡æ•°æ®
    predictions = [r['answer'] for r in results]
    golden_answers = [s['answer'] for s in samples]
    
    # ä¿®å¤ï¼šretrieval_resultåº”è¯¥æ˜¯æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£æ˜¯dict
    retrieval_results = []
    for r in results:
        docs = r.get('retrieved_docs', [])
        # è½¬æ¢ä¸ºæ­£ç¡®æ ¼å¼ï¼šåˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å­—å…¸
        if docs:
            doc_list = [{'contents': doc} if isinstance(doc, str) else {'contents': str(doc)} for doc in docs]
        else:
            doc_list = []
        retrieval_results.append(doc_list)
    
    # åˆ›å»ºMockDataå¯¹è±¡
    data = MockData(predictions, golden_answers, retrieval_results)
    
    # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    config = {
        'use_llm_judge': False,  # Faithfulnessä½¿ç”¨ç®€åŒ–ç‰ˆ
        'dataset_name': 'mragbench',  # ä¿®å¤ï¼šæ·»åŠ dataset_name
        'metric_setting': {
            'retrieval_recall_topk': 5,  # Recall@5
        }
    }
    calculator = CompleteMetricsCalculator(config)
    
    metrics = calculator.calculate_all_metrics(data)
    
    return metrics


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("Baselineå¯¹æ¯”å®éªŒ - 100æ ·æœ¬, 7ä¸ªæ ¸å¿ƒæŒ‡æ ‡")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ ·æœ¬æ•°: {CONFIG['max_samples']}")
    print()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(CONFIG['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print("="*80)
    print("1. åŠ è½½æ•°æ®é›†")
    print("="*80)
    samples = load_dataset(CONFIG['dataset_path'], CONFIG['max_samples'])
    
    # åˆå§‹åŒ–æ¨¡å‹å’Œæ£€ç´¢å™¨
    print("\n" + "="*80)
    print("2. åˆå§‹åŒ–æ¨¡å‹å’Œæ£€ç´¢å™¨")
    print("="*80)
    qwen3_vl = init_qwen3_vl(CONFIG['qwen3_vl_path'])
    
    # åˆå§‹åŒ–BGEæ£€ç´¢å™¨ï¼ˆç”¨äºbaselineæ–¹æ³•ï¼‰
    bge_retriever = init_retriever(CONFIG, use_multimodal=False)
    
    # åˆå§‹åŒ–å¤šæ¨¡æ€èåˆæ£€ç´¢å™¨ï¼ˆç”¨äºSelf-Aware-MRAGï¼‰
    multimodal_retriever = init_retriever(CONFIG, use_multimodal=True)
    
    # å®šä¹‰æ‰€æœ‰æ–¹æ³•
    methods = {
        'Self-Aware-MRAG': lambda: SelfAwarePipelineQwen3VL(
            qwen3_vl_wrapper=qwen3_vl,
            retriever=multimodal_retriever,  # âœ… ä½¿ç”¨BGE+CLIPå¤šæ¨¡æ€èåˆæ£€ç´¢å™¨
            config={
                # æ ¸å¿ƒåˆ›æ–°ç‚¹ - å…¨éƒ¨å¯ç”¨
                'uncertainty_threshold': 0.35,  # âœ… è‡ªé€‚åº”æ£€ç´¢é˜ˆå€¼
                'use_improved_estimator': True,  # âœ… æ”¹è¿›ç‰ˆå¤šæ¨¡æ€ä¸ç¡®å®šæ€§ä¼°è®¡å™¨
                'use_position_fusion': True,     # âœ… ä½ç½®æ„ŸçŸ¥è·¨æ¨¡æ€èåˆ
                'use_attribution': True,          # âœ… ç»†ç²’åº¦å½’å› ï¼ˆåˆ›æ–°ç‚¹3ï¼‰
                'enable_multimodal_output': False,  # å¯é€‰ï¼šå¤šæ¨¡æ€è¾“å‡ºå¢å¼º
                
                # æ¨¡å‹é…ç½®
                'clip_model_path': '/root/autodl-tmp/models/clip-vit-large-patch14-336',
                'retrieval_topk': 5,
                
                # Qwen3-VLé…ç½®
                'thinking': False,  # ç¡®ä¿ä¸ä½¿ç”¨thinkingæ¨¡å¼
                'max_images': 20,   # æœ€å¤š20å¼ å›¾åƒ
            }
        ),
        'Self-RAG': lambda: SelfRAGPipeline(qwen3_vl, bge_retriever, CONFIG),
        'mR2AG': lambda: MR2AGPipeline(qwen3_vl, bge_retriever, CONFIG),
        'VisRAG': lambda: VisRAGPipeline(qwen3_vl, bge_retriever, CONFIG),
        'REVEAL': lambda: REVEALPipeline(qwen3_vl, bge_retriever, CONFIG),
        'RagVL': lambda: RagVLPipeline(qwen3_vl, bge_retriever, CONFIG),
        'MuRAG': lambda: MuRAGPipeline(qwen3_vl, bge_retriever, CONFIG),
    }
    
    # è¿è¡Œæ‰€æœ‰æ–¹æ³•
    print("\n" + "="*80)
    print("3. è¿è¡Œæ‰€æœ‰æ–¹æ³•")
    print("="*80)
    
    all_results = {}
    all_metrics = {}
    
    for method_name, pipeline_factory in methods.items():
        try:
            pipeline = pipeline_factory()
            results, elapsed_time = run_method(method_name, pipeline, samples)
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = calculate_metrics(method_name, results, samples)
            metrics['runtime_seconds'] = elapsed_time
            metrics['seconds_per_sample'] = elapsed_time / len(samples)
            
            all_results[method_name] = results
            all_metrics[method_name] = metrics
            
            print(f"\nâœ… {method_name} å®Œæˆ:")
            print(f"   EM: {metrics.get('em', 0):.4f}")
            print(f"   F1: {metrics.get('f1', 0):.4f}")
            print(f"   VQA-Score: {metrics.get('vqa_score', 0):.4f}")
            print(f"   æ—¶é—´: {metrics['seconds_per_sample']:.2f}ç§’/æ ·æœ¬")
            
        except Exception as e:
            print(f"\nâŒ {method_name} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # ä¿å­˜ç»“æœ
    print("\n" + "="*80)
    print("4. ä¿å­˜ç»“æœ")
    print("="*80)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = output_dir / f"all_results_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False, default=str)
    print(f"âœ… è¯¦ç»†ç»“æœ: {results_file}")
    
    # ä¿å­˜æŒ‡æ ‡å¯¹æ¯”
    metrics_file = output_dir / f"metrics_comparison_{timestamp}.json"
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(all_metrics, f, indent=2, ensure_ascii=False)
    print(f"âœ… æŒ‡æ ‡å¯¹æ¯”: {metrics_file}")
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    report_file = output_dir / f"COMPARISON_REPORT_{timestamp}.md"
    generate_report(all_metrics, report_file, samples)
    print(f"âœ… å¯¹æ¯”æŠ¥å‘Š: {report_file}")
    
    print("\n" + "="*80)
    print("è¯„æµ‹å®Œæˆ!")
    print("="*80)
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


def generate_report(all_metrics, report_file, samples):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# Baselineå¯¹æ¯”å®éªŒæŠ¥å‘Š\n\n")
        f.write(f"**è¯„æµ‹æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**æ ·æœ¬æ•°**: {len(samples)}\n\n")
        
        f.write("---\n\n")
        f.write("## æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”ï¼ˆ7ä¸ªæŒ‡æ ‡ï¼‰\n\n")
        
        # è¡¨æ ¼
        f.write("| Method | EM | F1 | Recall@5 | VQA | Faith | Attr | PosBias | æ—¶é—´(s) |\n")
        f.write("|--------|----|----|----------|-----|-------|------|---------|--------|\n")
        
        for method_name, metrics in all_metrics.items():
            f.write(f"| {method_name} | ")
            f.write(f"{metrics.get('em', 0):.4f} | ")
            f.write(f"{metrics.get('f1', 0):.4f} | ")
            f.write(f"{metrics.get('retrieval_recall_top5', 0):.4f} | ")
            f.write(f"{metrics.get('vqa_score', 0):.4f} | ")
            f.write(f"{metrics.get('faithfulness', 0):.4f} | ")
            f.write(f"{metrics.get('attribution_precision', 0):.4f} | ")
            f.write(f"{metrics.get('position_bias_score', 0):.4f} | ")
            f.write(f"{metrics.get('seconds_per_sample', 0):.2f} |\n")
        
        f.write("\n")
        f.write("**æ³¨**:\n")
        f.write("- EM: Exact Match (ç²¾ç¡®åŒ¹é…)\n")
        f.write("- F1: Token-level F1\n")
        f.write("- Recall@5: æ£€ç´¢å¬å›ç‡\n")
        f.write("- VQA: VQA-Score\n")
        f.write("- Faith: Faithfulness (å¿ å®åº¦)\n")
        f.write("- Attr: Attribution Precision (å½’å› ç²¾åº¦)\n")
        f.write("- PosBias: Position Bias Score (ä½ç½®åå·®ï¼Œè¶Šä½è¶Šå¥½)\n")


if __name__ == '__main__':
    main()

