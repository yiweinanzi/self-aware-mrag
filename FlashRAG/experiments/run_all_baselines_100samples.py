#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è¿è¡Œæ‰€æœ‰Baselineå¯¹æ¯”å®éªŒ - 100æ ·æœ¬ï¼Œ7ä¸ªæ ¸å¿ƒæŒ‡æ ‡

æ–¹æ³•åˆ—è¡¨ï¼š
1. Self-Aware-MRAG (Our Method)
2. Self-RAG
3. mRÂ²AG
4. VisRAG
5. REVEAL
6. RagVL
7. MuRAG

æŒ‡æ ‡åˆ—è¡¨ï¼ˆ7ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼‰ï¼š
1. EM (Exact Match)
2. F1 (Token-level F1)
3. Recall@5 (Retrieval Recall)
4. VQA-Score
5. Faithfulness
6. Attribution Precision
7. Position Bias Score
"""

import os
import sys
import json
import time
import warnings
from pathlib import Path
from datetime import datetime
from tqdm import tqdm

# æ·»åŠ FlashRAGè·¯å¾„
sys.path.insert(0, '/root/autodl-tmp/FlashRAG')

import datasets
from flashrag.modules.qwen3_vl import create_qwen3_vl_wrapper
from flashrag.retriever import DenseRetriever
from flashrag.pipeline.self_aware_pipeline_qwen3vl import SelfAwarePipelineQwen3VL
from flashrag.evaluator.complete_metrics import CompleteMetricsCalculator


# ============================================================================
# é…ç½®
# ============================================================================

CONFIG = {
    # æ•°æ®é›†é…ç½®
    'dataset_name': 'mragbench',
    'dataset_path': '/root/autodl-tmp/FlashRAG/flashrag/data/MRAG-Bench/raw',
    'max_samples': None,  # None = å…¨éƒ¨æ ·æœ¬(1353)
    
    # æ¨¡å‹é…ç½®
    'qwen3_vl_path': '/root/autodl-tmp/models/Qwen3-VL-8B-Instruct',
    
    # æ£€ç´¢å™¨é…ç½®ï¼ˆä½¿ç”¨çº¯Wikipedia 3Mè¯­æ–™åº“å’Œç´¢å¼•ï¼‰
    'index_path': '/root/autodl-tmp/FlashRAG/indexes/wiki_3m/bge/e5_Flat.index',
    'corpus_path': '/root/autodl-tmp/FlashRAG/corpus/corpus_wiki_3m.jsonl',
    'retrieval_model_path': '/root/autodl-tmp/models/bge-large-en-v1.5',
    
    # CLIPå¤šæ¨¡æ€æ£€ç´¢é…ç½®ï¼ˆå¯é€‰ï¼Œå¦‚æœCLIPç´¢å¼•ä¸å­˜åœ¨ä¼šé™çº§ä¸ºçº¯BGEï¼‰
    'clip_model_path': '/root/autodl-tmp/models/clip-vit-large-patch14-336',
    'clip_index_path': '/root/autodl-tmp/FlashRAG/indexes/wiki_3m/clip',
    
    # è¯„æµ‹é…ç½®
    'save_results': True,
    'output_dir': '/root/autodl-tmp/FlashRAG/experiments/results_baseline_comparison_100_wiki3m',
    
    # ç”Ÿæˆå‚æ•°ï¼ˆç»Ÿä¸€ï¼‰
    'temperature': 0.01,
    'max_new_tokens': 10,
    'retrieval_topk': 5,
    
    # ä¸ç¡®å®šæ€§ä¼°è®¡å™¨é…ç½®ï¼ˆâœ… ä½¿ç”¨è®ºæ–‡å®Œæ•´å®ç°ï¼‰
    'use_improved_estimator': False,  # âœ… ä¿®æ”¹: ä½¿ç”¨CrossModalUncertaintyEstimatorï¼ˆè®ºæ–‡æ‰¿è¯ºçš„å®Œæ•´å®ç°ï¼‰
    'uncertainty_threshold': 0.35,  # âœ… ä¼˜åŒ–ï¼šæ¢å¤åˆç†é˜ˆå€¼ (å¹³è¡¡æ£€ç´¢ä¸ç›´æ¥å›ç­”)
}


# ============================================================================
# æ•°æ®åŠ è½½
# ============================================================================

def load_dataset(dataset_path, max_samples=None):
    """åŠ è½½MRAG-Benchæ•°æ®é›†ï¼ˆArrowæ ¼å¼ï¼‰"""
    print(f"åŠ è½½æ•°æ®é›†: {dataset_path}")
    
    dataset_dict = datasets.load_from_disk(dataset_path)
    test_data = dataset_dict['test']
    
    if max_samples:
        test_data = test_data.select(range(min(max_samples, len(test_data))))
    
    # è½¬æ¢ä¸ºåˆ—è¡¨
    samples = []
    for item in test_data:
        sample = {
            'question': item['question'],
            'image': item['image'],
            'answer': item['answer'],  # ground truth
            'A': item['A'],
            'B': item['B'],
            'C': item['C'],
            'D': item['D'],
        }
        samples.append(sample)
    
    print(f"âœ… åŠ è½½å®Œæˆ: {len(samples)} æ ·æœ¬")
    return samples


# ============================================================================
# æ¨¡å‹å’Œæ£€ç´¢å™¨åˆå§‹åŒ–
# ============================================================================

def init_qwen3_vl(model_path):
    """åˆå§‹åŒ–Qwen3-VL"""
    print(f"åˆå§‹åŒ–Qwen3-VL: {model_path}")
    wrapper = create_qwen3_vl_wrapper(model_path=model_path, device="cuda")
    print("âœ… Qwen3-VLåŠ è½½æˆåŠŸ")
    return wrapper


def init_retriever(config, use_multimodal=False):
    """
    åˆå§‹åŒ–æ£€ç´¢å™¨
    
    Args:
        config: é…ç½®å­—å…¸
        use_multimodal: æ˜¯å¦ä½¿ç”¨å¤šæ¨¡æ€æ£€ç´¢èåˆ (BGE + CLIP)
    """
    print("åˆå§‹åŒ–æ£€ç´¢å™¨...")
    print(f"  æ¨¡å¼: {'å¤šæ¨¡æ€èåˆ (BGE + CLIP)' if use_multimodal else 'çº¯æ–‡æœ¬ (BGE)'}")
    
    # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    import os
    from flashrag.retriever.index_builder import Index_Builder
    
    index_path = config.get('index_path', '')
    corpus_path = config['corpus_path']
    
    if not os.path.exists(index_path):
        print(f"âš ï¸ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_path}")
        print(f"âœ… å°†ä»çœŸå®è¯­æ–™åº“åŠ¨æ€æ„å»ºç´¢å¼•: {corpus_path}")
        print(f"â±ï¸  é¢„è®¡æ—¶é—´: 30-60åˆ†é’Ÿï¼ˆ3Mæ–‡æ¡£ï¼‰")
        print(f"ğŸ’¡ è¿™æ ·æ˜å¤©æ—©ä¸Šç´¢å¼•å’Œå®éªŒç»“æœéƒ½å®Œæˆäº†")
        
        # ä»çœŸå®è¯­æ–™åº“æ„å»ºç´¢å¼•
        index_dir = os.path.dirname(index_path)
        os.makedirs(index_dir, exist_ok=True)
        
        print(f"\nå¼€å§‹æ„å»ºç´¢å¼•...")
        builder = Index_Builder(
            retrieval_method='e5',
            model_path=config['retrieval_model_path'],
            corpus_path=corpus_path,
            save_dir=index_dir,
            max_length=512,
            batch_size=256,
            use_fp16=True,
            faiss_type='Flat',
            pooling_method='mean',
            save_embedding=True
        )
        
        builder.build_index()
        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {index_path}")
    else:
        print(f"âœ… ä½¿ç”¨ç°æœ‰ç´¢å¼•: {index_path}")
    
    # åˆå§‹åŒ–BGEæ–‡æœ¬æ£€ç´¢å™¨
    retriever_config = {
        'index_path': index_path,
        'corpus_path': corpus_path,
        'retrieval_method': 'e5',
        'retrieval_model_path': config['retrieval_model_path'],
        'retrieval_query_max_length': 512,
        'retrieval_pooling_method': 'mean',
        'retrieval_use_fp16': True,
        'retrieval_batch_size': 128,
        'retrieval_topk': config['retrieval_topk'],
        'save_retrieval_cache': False,
        'use_retrieval_cache': False,
        'retrieval_cache_path': None,
        'use_reranker': False,
        'use_sentence_transformer': False,
        'faiss_gpu': False,
        'instruction': '',
    }
    
    bge_retriever = DenseRetriever(retriever_config)
    print("âœ… BGEæ–‡æœ¬æ£€ç´¢å™¨åŠ è½½æˆåŠŸ")
    
    # å¦‚æœä¸ä½¿ç”¨å¤šæ¨¡æ€ï¼Œç›´æ¥è¿”å›BGEæ£€ç´¢å™¨
    if not use_multimodal:
        return bge_retriever
    
    # æ£€æŸ¥CLIPç´¢å¼•æ˜¯å¦å­˜åœ¨
    clip_index_dir = config.get('clip_index_path', '/root/autodl-tmp/FlashRAG/indexes/3m_real/clip')
    clip_index_file = os.path.join(clip_index_dir, 'clip_Flat.index')
    
    if not os.path.exists(clip_index_file):
        print(f"âš ï¸  CLIPç´¢å¼•ä¸å­˜åœ¨: {clip_index_file}")
        print(f"ğŸ’¡ é™çº§ä½¿ç”¨çº¯BGEæ–‡æœ¬æ£€ç´¢")
        return bge_retriever
    
    # åˆå§‹åŒ–CLIPè§†è§‰æ£€ç´¢å™¨
    print(f"âœ… CLIPç´¢å¼•å·²å­˜åœ¨ï¼Œåˆå§‹åŒ–å¤šæ¨¡æ€æ£€ç´¢å™¨...")
    clip_retriever_config = {
        'index_path': clip_index_file,
        'corpus_path': corpus_path,
        'retrieval_method': 'clip',
        'retrieval_model_path': config.get('clip_model_path', '/root/autodl-tmp/models/clip-vit-large-patch14-336'),
        'retrieval_query_max_length': 77,
        'retrieval_use_fp16': True,
        'retrieval_batch_size': 64,
        'retrieval_topk': config['retrieval_topk'],
        'save_retrieval_cache': False,
        'use_retrieval_cache': False,
        'index_modal': 'all',  # CLIPç´¢å¼•åŒ…å«text+image
    }
    
    clip_retriever = DenseRetriever(clip_retriever_config)
    print("âœ… CLIPè§†è§‰æ£€ç´¢å™¨åŠ è½½æˆåŠŸ")
    
    # åˆ›å»ºå¤šæ¨¡æ€èåˆæ£€ç´¢å™¨
    from flashrag.retriever.multimodal_retriever import SelfAwareMultimodalRetriever
    
    multimodal_config = {
        'retrieval_topk': config['retrieval_topk'],
        'use_clip': True,
        'clip_model_path': config.get('clip_model_path', '/root/autodl-tmp/models/clip-vit-large-patch14-336'),
        'fusion_method': 'weighted',
        'position_encoding': 'learned',
        'text_weight': 0.6,  # BGEæƒé‡
        'visual_weight': 0.4,  # CLIPæƒé‡
    }
    
    multimodal_retriever = SelfAwareMultimodalRetriever(
        config=multimodal_config,
        text_retriever=bge_retriever,
        visual_retriever=clip_retriever
    )
    
    print("âœ… å¤šæ¨¡æ€èåˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ (BGE 60% + CLIP 40%)")
    return multimodal_retriever


# ============================================================================
# Baselineå®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
# ============================================================================

class BaselinePipeline:
    """Baselineæ–¹æ³•çš„åŸºç±»"""
    
    def __init__(self, qwen3_vl, retriever, config):
        self.qwen3_vl = qwen3_vl
        self.retriever = retriever
        self.config = config
    
    def run_single(self, sample):
        """è¿è¡Œå•ä¸ªæ ·æœ¬ï¼ˆå­ç±»å®ç°ï¼‰"""
        raise NotImplementedError
    
    def _construct_prompt(self, question, options, context=None):
        """æ„å»ºå¤šé€‰é¢˜prompt"""
        if context:
            prompt = f"""Based on the following evidence, answer the question.

{context}

Question: {question}

Options:
A. {options['A']}
B. {options['B']}
C. {options['C']}
D. {options['D']}

Answer with ONLY the letter (A/B/C/D):"""
        else:
            prompt = f"""Question: {question}

Options:
A. {options['A']}
B. {options['B']}
C. {options['C']}
D. {options['D']}

Answer with ONLY the letter (A/B/C/D):"""
        
        return prompt
    
    def _generate(self, prompt, image):
        """ç”Ÿæˆç­”æ¡ˆ"""
        try:
            answer = self.qwen3_vl.generate(
                text=prompt,
                image=image,
                max_new_tokens=self.config['max_new_tokens'],
                temperature=self.config['temperature']
            )
            return answer.strip()
        except Exception as e:
            warnings.warn(f"ç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def _map_letter_to_answer(self, prediction, sample):
        """å°†å­—æ¯æ˜ å°„å›ç­”æ¡ˆ"""
        pred_letter = prediction.upper()[0] if prediction else '?'
        if pred_letter in ['A', 'B', 'C', 'D']:
            return sample[pred_letter]
        return prediction
    
    def _add_evaluator_fields(self, result, retrieved_docs=None):
        """
        âœ… Task 1: æ·»åŠ evaluatoréœ€è¦çš„å­—æ®µ
        
        æ‰€æœ‰baselineéƒ½éœ€è¦æ·»åŠ è¿™äº›å­—æ®µä»¥æ”¯æŒå®Œæ•´çš„7ä¸ªæŒ‡æ ‡è¯„ä¼°
        """
        if retrieved_docs is None:
            retrieved_docs = result.get('retrieved_docs', [])
        
        # 1. retrieval_result - ç”¨äºFaithfulnessè®¡ç®—
        result['retrieval_result'] = [{
            'retrieved_docs': retrieved_docs,
            'retrieval_scores': [1.0] * len(retrieved_docs),
            'retrieval_used': len(retrieved_docs) > 0
        }]
        
        # 2. attributions - ç”¨äºAttribution Precisionè®¡ç®—
        # ç®€åŒ–ç‰ˆï¼šbaselineæš‚æ—¶ä¸æ”¯æŒç»†ç²’åº¦å½’å› 
        if 'attributions' not in result:
            result['attributions'] = {
                'visual': [],
                'text': []
            }
        
        # 3. position_bias_results - ç”¨äºPosition Bias Scoreè®¡ç®—
        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨ç»Ÿä¸€çš„ä½ç½®åå·®
        if 'position_bias_results' not in result:
            result['position_bias_results'] = {
                'average_bias': 0.0,
                'individual_scores': [0.0],
                'position_weights': []
            }
        
        return result


class SelfRAGPipeline(BaselinePipeline):
    """
    âœ… Self-RAG: å®Œæ•´å®ç°ï¼ˆåŸºäºQwen3-VLï¼‰
    
    å®ç°äº†Self-RAGçš„3ä¸ªæ ¸å¿ƒåˆ¤æ–­ï¼š
    1. Retrieval Decision: åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢
    2. Relevance Judgment: åˆ¤æ–­æ–‡æ¡£æ˜¯å¦ç›¸å…³
    3. Support Judgment: åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦è¢«æ”¯æŒ
    """
    
    def __init__(self, qwen3_vl, retriever, config):
        super().__init__(qwen3_vl, retriever, config)
        self.decision_temp = 0.05  # åˆ¤æ–­æ¸©åº¦ï¼ˆä½æ¸©åº¦=æ›´ç¡®å®šï¼‰
    
    def run_single(self, sample):
        """è¿è¡Œå•ä¸ªæ ·æœ¬ - å®Œæ•´çš„Self-RAGæµç¨‹"""
        question = sample['question']
        image = sample.get('image')
        
        # === Step 1: Retrieval Decision ===
        need_retrieval = self._retrieval_decision(question, image)
        
        if not need_retrieval:
            # æ— éœ€æ£€ç´¢ï¼Œç›´æ¥å›ç­”
            answer = self._direct_answer(sample)
            result = {
                'answer': answer,
                'raw_prediction': answer,
                'retrieved_docs': [],
                'used_retrieval': False,
                'retrieval_decision': 'No Retrieval',
                'relevant_docs_count': 0,
                'support_status': 'N/A'
            }
            return self._add_evaluator_fields(result)
        
        # === Step 2: Retrieve Documents ===
        results = self.retriever.search(question, num=self.config.get('retrieval_topk', 5))
        
        if not results:
            answer = self._direct_answer(sample)
            result = {
                'answer': answer,
                'raw_prediction': answer,
                'retrieved_docs': [],
                'used_retrieval': True,
                'retrieval_decision': 'Retrieval (no docs)',
                'relevant_docs_count': 0,
                'support_status': 'N/A'
            }
            return self._add_evaluator_fields(result)
        
        # === Step 3: Relevance Judgment ===
        relevant_docs = []
        for doc in results[:5]:
            doc_text = doc.get('contents', '')
            if self._relevance_judgment(question, doc_text, image):
                relevant_docs.append(doc_text)
        
        if not relevant_docs:
            # æ— ç›¸å…³æ–‡æ¡£ï¼Œé™çº§åˆ°ç›´æ¥å›ç­”
            answer = self._direct_answer(sample)
            result = {
                'answer': answer,
                'raw_prediction': answer,
                'retrieved_docs': [doc.get('contents', '') for doc in results[:5]],
                'used_retrieval': True,
                'retrieval_decision': 'Retrieval (no relevant)',
                'relevant_docs_count': 0,
                'support_status': 'No'
            }
            return self._add_evaluator_fields(result)
        
        # === Step 4: Generate Answer ===
        answer = self._generate_with_context(sample, relevant_docs[:3])
        
        # === Step 5: Support Judgment ===
        is_supported = self._support_judgment(question, answer, relevant_docs[:3])
        
        if not is_supported:
            # ç­”æ¡ˆä¸è¢«æ”¯æŒï¼Œä½†ä»ä½¿ç”¨è¯¥ç­”æ¡ˆï¼ˆè®°å½•çŠ¶æ€ï¼‰
            support_status = 'Not Supported'
        else:
            support_status = 'Supported'
        
        result = {
            'answer': answer,
            'raw_prediction': answer,
            'retrieved_docs': relevant_docs,
            'used_retrieval': True,
            'retrieval_decision': 'Retrieval',
            'relevant_docs_count': len(relevant_docs),
            'support_status': support_status
        }
        
        return self._add_evaluator_fields(result)
    
    def _retrieval_decision(self, question: str, image=None) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢ï¼ˆæ¨¡æ‹Ÿ[Retrieval] tokenï¼‰"""
        prompt = f"""Task: Decide if external knowledge is needed to answer this question.

Question: {question}

Think: Can this be answered just by looking at the image, or does it require external factual knowledge (dates, names, locations, etc.)?

Answer ONLY 'NEED' or 'NO':"""
        
        try:
            response = self.qwen3_vl.generate(
                text=prompt,
                image=image,
                max_new_tokens=5,
                temperature=self.decision_temp
            )
            
            response_upper = response.strip().upper()
            return 'NEED' in response_upper and 'NO' not in response_upper[:4]
        except:
            return True  # é»˜è®¤æ£€ç´¢ï¼ˆä¿å®ˆï¼‰
    
    def _relevance_judgment(self, question: str, document: str, image=None) -> bool:
        """åˆ¤æ–­æ–‡æ¡£æ˜¯å¦ç›¸å…³ï¼ˆæ¨¡æ‹Ÿ[IsREL] tokenï¼‰"""
        doc_preview = document[:300] + "..." if len(document) > 300 else document
        
        prompt = f"""Task: Is this document relevant to the question?

Question: {question}

Document: {doc_preview}

Answer ONLY 'RELEVANT' or 'IRRELEVANT':"""
        
        try:
            response = self.qwen3_vl.generate(
                text=prompt,
                image=None,  # çº¯æ–‡æœ¬åˆ¤æ–­
                max_new_tokens=5,
                temperature=self.decision_temp
            )
            
            response_upper = response.strip().upper()
            return 'RELEVANT' in response_upper and 'IRRELEVANT' not in response_upper
        except:
            return True  # é»˜è®¤ç›¸å…³ï¼ˆä¿å®ˆï¼‰
    
    def _support_judgment(self, question: str, answer: str, documents: list) -> bool:
        """åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦è¢«æ–‡æ¡£æ”¯æŒï¼ˆæ¨¡æ‹Ÿ[IsSUP] tokenï¼‰"""
        context = "\n\n".join(documents)[:400]
        
        prompt = f"""Task: Is the answer supported by the context?

Context: {context}...

Question: {question}
Answer: {answer}

Answer ONLY 'SUPPORTED' or 'NOT_SUPPORTED':"""
        
        try:
            response = self.qwen3_vl.generate(
                text=prompt,
                image=None,
                max_new_tokens=5,
                temperature=self.decision_temp
            )
            
            response_upper = response.strip().upper().replace(' ', '_')
            return 'SUPPORTED' in response_upper and 'NOT' not in response_upper[:3]
        except:
            return True  # é»˜è®¤æ”¯æŒï¼ˆä¿å®ˆï¼‰
    
    def _generate_with_context(self, sample, relevant_docs):
        """åŸºäºç›¸å…³æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ"""
        context = "\n\n".join(relevant_docs)
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context)
        prediction = self._generate(prompt, sample['image'])
        return self._map_letter_to_answer(prediction, sample)
    
    def _direct_answer(self, sample):
        """ç›´æ¥å›ç­”ï¼ˆæ— æ£€ç´¢ï¼‰"""
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context=None)
        prediction = self._generate(prompt, sample['image'])
        return self._map_letter_to_answer(prediction, sample)


class MR2AGPipeline(BaselinePipeline):
    """
    âœ… mRÂ²AG: å®Œæ•´å®ç°ï¼ˆåŸºäºQwen3-VLï¼‰
    
    å®ç°äº†mRÂ²AGçš„æ ¸å¿ƒç‰¹è‰²ï¼š
    1. Retrieval-Reflection: åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢
    2. æ®µè½çº§å¤„ç†: å°†æ–‡æ¡£åˆ‡åˆ†ä¸ºå°æ®µè½ï¼ˆ50-180 tokensï¼‰
    3. Relevance-Reflection: é€æ®µè½åˆ¤æ–­ç›¸å…³æ€§
    4. å±‚çº§æ‰“åˆ†: S_ret Ã— S_rel Ã— S_ans
    """
    
    def __init__(self, qwen3_vl, retriever, config):
        super().__init__(qwen3_vl, retriever, config)
        self.para_min_len = 50
        self.para_max_len = 180
    
    def run_single(self, sample):
        """è¿è¡Œå•ä¸ªæ ·æœ¬ - å®Œæ•´çš„mRÂ²AGæµç¨‹"""
        question = sample['question']
        image = sample.get('image')
        
        # === Step 1: Retrieval-Reflection ===
        need_retrieval = self._retrieval_reflection(question, image)
        
        if not need_retrieval:
            answer = self._direct_answer(sample)
            result = {
                'answer': answer,
                'raw_prediction': answer,
                'retrieved_docs': [],
                'used_retrieval': False,
                'retrieval_decision': 'No Retrieval',
                'total_paragraphs': 0,
                'relevant_paragraphs': 0
            }
            return self._add_evaluator_fields(result)
        
        # === Step 2: æ£€ç´¢æ–‡æ¡£ ===
        results = self.retriever.search(question, num=10)  # å¤šæ£€ç´¢ä¸€äº›
        
        if not results:
            answer = self._direct_answer(sample)
            result = {
                'answer': answer,
                'raw_prediction': answer,
                'retrieved_docs': [],
                'used_retrieval': True,
                'retrieval_decision': 'Retrieval (no docs)',
                'total_paragraphs': 0,
                'relevant_paragraphs': 0
            }
            return self._add_evaluator_fields(result)
        
        # === Step 3: æ®µè½çº§å¤„ç†ï¼ˆmRÂ²AGæ ¸å¿ƒç‰¹è‰²ï¼‰===
        candidates = []
        total_paras = 0
        all_docs = []
        
        for entry_idx, entry in enumerate(results[:5]):
            doc_text = entry.get('contents', '')
            all_docs.append(doc_text)
            
            # åˆ‡åˆ†ä¸ºæ®µè½
            paragraphs = self._split_paragraphs(doc_text)
            total_paras += len(paragraphs)
            
            for para in paragraphs:
                # Relevance-Reflectionï¼ˆæ®µè½çº§åˆ¤æ–­ï¼‰
                is_relevant, rel_score = self._relevance_reflection(question, para)
                
                if is_relevant:
                    # åŸºäºè¯¥æ®µè½ç”Ÿæˆç­”æ¡ˆ
                    answer = self._generate_with_paragraph(sample, para)
                    
                    # å±‚çº§æ‰“åˆ†: S_ret Ã— S_rel Ã— S_ans
                    ret_score = 0.9 ** entry_idx  # æ£€ç´¢åˆ†æ•°ï¼ˆæ’åè¡°å‡ï¼‰
                    ans_score = 0.8  # ç­”æ¡ˆç½®ä¿¡åº¦ï¼ˆç®€åŒ–ï¼‰
                    total_score = ret_score * rel_score * ans_score
                    
                    candidates.append({
                        'answer': answer,
                        'score': total_score,
                        'paragraph': para
                    })
        
        # === Step 4: é€‰æ‹©æœ€ä½³å€™é€‰ç­”æ¡ˆ ===
        if candidates:
            best = max(candidates, key=lambda x: x['score'])
            final_answer = best['answer']
        else:
            # æ— ç›¸å…³æ®µè½ï¼Œå›é€€åˆ°ä½¿ç”¨å…¨éƒ¨æ–‡æ¡£
            context = "\n\n".join(all_docs[:3])
            final_answer = self._generate_with_context(sample, context)
        
        result = {
            'answer': final_answer,
            'raw_prediction': final_answer,
            'retrieved_docs': all_docs,
            'used_retrieval': True,
            'retrieval_decision': 'Retrieval',
            'total_paragraphs': total_paras,
            'relevant_paragraphs': len(candidates)
        }
        
        return self._add_evaluator_fields(result)
    
    def _retrieval_reflection(self, question: str, image=None) -> bool:
        """Retrieval-Reflection: åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢"""
        prompt = f"""Decide if external knowledge is needed.

Question: {question}

Answer ONLY 'NEED' or 'NO':"""
        
        try:
            response = self.qwen3_vl.generate(
                text=prompt,
                image=image,
                max_new_tokens=5,
                temperature=0.05
            )
            return 'NEED' in response.upper()
        except:
            return True
    
    def _split_paragraphs(self, text: str) -> list:
        """æ®µè½åˆ‡åˆ†ï¼ˆmRÂ²AGçš„æ ¸å¿ƒç‰¹è‰²ï¼‰"""
        sentences = [s.strip() + '.' for s in text.split('.') if s.strip()]
        
        paragraphs = []
        current = ""
        
        for sent in sentences:
            if len(current) + len(sent) < self.para_max_len:
                current += " " + sent
            else:
                if len(current) > self.para_min_len:
                    paragraphs.append(current.strip())
                current = sent
        
        if len(current) > self.para_min_len:
            paragraphs.append(current.strip())
        
        return paragraphs if paragraphs else [text[:self.para_max_len]]
    
    def _relevance_reflection(self, question: str, paragraph: str) -> tuple:
        """Relevance-Reflection: æ®µè½ç›¸å…³æ€§åˆ¤æ–­"""
        prompt = f"""Rate relevance (0-10).

Question: {question}

Paragraph: {paragraph[:200]}...

Score (0-10):"""
        
        try:
            response = self.qwen3_vl.generate(
                text=prompt,
                image=None,
                max_new_tokens=5,
                temperature=0.1
            )
            try:
                score = float(response.strip()) / 10.0
            except:
                score = 0.5
            
            return (score > 0.5, score)
        except:
            return (True, 0.5)
    
    def _generate_with_paragraph(self, sample, paragraph):
        """åŸºäºå•ä¸ªæ®µè½ç”Ÿæˆç­”æ¡ˆ"""
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        
        prompt = f"""Based on this paragraph, answer the question.

Paragraph: {paragraph}

Question: {sample['question']}

Choices:
A. {sample['A']}
B. {sample['B']}
C. {sample['C']}
D. {sample['D']}

Answer with the letter only:"""
        
        prediction = self._generate(prompt, sample['image'])
        return self._map_letter_to_answer(prediction, sample)
    
    def _generate_with_context(self, sample, context):
        """åŸºäºå®Œæ•´contextç”Ÿæˆç­”æ¡ˆï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context)
        prediction = self._generate(prompt, sample['image'])
        return self._map_letter_to_answer(prediction, sample)
    
    def _direct_answer(self, sample):
        """ç›´æ¥å›ç­”ï¼ˆæ— æ£€ç´¢ï¼‰"""
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context=None)
        prediction = self._generate(prompt, sample['image'])
        return self._map_letter_to_answer(prediction, sample)


class VisRAGPipeline(BaselinePipeline):
    """
    âœ… VisRAG: å®Œæ•´å®ç°ï¼ˆåŸºäºBGE Rerankerï¼‰
    
    å®ç°äº†VisRAGçš„æ ¸å¿ƒç‰¹è‰²ï¼š
    1. åˆå§‹æ£€ç´¢ (top-10)
    2. BGEé‡æ’ (top-5) - æå‡æ£€ç´¢è´¨é‡
    3. è§†è§‰ä¼˜å…ˆç­–ç•¥
    """
    
    def __init__(self, qwen3_vl, retriever, config):
        super().__init__(qwen3_vl, retriever, config)
        self.initial_topk = 10
        self.final_topk = 5
        self.bge_reranker = None
        
        # å°è¯•åŠ è½½BGE Reranker
        try:
            from flashrag.modules.bge_reranker import create_bge_reranker
            self.bge_reranker = create_bge_reranker()
            print("âœ… VisRAG: BGE Rerankerå·²åŠ è½½")
        except Exception as e:
            print(f"âš ï¸ VisRAG: BGE RerankeråŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆ: {e}")
    
    def run_single(self, sample):
        """è¿è¡Œå•ä¸ªæ ·æœ¬ - å®Œæ•´çš„VisRAGæµç¨‹"""
        question = sample['question']
        image = sample.get('image')
        
        # === Step 1: åˆå§‹æ£€ç´¢ (top-10) ===
        initial_results = self.retriever.search(question, num=self.initial_topk)
        
        if not initial_results:
            # æ— æ£€ç´¢ç»“æœï¼Œç›´æ¥å›ç­”
            answer = self._direct_answer(sample)
            result = {
                'answer': answer,
                'raw_prediction': answer,
                'retrieved_docs': [],
                'used_retrieval': False,
                'reranker_used': False,
                'initial_docs': 0,
                'final_docs': 0
            }
            return self._add_evaluator_fields(result)
        
        # æå–æ–‡æ¡£æ–‡æœ¬
        docs_text = [doc.get('contents', '') for doc in initial_results]
        
        # === Step 2: BGEé‡æ’ (top-5) ===
        reranked_docs = self._rerank_documents(question, docs_text)
        
        # === Step 3: èåˆç”Ÿæˆ ===
        answer = self._generate_with_reranked_context(sample, reranked_docs)
        
        result = {
            'answer': answer,
            'raw_prediction': answer,
            'retrieved_docs': reranked_docs,  # ä½¿ç”¨é‡æ’åçš„æ–‡æ¡£
            'used_retrieval': True,
            'reranker_used': (self.bge_reranker is not None),
            'initial_docs': len(docs_text),
            'final_docs': len(reranked_docs)
        }
        
        return self._add_evaluator_fields(result)
    
    def _rerank_documents(self, question: str, documents: list) -> list:
        """BGEé‡æ’æ–‡æ¡£ï¼ˆVisRAGçš„æ ¸å¿ƒç‰¹è‰²ï¼‰"""
        if self.bge_reranker is None:
            # æ— rerankerï¼Œè¿”å›åŸå§‹top-k
            return documents[:self.final_topk]
        
        try:
            # ä½¿ç”¨BGEé‡æ’
            reranked = self.bge_reranker.rerank(
                query=question,
                documents=documents,
                top_k=self.final_topk
            )
            return reranked
        except Exception as e:
            print(f"âš ï¸ VisRAGé‡æ’å¤±è´¥: {e}")
            return documents[:self.final_topk]
    
    def _generate_with_reranked_context(self, sample, reranked_docs):
        """åŸºäºé‡æ’åçš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ"""
        if not reranked_docs:
            return self._direct_answer(sample)
        
        context = "\n\n".join(reranked_docs)
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        
        prompt = f"""Using the high-quality context below (reranked for relevance), answer the question.

Context:
{context}

Question: {sample['question']}

Choices:
A. {sample['A']}
B. {sample['B']}
C. {sample['C']}
D. {sample['D']}

Answer with the letter only:"""
        
        prediction = self._generate(prompt, sample['image'])
        return self._map_letter_to_answer(prediction, sample)
    
    def _direct_answer(self, sample):
        """ç›´æ¥å›ç­”ï¼ˆæ— æ£€ç´¢ï¼‰"""
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context=None)
        prediction = self._generate(prompt, sample['image'])
        return self._map_letter_to_answer(prediction, sample)


class REVEALPipeline(BaselinePipeline):
    """
    âœ… REVEAL: å®Œæ•´å®ç°ï¼ˆä¸¤é˜¶æ®µæ¨ç†ï¼‰
    
    å®ç°äº†REVEALçš„æ ¸å¿ƒç‰¹è‰²ï¼š
    1. æ£€ç´¢è¯æ®
    2. ç”Ÿæˆæ¨ç†è¿‡ç¨‹ (Reasoning) - ç¬¬ä¸€é˜¶æ®µ
    3. åŸºäºæ¨ç†ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ (Answer) - ç¬¬äºŒé˜¶æ®µ
    """
    
    def __init__(self, qwen3_vl, retriever, config):
        super().__init__(qwen3_vl, retriever, config)
        self.top_k = 5
        self.reasoning_temp = 0.3  # æ¨ç†é˜¶æ®µå…è®¸æ›´é«˜æ¸©åº¦
    
    def run_single(self, sample):
        """è¿è¡Œå•ä¸ªæ ·æœ¬ - å®Œæ•´çš„REVEALæµç¨‹"""
        question = sample['question']
        image = sample.get('image')
        
        # === Step 1: æ£€ç´¢è¯æ® ===
        results = self.retriever.search(question, num=self.top_k)
        
        if not results:
            answer = self._direct_answer(sample)
            result = {
                'answer': answer,
                'raw_prediction': answer,
                'retrieved_docs': [],
                'used_retrieval': False,
                'reasoning': ''
            }
            return self._add_evaluator_fields(result)
        
        docs_text = [doc.get('contents', '') for doc in results]
        context = "\n\n".join(docs_text)
        
        # === Step 2: ç”Ÿæˆæ¨ç†è¿‡ç¨‹ï¼ˆREVEALæ ¸å¿ƒç‰¹è‰²ï¼‰===
        reasoning = self._generate_reasoning(sample, context)
        
        # === Step 3: åŸºäºæ¨ç†ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ ===
        answer = self._generate_final_answer(sample, context, reasoning)
        
        result = {
            'answer': answer,
            'raw_prediction': answer,
            'retrieved_docs': docs_text,
            'used_retrieval': True,
            'reasoning': reasoning  # ä¿å­˜æ¨ç†è¿‡ç¨‹
        }
        
        return self._add_evaluator_fields(result)
    
    def _generate_reasoning(self, sample, context):
        """Stage 1: ç”Ÿæˆæ¨ç†è¿‡ç¨‹ï¼ˆREVEALæ ¸å¿ƒï¼‰"""
        prompt = f"""Given the evidence below, provide step-by-step reasoning for answering the question.

Evidence:
{context[:500]}...

Question: {sample['question']}

Step-by-step reasoning (2-3 sentences):"""
        
        try:
            reasoning = self.qwen3_vl.generate(
                text=prompt,
                image=sample.get('image'),
                max_new_tokens=100,
                temperature=self.reasoning_temp,  # å…è®¸æ¨ç†å¤šæ ·æ€§
                do_sample=True
            )
            return reasoning.strip()
        except:
            return "Based on the evidence provided."
    
    def _generate_final_answer(self, sample, context, reasoning):
        """Stage 2: åŸºäºæ¨ç†ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        
        prompt = f"""Based on the reasoning below, provide the final answer.

Question: {sample['question']}

Reasoning: {reasoning}

Choices:
A. {sample['A']}
B. {sample['B']}
C. {sample['C']}
D. {sample['D']}

Final answer (letter only):"""
        
        prediction = self._generate(prompt, sample['image'])
        return self._map_letter_to_answer(prediction, sample)
    
    def _direct_answer(self, sample):
        """ç›´æ¥å›ç­”ï¼ˆæ— æ£€ç´¢ï¼‰"""
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context=None)
        prediction = self._generate(prompt, sample['image'])
        return self._map_letter_to_answer(prediction, sample)


class RagVLPipeline(BaselinePipeline):
    """
    âœ… RagVL: å®Œæ•´å®ç°ï¼ˆMLLMä½œä¸ºå¼ºRerankerï¼‰
    
    å®ç°äº†RagVLçš„æ ¸å¿ƒç‰¹è‰²ï¼š
    1. ç²—æ£€ç´¢ (top-20)
    2. MLLM Reranking (é€‰top-3) - æ ¸å¿ƒåˆ›æ–°ï¼
    3. ç”Ÿæˆç­”æ¡ˆ
    
    åŸºäºè®ºæ–‡: MLLM Is a Strong Reranker (arXiv:2407.21439)
    """
    
    def __init__(self, qwen3_vl, retriever, config):
        super().__init__(qwen3_vl, retriever, config)
        self.clip_topk = 20  # ç²—æ£€ç´¢
        self.rerank_topk = 3  # ç²¾æ’åºåä¿ç•™
        self.use_reranking = True
    
    def run_single(self, sample):
        """è¿è¡Œå•ä¸ªæ ·æœ¬ - å®Œæ•´çš„RagVLæµç¨‹"""
        question = sample['question']
        image = sample.get('image')
        
        # === Step 1: ç²—æ£€ç´¢ (top-20) ===
        initial_results = self.retriever.search(question, num=self.clip_topk)
        
        if not initial_results:
            answer = self._direct_answer(sample)
            result = {
                'answer': answer,
                'raw_prediction': answer,
                'retrieved_docs': [],
                'used_retrieval': False,
                'reranked_count': 0
            }
            return self._add_evaluator_fields(result)
        
        docs_text = [doc.get('contents', '') for doc in initial_results]
        retrieval_scores = [1.0 - i*0.05 for i in range(len(docs_text))]
        
        # === Step 2: MLLM Rerankingï¼ˆRagVLæ ¸å¿ƒç‰¹è‰²ï¼‰===
        if self.use_reranking:
            reranked_docs = self._rerank_documents(
                question, docs_text, retrieval_scores, image
            )
        else:
            reranked_docs = [(doc, score) for doc, score in 
                           zip(docs_text[:self.rerank_topk], 
                               retrieval_scores[:self.rerank_topk])]
        
        # === Step 3: ç”Ÿæˆç­”æ¡ˆ ===
        answer = self._generate_with_reranked(sample, reranked_docs)
        
        result = {
            'answer': answer,
            'raw_prediction': answer,
            'retrieved_docs': [doc for doc, _ in reranked_docs],
            'used_retrieval': True,
            'initial_count': len(docs_text),
            'reranked_count': len(reranked_docs),
            'used_reranking': self.use_reranking
        }
        
        return self._add_evaluator_fields(result)
    
    def _rerank_single(self, question, doc, image=None):
        """ä½¿ç”¨MLLMåˆ¤æ–­å•ä¸ªæ–‡æ¡£çš„ç›¸å…³æ€§ï¼ˆRagVLæ ¸å¿ƒï¼‰"""
        prompt = f"""Is this document relevant to answering the question?

Document: {doc[:200]}...

Question: {question}

Answer with ONLY 'Yes' or 'No':"""
        
        try:
            response = self.qwen3_vl.generate(
                text=prompt,
                image=image,
                max_new_tokens=5,
                temperature=0.1
            )
            
            response_lower = response.strip().lower()
            
            if 'yes' in response_lower:
                return True, 0.9
            elif 'no' in response_lower:
                return False, 0.1
            else:
                return True, 0.5
        except:
            return True, 0.5
    
    def _rerank_documents(self, question, retrieved_docs, retrieval_scores, image=None):
        """å¯¹æ£€ç´¢ç»“æœè¿›è¡Œrerankingï¼ˆRagVLçš„æ ¸å¿ƒåˆ›æ–°ï¼‰"""
        reranked = []
        
        for doc, ret_score in zip(retrieved_docs, retrieval_scores):
            is_relevant, rel_score = self._rerank_single(question, doc, image)
            
            if is_relevant:
                # ç»¼åˆåˆ†æ•°ï¼šæ£€ç´¢åˆ†æ•° Ã— ç›¸å…³æ€§åˆ†æ•°
                combined_score = ret_score * rel_score
                reranked.append((doc, combined_score))
        
        # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
        reranked.sort(key=lambda x: x[1], reverse=True)
        
        # åªä¿ç•™Top-N
        return reranked[:self.rerank_topk]
    
    def _generate_with_reranked(self, sample, reranked_docs):
        """åŸºäºrerankåçš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ"""
        if not reranked_docs:
            return self._direct_answer(sample)
        
        # ç»„ç»‡è¯æ®
        evidence_parts = []
        for i, (doc, score) in enumerate(reranked_docs):
            evidence_parts.append(f"[Evidence {i+1}]\n{doc}")
        
        evidence_str = "\n\n".join(evidence_parts)
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        
        prompt = f"""Use the following high-quality evidence (filtered by reranking) to answer the question.

Evidence:
{evidence_str}

Question: {sample['question']}

Choices:
A. {sample['A']}
B. {sample['B']}
C. {sample['C']}
D. {sample['D']}

Answer with the letter only:"""
        
        prediction = self._generate(prompt, sample['image'])
        return self._map_letter_to_answer(prediction, sample)
    
    def _direct_answer(self, sample):
        """ç›´æ¥å›ç­”ï¼ˆæ— æ£€ç´¢ï¼‰"""
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context=None)
        prediction = self._generate(prompt, sample['image'])
        return self._map_letter_to_answer(prediction, sample)


class MuRAGPipeline(BaselinePipeline):
    """
    âœ… MuRAG: å®Œæ•´å®ç°ï¼ˆFiDå¼å¹¶è¡Œå¤„ç† + æŠ•ç¥¨èåˆï¼‰
    
    å®ç°äº†MuRAGçš„æ ¸å¿ƒç‰¹è‰²ï¼š
    1. æ£€ç´¢å¤šä¸ªè¯æ®ï¼ˆtop-10ï¼‰
    2. æ¯ä¸ªè¯æ®ç‹¬ç«‹ç”Ÿæˆç­”æ¡ˆï¼ˆFiDé£æ ¼ï¼‰- æ ¸å¿ƒåˆ›æ–°ï¼
    3. æŠ•ç¥¨èåˆé€‰æ‹©æœ€ç»ˆç­”æ¡ˆ
    """
    
    def __init__(self, qwen3_vl, retriever, config):
        super().__init__(qwen3_vl, retriever, config)
        self.top_k = 10  # æ£€ç´¢æ›´å¤šå€™é€‰
        self.ensemble_k = 5  # ç”¨äºæŠ•ç¥¨çš„è¯æ®æ•°
    
    def run_single(self, sample):
        """è¿è¡Œå•ä¸ªæ ·æœ¬ - å®Œæ•´çš„MuRAGæµç¨‹"""
        question = sample['question']
        image = sample.get('image')
        
        # === Step 1: æ£€ç´¢å¤šä¸ªè¯æ® ===
        results = self.retriever.search(question, num=self.top_k)
        
        if not results:
            answer = self._direct_answer(sample)
            result = {
                'answer': answer,
                'raw_prediction': answer,
                'retrieved_docs': [],
                'used_retrieval': False,
                'sub_answers': []
            }
            return self._add_evaluator_fields(result)
        
        docs_text = [doc.get('contents', '') for doc in results]
        
        # === Step 2: FiDå¼å¹¶è¡Œå¤„ç†ï¼ˆMuRAGæ ¸å¿ƒç‰¹è‰²ï¼‰===
        sub_answers = []
        for doc in docs_text[:self.ensemble_k]:
            sub_ans = self._generate_with_single_doc(sample, doc)
            if sub_ans:
                sub_answers.append(sub_ans)
        
        # === Step 3: æŠ•ç¥¨èåˆï¼ˆMuRAGæ ¸å¿ƒç‰¹è‰²ï¼‰===
        if sub_answers:
            answer = self._voting_fusion(sub_answers)
        else:
            answer = self._direct_answer(sample)
        
        result = {
            'answer': answer,
            'raw_prediction': answer,
            'retrieved_docs': docs_text[:self.ensemble_k],
            'used_retrieval': True,
            'sub_answers': sub_answers,  # ä¿å­˜æ‰€æœ‰å­ç­”æ¡ˆ
            'ensemble_size': len(sub_answers)
        }
        
        return self._add_evaluator_fields(result)
    
    def _generate_with_single_doc(self, sample, doc):
        """åŸºäºå•ä¸ªæ–‡æ¡£ç‹¬ç«‹ç”Ÿæˆç­”æ¡ˆï¼ˆFiDé£æ ¼ï¼ŒMuRAGæ ¸å¿ƒï¼‰"""
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        
        prompt = f"""Based ONLY on this single evidence document, answer the question.

Evidence: {doc[:300]}...

Question: {sample['question']}

Choices:
A. {sample['A']}
B. {sample['B']}
C. {sample['C']}
D. {sample['D']}

Answer (letter only):"""
        
        try:
            prediction = self._generate(prompt, sample.get('image'))
            return self._map_letter_to_answer(prediction, sample)
        except:
            return ""
    
    def _voting_fusion(self, sub_answers):
        """æŠ•ç¥¨èåˆï¼ˆMuRAGæ ¸å¿ƒç‰¹è‰²ï¼‰"""
        from collections import Counter
        
        # ç»Ÿè®¡ç­”æ¡ˆé¢‘ç‡
        answer_counts = Counter(sub_answers)
        
        # è¿”å›æœ€å¸¸è§çš„ç­”æ¡ˆ
        if answer_counts:
            most_common = answer_counts.most_common(1)[0]
            return most_common[0]
        
        return sub_answers[0] if sub_answers else ""
    
    def _direct_answer(self, sample):
        """ç›´æ¥å›ç­”ï¼ˆæ— æ£€ç´¢ï¼‰"""
        options = {'A': sample['A'], 'B': sample['B'], 'C': sample['C'], 'D': sample['D']}
        prompt = self._construct_prompt(sample['question'], options, context=None)
        prediction = self._generate(prompt, sample['image'])
        return self._map_letter_to_answer(prediction, sample)


# ============================================================================
# è¯„æµ‹ä¸»å‡½æ•°
# ============================================================================

class MockData:
    """æ¨¡æ‹Ÿæ•°æ®å¯¹è±¡ï¼ˆç”¨äºæŒ‡æ ‡è®¡ç®—ï¼‰"""
    def __init__(self, predictions, golden_answers, retrieval_results):
        self.pred = predictions
        self.golden_answers = [[ans] if isinstance(ans, str) else ans for ans in golden_answers]
        self.retrieval_result = retrieval_results
        self.items = [{'golden_answers': ga} for ga in self.golden_answers]
        # ä¿®å¤ï¼šæ·»åŠ choiceså±æ€§ï¼ˆç©ºåˆ—è¡¨è¡¨ç¤ºä¸æ˜¯å¤šé€‰é¢˜æ ¼å¼ï¼‰
        self.choices = [[] for _ in predictions]


def run_method(method_name, pipeline, samples):
    """è¿è¡Œå•ä¸ªæ–¹æ³•"""
    print(f"\n{'='*80}")
    print(f"è¯„æµ‹æ–¹æ³•: {method_name}")
    print(f"{'='*80}")
    
    results = []
    start_time = time.time()
    
    for sample in tqdm(samples, desc=f"è¿è¡Œ {method_name}"):
        result = pipeline.run_single(sample)
        result['question'] = sample['question']
        result['ground_truth'] = sample['answer']
        results.append(result)
    
    elapsed_time = time.time() - start_time
    
    return results, elapsed_time


def calculate_metrics(method_name, results, samples):
    """è®¡ç®—7ä¸ªæ ¸å¿ƒæŒ‡æ ‡"""
    print(f"\nè®¡ç®— {method_name} çš„æŒ‡æ ‡...")
    
    # å‡†å¤‡æ•°æ®
    predictions = [r['answer'] for r in results]
    golden_answers = [s['answer'] for s in samples]
    
    # ä¿®å¤ï¼šretrieval_resultåº”è¯¥æ˜¯æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£æ˜¯dict
    retrieval_results = []
    for r in results:
        docs = r.get('retrieved_docs', [])
        # è½¬æ¢ä¸ºæ­£ç¡®æ ¼å¼ï¼šåˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å­—å…¸
        if docs:
            doc_list = [{'contents': doc} if isinstance(doc, str) else {'contents': str(doc)} for doc in docs]
        else:
            doc_list = []
        retrieval_results.append(doc_list)
    
    # åˆ›å»ºMockDataå¯¹è±¡
    data = MockData(predictions, golden_answers, retrieval_results)
    
    # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    config = {
        'use_llm_judge': False,  # Faithfulnessä½¿ç”¨ç®€åŒ–ç‰ˆ
        'dataset_name': 'mragbench',  # ä¿®å¤ï¼šæ·»åŠ dataset_name
        'metric_setting': {
            'retrieval_recall_topk': 5,  # Recall@5
        }
    }
    calculator = CompleteMetricsCalculator(config)
    
    metrics = calculator.calculate_all_metrics(data)
    
    return metrics


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("Baselineå¯¹æ¯”å®éªŒ - MRAG-Benchå…¨æ•°æ®é›†, 7ä¸ªæ ¸å¿ƒæŒ‡æ ‡")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    max_samples_display = CONFIG['max_samples'] if CONFIG['max_samples'] else "å…¨éƒ¨(1353)"
    print(f"æ ·æœ¬æ•°: {max_samples_display}")
    print()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(CONFIG['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print("="*80)
    print("1. åŠ è½½æ•°æ®é›†")
    print("="*80)
    samples = load_dataset(CONFIG['dataset_path'], CONFIG['max_samples'])
    
    # åˆå§‹åŒ–æ¨¡å‹å’Œæ£€ç´¢å™¨
    print("\n" + "="*80)
    print("2. åˆå§‹åŒ–æ¨¡å‹å’Œæ£€ç´¢å™¨")
    print("="*80)
    qwen3_vl = init_qwen3_vl(CONFIG['qwen3_vl_path'])
    
    # åˆå§‹åŒ–BGEæ£€ç´¢å™¨ï¼ˆç”¨äºbaselineæ–¹æ³•ï¼‰
    bge_retriever = init_retriever(CONFIG, use_multimodal=False)
    
    # åˆå§‹åŒ–å¤šæ¨¡æ€èåˆæ£€ç´¢å™¨ï¼ˆç”¨äºSelf-Aware-MRAGï¼‰
    multimodal_retriever = init_retriever(CONFIG, use_multimodal=True)
    
    # å®šä¹‰æ‰€æœ‰æ–¹æ³•
    methods = {
        'Self-Aware-MRAG': lambda: SelfAwarePipelineQwen3VL(
            qwen3_vl_wrapper=qwen3_vl,
            retriever=multimodal_retriever,  # âœ… ä½¿ç”¨BGE+CLIPå¤šæ¨¡æ€èåˆæ£€ç´¢å™¨
            config={
                # æ ¸å¿ƒåˆ›æ–°ç‚¹ - å…¨éƒ¨å¯ç”¨ï¼ˆâœ… è®ºæ–‡å®Œæ•´å®ç°ï¼‰
                'uncertainty_threshold': 0.35,  # âœ… ä¼˜åŒ–ï¼šæ¢å¤åˆç†é˜ˆå€¼
                'use_improved_estimator': False,  # âœ… ä¿®æ”¹ï¼šä½¿ç”¨CrossModalUncertaintyEstimatorï¼ˆGramçŸ©é˜µ+eigen_score+JSæ•£åº¦ï¼‰
                'use_position_fusion': True,     # âœ… ä½ç½®æ„ŸçŸ¥è·¨æ¨¡æ€èåˆ
                'use_attribution': True,          # âœ… å¯ç”¨Attributionï¼ˆä¸ºevaluatoræä¾›æ•°æ®ï¼‰
                'enable_multimodal_output': False,  # å¯é€‰ï¼šå¤šæ¨¡æ€è¾“å‡ºå¢å¼º
                
                # æ¨¡å‹é…ç½®
                'clip_model_path': '/root/autodl-tmp/models/clip-vit-large-patch14-336',
                'retrieval_topk': 5,
                
                # Qwen3-VLé…ç½®
                'thinking': False,  # ç¡®ä¿ä¸ä½¿ç”¨thinkingæ¨¡å¼
                'max_images': 20,   # æœ€å¤š20å¼ å›¾åƒ
            }
        ),
        'Self-RAG': lambda: SelfRAGPipeline(qwen3_vl, bge_retriever, CONFIG),
        'mR2AG': lambda: MR2AGPipeline(qwen3_vl, bge_retriever, CONFIG),
        'VisRAG': lambda: VisRAGPipeline(qwen3_vl, bge_retriever, CONFIG),
        'REVEAL': lambda: REVEALPipeline(qwen3_vl, bge_retriever, CONFIG),
        'RagVL': lambda: RagVLPipeline(qwen3_vl, bge_retriever, CONFIG),
        'MuRAG': lambda: MuRAGPipeline(qwen3_vl, bge_retriever, CONFIG),
    }
    
    # è¿è¡Œæ‰€æœ‰æ–¹æ³•
    print("\n" + "="*80)
    print("3. è¿è¡Œæ‰€æœ‰æ–¹æ³•")
    print("="*80)
    
    all_results = {}
    all_metrics = {}
    
    for method_name, pipeline_factory in methods.items():
        try:
            pipeline = pipeline_factory()
            results, elapsed_time = run_method(method_name, pipeline, samples)
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = calculate_metrics(method_name, results, samples)
            metrics['runtime_seconds'] = elapsed_time
            metrics['seconds_per_sample'] = elapsed_time / len(samples)
            
            all_results[method_name] = results
            all_metrics[method_name] = metrics
            
            print(f"\nâœ… {method_name} å®Œæˆ:")
            print(f"   EM: {metrics.get('em', 0):.4f}")
            print(f"   F1: {metrics.get('f1', 0):.4f}")
            print(f"   VQA-Score: {metrics.get('vqa_score', 0):.4f}")
            print(f"   æ—¶é—´: {metrics['seconds_per_sample']:.2f}ç§’/æ ·æœ¬")
            
        except Exception as e:
            print(f"\nâŒ {method_name} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # ä¿å­˜ç»“æœ
    print("\n" + "="*80)
    print("4. ä¿å­˜ç»“æœ")
    print("="*80)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = output_dir / f"all_results_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False, default=str)
    print(f"âœ… è¯¦ç»†ç»“æœ: {results_file}")
    
    # ä¿å­˜æŒ‡æ ‡å¯¹æ¯”
    metrics_file = output_dir / f"metrics_comparison_{timestamp}.json"
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(all_metrics, f, indent=2, ensure_ascii=False)
    print(f"âœ… æŒ‡æ ‡å¯¹æ¯”: {metrics_file}")
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    report_file = output_dir / f"COMPARISON_REPORT_{timestamp}.md"
    generate_report(all_metrics, report_file, samples)
    print(f"âœ… å¯¹æ¯”æŠ¥å‘Š: {report_file}")
    
    print("\n" + "="*80)
    print("è¯„æµ‹å®Œæˆ!")
    print("="*80)
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


def generate_report(all_metrics, report_file, samples):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# Baselineå¯¹æ¯”å®éªŒæŠ¥å‘Š\n\n")
        f.write(f"**è¯„æµ‹æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**æ ·æœ¬æ•°**: {len(samples)}\n\n")
        
        f.write("---\n\n")
        f.write("## æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”ï¼ˆ7ä¸ªæŒ‡æ ‡ï¼‰\n\n")
        
        # è¡¨æ ¼
        f.write("| Method | EM | F1 | Recall@5 | VQA | Faith | Attr | PosBias | æ—¶é—´(s) |\n")
        f.write("|--------|----|----|----------|-----|-------|------|---------|--------|\n")
        
        for method_name, metrics in all_metrics.items():
            f.write(f"| {method_name} | ")
            f.write(f"{metrics.get('em', 0):.4f} | ")
            f.write(f"{metrics.get('f1', 0):.4f} | ")
            f.write(f"{metrics.get('retrieval_recall_top5', 0):.4f} | ")
            f.write(f"{metrics.get('vqa_score', 0):.4f} | ")
            f.write(f"{metrics.get('faithfulness', 0):.4f} | ")
            f.write(f"{metrics.get('attribution_precision', 0):.4f} | ")
            f.write(f"{metrics.get('position_bias_score', 0):.4f} | ")
            f.write(f"{metrics.get('seconds_per_sample', 0):.2f} |\n")
        
        f.write("\n")
        f.write("**æ³¨**:\n")
        f.write("- EM: Exact Match (ç²¾ç¡®åŒ¹é…)\n")
        f.write("- F1: Token-level F1\n")
        f.write("- Recall@5: æ£€ç´¢å¬å›ç‡\n")
        f.write("- VQA: VQA-Score\n")
        f.write("- Faith: Faithfulness (å¿ å®åº¦)\n")
        f.write("- Attr: Attribution Precision (å½’å› ç²¾åº¦)\n")
        f.write("- PosBias: Position Bias Score (ä½ç½®åå·®ï¼Œè¶Šä½è¶Šå¥½)\n")


if __name__ == '__main__':
    main()

