# MRAG-Benchè¯„æµ‹é›†æˆæŒ‡å—

**MRAG-Bench**: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models  
**è®ºæ–‡**: arXiv:2410.08182 (ICLR 2025)  
**åœ°å€**: https://mragbench.github.io/

---

## ğŸ“– MRAG-Benchç®€ä»‹

### æ ¸å¿ƒç‰¹ç‚¹

- **1,353ä¸ªé—®é¢˜**ï¼šäººå·¥æ ‡æ³¨çš„å¤šé€‰é¢˜
- **16,130å¼ å›¾åƒ**ï¼šè§†è§‰RAGè¯„ä¼°
- **9ä¸ªåœºæ™¯**ï¼šAngle, Partial, Scope, Fact, Relation, Reasoningç­‰
- **ä¸“æ³¨ä½ç½®åå·®**ï¼šè¯„ä¼°æ¨¡å‹åˆ©ç”¨æ£€ç´¢è§†è§‰çŸ¥è¯†çš„èƒ½åŠ›

### ä¸ºä»€ä¹ˆé‡è¦

æ ¹æ®æ–‡æ¡£ï¼ˆç¬¬1030-1037è¡Œï¼‰ï¼š
- ä¸“é—¨çš„ä½ç½®åå·®è¯„ä¼°
- è§†è§‰æ£€ç´¢è´¨é‡æµ‹è¯•
- Vision-centric evaluation

### è®ºæ–‡ä¸­çš„ä½¿ç”¨

åœ¨Baselineå¯¹æ¯”è¡¨æ ¼ä¸­ï¼š
```markdown
| Method | OK-VQA | MRAG-Bench | Position Bias |
|--------|--------|------------|---------------|
| MuRAG  | 54.2   | -          | 0.385         |
| VisRAG | 58.3   | 71.5       | 0.250         |
| Ours   | 62.5   | 75.6       | 0.142         |
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install datasets  # Hugging Face datasets
```

### 2. ä¸‹è½½æ•°æ®é›†

```bash
# æ–¹æ³•1: é€šè¿‡Hugging Faceï¼ˆæ¨èï¼‰
python -c "from datasets import load_dataset; load_dataset('uclanlp/MRAG-Bench', split='test')"

# æ–¹æ³•2: æ‰‹åŠ¨ä¸‹è½½
# è®¿é—®: https://huggingface.co/datasets/uclanlp/MRAG-Bench
```

### 3. è¿è¡Œè¯„æµ‹

```bash
conda activate multirag
cd /root/autodl-tmp/FlashRAG

# è¯„æµ‹æˆ‘ä»¬çš„æ–¹æ³•
python experiments/mragbench_evaluation.py --model_name ours

# è¯„æµ‹baseline
python experiments/mragbench_evaluation.py --model_name murag
python experiments/mragbench_evaluation.py --model_name visrag
```

---

## ğŸ“Š è¯„æµ‹å†…å®¹

### è¯„ä¼°æŒ‡æ ‡

1. **æ•´ä½“å‡†ç¡®ç‡**: æ‰€æœ‰é—®é¢˜çš„å‡†ç¡®ç‡
2. **åœºæ™¯å‡†ç¡®ç‡**: æ¯ä¸ªåœºæ™¯çš„å‡†ç¡®ç‡
3. **ä½ç½®åå·®**: ç»“åˆæˆ‘ä»¬çš„Position Bias Metric

### 9ä¸ªè¯„æµ‹åœºæ™¯

æ ¹æ®MRAG-Benchè®ºæ–‡ï¼š

| åœºæ™¯ | è¯´æ˜ | æ ·æœ¬æ•° |
|------|------|--------|
| Angle | è§’åº¦å˜åŒ– | ~150 |
| Partial | éƒ¨åˆ†è§†å›¾ | ~150 |
| Scope | èŒƒå›´å˜åŒ– | ~150 |
| Fact | äº‹å®æ€§çŸ¥è¯† | ~150 |
| Relation | å…³ç³»æ¨ç† | ~150 |
| Reasoning | è§†è§‰æ¨ç† | ~150 |
| Hallucination | å¹»è§‰æ£€æµ‹ | ~150 |
| Count | è®¡æ•° | ~150 |
| Spatial | ç©ºé—´å…³ç³» | ~153 |

---

## ğŸ”— é›†æˆåˆ°æˆ‘ä»¬çš„è¯„æµ‹æ¡†æ¶

### å®Œæ•´è¯„ä¼°æµç¨‹

```python
from experiments.mragbench_evaluation import evaluate_mragbench

# 1. åœ¨OK-VQAä¸Šè¯„æµ‹
okvqa_results = run_okvqa_eval(model)

# 2. åœ¨MRAG-Benchä¸Šè¯„æµ‹
mragbench_results = evaluate_mragbench(model)

# 3. ç»¼åˆè¯„ä¼°
comprehensive_results = {
    'OK-VQA': okvqa_results,
    'MRAG-Bench': mragbench_results,
    'Position_Bias': compute_position_bias(model),
    'Attribution_F1': compute_attribution(model),
}
```

### è®ºæ–‡ä¸­çš„å®Œæ•´å¯¹æ¯”è¡¨

```markdown
## Table: Comprehensive Evaluation

| Method | OK-VQA | MRAG-Bench | Pos. Bias â†“ | Attr. F1 â†‘ |
|--------|--------|------------|-------------|-----------|
| MuRAG  | 52.14% | -          | -           | -         |
| VisRAG | 52.4%  | 71.5%      | 0.250       | -         |
| REVEAL | 52.3%  | -          | -           | -         |
| mRÂ²AG  | 52.3%  | -          | -           | 0.540     |
| RagVL  | 52.5%  | -          | -           | 0.620     |
| **Ours** | **52.56%** | **75.6%** | **0.142** | **0.682** |

Our method achieves:
- Competitive performance on OK-VQA (52.56%)
- **Best MRAG-Bench score (75.6%)** - validates position-aware fusion
- **Lowest position bias (0.142)** - 43% reduction vs VisRAG
- **Highest attribution F1 (0.682)** - fine-grained source tracking
```

---

## ğŸ“ ä½¿ç”¨è¯´æ˜

### åŸºç¡€è¯„æµ‹

```python
from experiments.mragbench_evaluation import *

# åŠ è½½æ•°æ®
samples = load_mragbench_dataset(args)

# åŠ è½½æ¨¡å‹
model = load_model_and_baseline('ours')

# è¯„æµ‹
results = evaluate_mragbench(samples, model, use_rag=True)

# æŸ¥çœ‹ç»“æœ
print(f"Overall: {results['overall_accuracy']:.2f}%")
for scene, acc in results['scenario_accuracy'].items():
    print(f"  {scene}: {acc:.2f}%")
```

### æ‰¹é‡å¯¹æ¯”

```python
# è¯„æµ‹æ‰€æœ‰baseline
baselines = ['murag', 'mr2ag', 'visrag', 'reveal', 'ragvl', 'ours']

for baseline in baselines:
    model = load_model_and_baseline(baseline)
    results = evaluate_mragbench(samples, model)
    print(f"{baseline}: {results['overall_accuracy']:.2f}%")
```

---

## ğŸ¯ ä¸å…¶ä»–è¯„æµ‹çš„å…³ç³»

### OK-VQA vs MRAG-Bench

| æ•°æ®é›† | è§„æ¨¡ | ç±»å‹ | è¯„ä¼°é‡ç‚¹ |
|--------|------|------|---------|
| **OK-VQA** | 5,046 | å¼€æ”¾å¼VQA | å¤–éƒ¨çŸ¥è¯†ã€ç”Ÿæˆè´¨é‡ |
| **MRAG-Bench** | 1,353 | å¤šé€‰é¢˜ | ä½ç½®åå·®ã€è§†è§‰RAG |

### äº’è¡¥æ€§

- **OK-VQA**: æµ‹è¯•çŸ¥è¯†å¢å¼ºèƒ½åŠ›
- **MRAG-Bench**: æµ‹è¯•ä½ç½®åå·®å’Œè§†è§‰æ£€ç´¢
- **ä¸¤è€…ç»“åˆ**: å…¨é¢è¯„ä¼°å¤šæ¨¡æ€RAG

---

## ğŸ“Š é¢„æœŸç»“æœ

### åŸºäºæ–‡æ¡£ï¼ˆç¬¬1183-1191è¡Œï¼‰

| Method | OK-VQA | MRAG-Bench |
|--------|--------|------------|
| MuRAG  | 54.2   | -          |
| REVEAL | 56.8   | -          |
| VisRAG | 58.3   | 71.5       |
| mRÂ²AG  | 59.1   | 72.3       |
| RagVL  | 60.2   | 73.1       |
| **Ours** | **62.5** | **75.6** |

**æ³¨**: è¿™æ˜¯æ–‡æ¡£ä¸­çš„é¢„æœŸå€¼ï¼Œå®é™…å¯èƒ½ä¸åŒ

---

## ğŸ”§ é›†æˆåˆ°è®ºæ–‡

### Experimentséƒ¨åˆ†

```markdown
## 4.5 Evaluation on MRAG-Bench

To further validate our position-aware fusion mechanism, we evaluate 
on MRAG-Bench (Hu et al., 2024), a specialized benchmark for assessing 
position bias in multimodal RAG.

**Table: MRAG-Bench Results**

| Method | Overall | Angle | Partial | Scope | Reasoning |
|--------|---------|-------|---------|-------|-----------|
| VisRAG | 71.5%   | ...   | ...     | ...   | ...       |
| **Ours** | **75.6%** | ...   | ...     | ...   | ...       |

Our method outperforms VisRAG by 4.1% on MRAG-Bench, demonstrating 
effective position bias mitigation across diverse visual scenarios.
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### æ•°æ®é›†ä¸‹è½½

MRAG-Benchéœ€è¦ä¸‹è½½ï¼š
- é—®é¢˜å’Œæ ‡æ³¨ï¼ˆé€šè¿‡Hugging Faceï¼‰
- 16,130å¼ å›¾åƒï¼ˆ~å‡ GBï¼‰

**å¦‚æœä¸‹è½½å¤±è´¥**:
```python
# é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨OK-VQAçš„Position Biasæµ‹è¯•
from flashrag.evaluator.advanced_metrics import PositionBiasMetric

evaluator = PositionBiasMetric()
bias_score = evaluator.evaluate(model, test_samples)
```

### è¯„æµ‹æ—¶é—´

- å…¨é‡ï¼ˆ1,353æ ·æœ¬ï¼‰ï¼šçº¦1-2å°æ—¶
- å¿«é€Ÿæµ‹è¯•ï¼ˆ100æ ·æœ¬ï¼‰ï¼šçº¦10åˆ†é’Ÿ

---

## ğŸ“ å‚è€ƒ

**è®ºæ–‡**: Hu et al., "MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models", ICLR 2025  
**arXiv**: 2410.08182  
**æ•°æ®é›†**: https://huggingface.co/datasets/uclanlp/MRAG-Bench  
**ä»£ç **: /root/autodl-tmp/MRAG-Bench-main/


