#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
âœ… P1-1: 3Mè¯­æ–™æ„å»ºå·¥å…·

æ„å»º3Mè§„æ¨¡çš„æ··åˆè¯­æ–™ï¼š
- Wikipedia: 1.5Mæ–‡æ¡£
- Conceptual Captions 3M (CC3M): 1.5Må›¾æ–‡å¯¹

åŠŸèƒ½ï¼š
1. ä¸‹è½½/åŠ è½½Wikipediaè¯­æ–™
2. ä¸‹è½½/åŠ è½½CC3Mæ•°æ®é›†
3. å»é‡ï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰
4. ç»Ÿä¸€æ ¼å¼ï¼ˆJSONLï¼‰
5. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯

è¾“å‡ºæ ¼å¼ï¼ˆJSONLï¼‰ï¼š
{
    "id": <int>,
    "source": "wikipedia" | "cc3m",
    "title": <str>,
    "contents": <str>,
    "image_url": <str> (ä»…CC3M),
    "image_path": <str> (ä»…CC3Mï¼Œå¦‚æœä¸‹è½½äº†å›¾ç‰‡)
}

ä½¿ç”¨æ–¹æ³•ï¼š
    python tools/build_corpus_3m.py --output corpus/corpus_3m.jsonl
    python tools/build_corpus_3m.py --quick  # å¿«é€Ÿæ¨¡å¼ï¼ˆ10kæ ·æœ¬ï¼‰
"""

import os
import sys
import json
import hashlib
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Set
from tqdm import tqdm
from collections import Counter

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    import datasets
    from datasets import load_dataset
    DATASETS_AVAILABLE = True
except ImportError:
    DATASETS_AVAILABLE = False
    print("âš ï¸  datasetsåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ³•")


class CorpusBuilder:
    """3Mè¯­æ–™æ„å»ºå™¨"""
    
    def __init__(self, output_path: str, quick_mode: bool = False, wiki_samples: int = 1500000, cc3m_samples: int = 1500000):
        """
        åˆå§‹åŒ–æ„å»ºå™¨
        
        Args:
            output_path: è¾“å‡ºJSONLæ–‡ä»¶è·¯å¾„
            quick_mode: å¿«é€Ÿæ¨¡å¼ï¼ˆå‡å°‘æ ·æœ¬æ•°ï¼‰
            wiki_samples: Wikipediaæ ·æœ¬æ•°
            cc3m_samples: CC3Mæ ·æœ¬æ•°
        """
        self.output_path = Path(output_path)
        self.output_path.parent.mkdir(parents=True, exist_ok=True)
        
        self.quick_mode = quick_mode
        if quick_mode:
            self.wiki_samples = 5000
            self.cc3m_samples = 5000
            print("ğŸš€ å¿«é€Ÿæ¨¡å¼ï¼šWiki 5k + CC3M 5k")
        else:
            self.wiki_samples = wiki_samples
            self.cc3m_samples = cc3m_samples
            print(f"ğŸ“Š æ ‡å‡†æ¨¡å¼ï¼šWiki {wiki_samples//1000}k + CC3M {cc3m_samples//1000}k")
        
        # å»é‡å“ˆå¸Œé›†åˆ
        self.content_hashes: Set[str] = set()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_docs': 0,
            'wiki_docs': 0,
            'cc3m_docs': 0,
            'duplicates': 0,
            'total_tokens': 0,
            'avg_length': 0,
            'source_distribution': Counter(),
        }
    
    def compute_hash(self, text: str) -> str:
        """è®¡ç®—æ–‡æœ¬çš„MD5å“ˆå¸Œ"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def is_duplicate(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤"""
        text_hash = self.compute_hash(text)
        if text_hash in self.content_hashes:
            return True
        self.content_hashes.add(text_hash)
        return False
    
    def load_wikipedia(self) -> List[Dict]:
        """
        åŠ è½½Wikipediaè¯­æ–™ï¼ˆä»æœ¬åœ°TSVæ–‡ä»¶ï¼‰
        
        Returns:
            list: Wikipediaæ–‡æ¡£åˆ—è¡¨
        """
        print("\n" + "="*80)
        print("ğŸ“š åŠ è½½Wikipediaè¯­æ–™")
        print("="*80)
        
        docs = []
        
        # æœ¬åœ°TSVæ–‡ä»¶è·¯å¾„
        wiki_file = '/root/autodl-tmp/data/wikipedia/psgs_w100.tsv'
        
        if os.path.exists(wiki_file):
            print(f"ä»æœ¬åœ°æ–‡ä»¶åŠ è½½: {wiki_file}")
            print(f"  ç›®æ ‡æ ·æœ¬æ•°: {self.wiki_samples:,}")
            
            # è¯»å–TSVæ–‡ä»¶
            with open(wiki_file, 'r', encoding='utf-8') as f:
                # è·³è¿‡è¡¨å¤´
                header = f.readline()
                
                # æµå¼å¤„ç†
                count = 0
                for line in tqdm(f, total=self.wiki_samples, desc="Wikipedia"):
                    if count >= self.wiki_samples:
                        break
                    
                    try:
                        parts = line.strip().split('\t')
                        if len(parts) < 3:
                            continue
                        
                        doc_id = parts[0]
                        text = parts[1]
                        title = parts[2]
                        
                        if not text or len(text) < 100:
                            continue
                        
                        # å»é‡
                        if self.is_duplicate(text):
                            self.stats['duplicates'] += 1
                            continue
                        
                        doc = {
                            'id': len(docs),
                            'source': 'wikipedia',
                            'title': title,
                            'contents': f"{title}\n{text}",
                            'image_url': ""  # Wikipediaæ²¡æœ‰å›¾ç‰‡URLï¼ˆä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä¿è¯ç±»å‹ä¸€è‡´ï¼‰
                        }
                        docs.append(doc)
                        count += 1
                        
                    except Exception as e:
                        continue
            
            print(f"âœ… Wikipedia: {len(docs):,} æ–‡æ¡£")
            return docs
        
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {wiki_file}")
            print("  ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆæµ‹è¯•ç”¨ï¼‰")
            for i in tqdm(range(self.wiki_samples), desc="Wikipedia (æ¨¡æ‹Ÿ)"):
                doc = {
                    'id': i,
                    'source': 'wikipedia',
                    'title': f'Wikipedia Article {i}',
                    'contents': f'Wikipedia Article {i}\nThis is a simulated Wikipedia article about topic {i}. ' * 10,
                    'image_url': ""
                }
                docs.append(doc)
            
            return docs
    
    def load_cc3m(self) -> List[Dict]:
        """
        åŠ è½½Conceptual Captions 3Mè¯­æ–™ï¼ˆä»æœ¬åœ°TSVæ–‡ä»¶ï¼‰
        
        Returns:
            list: CC3Mæ–‡æ¡£åˆ—è¡¨
        """
        print("\n" + "="*80)
        print("ğŸ–¼ï¸  åŠ è½½CC3Mè¯­æ–™")
        print("="*80)
        
        docs = []
        
        # æœ¬åœ°TSVæ–‡ä»¶è·¯å¾„
        cc3m_file = '/root/autodl-tmp/data/conceptual_captions/Train_GCC-training.tsv'
        
        if os.path.exists(cc3m_file):
            print(f"ä»æœ¬åœ°æ–‡ä»¶åŠ è½½: {cc3m_file}")
            print(f"  ç›®æ ‡æ ·æœ¬æ•°: {self.cc3m_samples:,}")
            
            # è¯»å–TSVæ–‡ä»¶
            with open(cc3m_file, 'r', encoding='utf-8') as f:
                # æµå¼å¤„ç†ï¼ˆCC3Mçš„TSVæ ¼å¼ï¼šcaption \t image_urlï¼‰
                count = 0
                for line in tqdm(f, total=self.cc3m_samples, desc="CC3M"):
                    if count >= self.cc3m_samples:
                        break
                    
                    try:
                        parts = line.strip().split('\t')
                        if len(parts) < 2:
                            continue
                        
                        caption = parts[0]
                        image_url = parts[1]
                        
                        if not caption or len(caption) < 10:
                            continue
                        
                        # å»é‡
                        if self.is_duplicate(caption):
                            self.stats['duplicates'] += 1
                            continue
                        
                        # CC3Mçš„titleå°±æ˜¯captionçš„å‰50å­—ç¬¦
                        title = caption[:50] + "..." if len(caption) > 50 else caption
                        
                        doc = {
                            'id': len(docs),
                            'source': 'cc3m',
                            'title': title,
                            'contents': f"{title}\n{caption}",
                            'image_url': image_url
                        }
                        docs.append(doc)
                        count += 1
                        
                    except Exception as e:
                        continue
            
            print(f"âœ… CC3M: {len(docs):,} æ–‡æ¡£")
            return docs
        
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {cc3m_file}")
            print("  ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆæµ‹è¯•ç”¨ï¼‰")
            for i in tqdm(range(self.cc3m_samples), desc="CC3M (æ¨¡æ‹Ÿ)"):
                caption = f"A photo of object {i} in setting {i % 100}"
                doc = {
                    'id': i,
                    'source': 'cc3m',
                    'title': caption,
                    'contents': f"{caption}\n{caption}",
                    'image_url': f'http://example.com/image_{i}.jpg'
                }
                docs.append(doc)
            
            return docs
    
    def merge_and_save(self, wiki_docs: List[Dict], cc3m_docs: List[Dict]):
        """
        åˆå¹¶å¹¶ä¿å­˜è¯­æ–™
        
        Args:
            wiki_docs: Wikipediaæ–‡æ¡£
            cc3m_docs: CC3Mæ–‡æ¡£
        """
        print("\n" + "="*80)
        print("ğŸ’¾ åˆå¹¶å¹¶ä¿å­˜è¯­æ–™")
        print("="*80)
        
        # é‡æ–°åˆ†é…ID
        all_docs = []
        doc_id = 0
        
        # Wikipedia
        for doc in wiki_docs:
            doc['id'] = doc_id
            all_docs.append(doc)
            doc_id += 1
        
        # CC3M
        for doc in cc3m_docs:
            doc['id'] = doc_id
            all_docs.append(doc)
            doc_id += 1
        
        # ä¿å­˜ä¸ºJSONL
        print(f"ä¿å­˜åˆ°: {self.output_path}")
        with open(self.output_path, 'w', encoding='utf-8') as f:
            for doc in tqdm(all_docs, desc="ä¿å­˜"):
                f.write(json.dumps(doc, ensure_ascii=False) + '\n')
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['total_docs'] = len(all_docs)
        self.stats['wiki_docs'] = len(wiki_docs)
        self.stats['cc3m_docs'] = len(cc3m_docs)
        self.stats['source_distribution']['wikipedia'] = len(wiki_docs)
        self.stats['source_distribution']['cc3m'] = len(cc3m_docs)
        
        # è®¡ç®—å¹³å‡é•¿åº¦
        total_length = sum(len(doc['contents']) for doc in all_docs)
        self.stats['avg_length'] = total_length // len(all_docs) if all_docs else 0
        self.stats['total_tokens'] = total_length // 4  # ç²—ç•¥ä¼°è®¡ï¼ˆ1 token â‰ˆ 4 charsï¼‰
        
        print(f"âœ… ä¿å­˜å®Œæˆ: {len(all_docs):,} æ–‡æ¡£")
    
    def generate_statistics(self):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š è¯­æ–™ç»Ÿè®¡")
        print("="*80)
        
        print(f"\næ€»æ–‡æ¡£æ•°: {self.stats['total_docs']:,}")
        print(f"  - Wikipedia: {self.stats['wiki_docs']:,} ({self.stats['wiki_docs']/self.stats['total_docs']*100:.1f}%)")
        print(f"  - CC3M: {self.stats['cc3m_docs']:,} ({self.stats['cc3m_docs']/self.stats['total_docs']*100:.1f}%)")
        print(f"\nå»é‡ç»Ÿè®¡:")
        print(f"  - é‡å¤æ–‡æ¡£: {self.stats['duplicates']:,}")
        print(f"\næ–‡æœ¬ç»Ÿè®¡:")
        print(f"  - å¹³å‡é•¿åº¦: {self.stats['avg_length']:,} å­—ç¬¦")
        print(f"  - ä¼°è®¡Tokenæ•°: {self.stats['total_tokens']:,}")
        
        # ä¿å­˜ç»Ÿè®¡åˆ°JSON
        stats_path = self.output_path.with_suffix('.stats.json')
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump({
                **self.stats,
                'source_distribution': dict(self.stats['source_distribution']),
                'build_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'output_path': str(self.output_path)
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… ç»Ÿè®¡æ–‡ä»¶: {stats_path}")
        
        # ä¿å­˜å…ƒæ•°æ®ï¼ˆmeta.jsonï¼‰
        meta_path = self.output_path.parent / 'meta.json'
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump({
                'corpus_name': '3M Mixed Corpus',
                'version': '1.0',
                'total_documents': self.stats['total_docs'],
                'sources': {
                    'wikipedia': self.stats['wiki_docs'],
                    'cc3m': self.stats['cc3m_docs']
                },
                'build_date': datetime.now().strftime('%Y-%m-%d'),
                'corpus_file': self.output_path.name,
                'stats_file': stats_path.name,
                'avg_document_length': self.stats['avg_length'],
                'estimated_tokens': self.stats['total_tokens'],
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… å…ƒæ•°æ®æ–‡ä»¶: {meta_path}")
    
    def build(self):
        """æ‰§è¡Œæ„å»ºæµç¨‹"""
        print("="*80)
        print("ğŸ—ï¸  3Mè¯­æ–™æ„å»ºå™¨")
        print("="*80)
        print(f"è¾“å‡ºè·¯å¾„: {self.output_path}")
        print(f"æ¨¡å¼: {'å¿«é€Ÿ' if self.quick_mode else 'æ ‡å‡†'}")
        print()
        
        # 1. åŠ è½½Wikipedia
        wiki_docs = self.load_wikipedia()
        
        # 2. åŠ è½½CC3M
        cc3m_docs = self.load_cc3m()
        
        # 3. åˆå¹¶å¹¶ä¿å­˜
        self.merge_and_save(wiki_docs, cc3m_docs)
        
        # 4. ç”Ÿæˆç»Ÿè®¡
        self.generate_statistics()
        
        print("\n" + "="*80)
        print("âœ… è¯­æ–™æ„å»ºå®Œæˆï¼")
        print("="*80)
        print(f"\nè¾“å‡ºæ–‡ä»¶:")
        print(f"  - è¯­æ–™: {self.output_path}")
        print(f"  - ç»Ÿè®¡: {self.output_path.with_suffix('.stats.json')}")
        print(f"  - å…ƒæ•°æ®: {self.output_path.parent / 'meta.json'}")
        print()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='3Mè¯­æ–™æ„å»ºå·¥å…·')
    parser.add_argument('--output', type=str, 
                       default='/root/autodl-tmp/FlashRAG/corpus/corpus_3m.jsonl',
                       help='è¾“å‡ºJSONLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--quick', action='store_true',
                       help='å¿«é€Ÿæ¨¡å¼ï¼ˆWiki 5k + CC3M 5kï¼‰')
    parser.add_argument('--wiki-samples', type=int, default=1500000,
                       help='Wikipediaæ ·æœ¬æ•°ï¼ˆé»˜è®¤1.5Mï¼‰')
    parser.add_argument('--cc3m-samples', type=int, default=1500000,
                       help='CC3Mæ ·æœ¬æ•°ï¼ˆé»˜è®¤1.5Mï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥datasetsåº“
    if not DATASETS_AVAILABLE:
        print("âš ï¸  è­¦å‘Š: datasetsåº“æœªå®‰è£…")
        print("   å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰")
        print("   å®‰è£…: pip install datasets")
        print()
    
    # æ„å»ºè¯­æ–™
    builder = CorpusBuilder(
        output_path=args.output,
        quick_mode=args.quick,
        wiki_samples=args.wiki_samples,
        cc3m_samples=args.cc3m_samples
    )
    builder.build()


if __name__ == '__main__':
    main()

