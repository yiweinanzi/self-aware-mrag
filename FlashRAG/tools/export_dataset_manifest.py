#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
âœ… P1-3: æ•°æ®é›†æ¸…å•å¯¼å‡ºå·¥å…·

å¯¼å‡ºæ•°æ®é›†æ¸…å•CSVæ–‡ä»¶ï¼ŒåŒ…å«ï¼š
- æ•°æ®é›†åç§°
- ç‰ˆæœ¬
- æ ·æœ¬æ•°
- æ–‡ä»¶è·¯å¾„
- æ ¡éªŒå’Œï¼ˆMD5/SHA256ï¼‰
- æ•°æ®é›†æè¿°

è¾“å‡ºæ–‡ä»¶ï¼šdatasets_manifest.csv
"""

import os
import sys
import hashlib
import json
import csv
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from flashrag.config.dataset_registry import get_dataset_registry


def calculate_file_hash(file_path: str, algorithm: str = 'md5') -> Optional[str]:
    """
    è®¡ç®—æ–‡ä»¶çš„å“ˆå¸Œå€¼
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        algorithm: å“ˆå¸Œç®—æ³•ï¼ˆ'md5' æˆ– 'sha256'ï¼‰
    
    Returns:
        str: å“ˆå¸Œå€¼ï¼ˆåå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰
    """
    if not os.path.exists(file_path):
        return None
    
    try:
        if algorithm == 'md5':
            hasher = hashlib.md5()
        elif algorithm == 'sha256':
            hasher = hashlib.sha256()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å“ˆå¸Œç®—æ³•: {algorithm}")
        
        # åˆ†å—è¯»å–ä»¥å¤„ç†å¤§æ–‡ä»¶
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                hasher.update(chunk)
        
        return hasher.hexdigest()
    except Exception as e:
        print(f"  âš ï¸  è®¡ç®—å“ˆå¸Œå¤±è´¥ ({file_path}): {e}")
        return None


def calculate_dir_hash(dir_path: str, algorithm: str = 'md5') -> Optional[str]:
    """
    è®¡ç®—ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶çš„ç»¼åˆå“ˆå¸Œå€¼
    
    Args:
        dir_path: ç›®å½•è·¯å¾„
        algorithm: å“ˆå¸Œç®—æ³•
    
    Returns:
        str: ç»¼åˆå“ˆå¸Œå€¼
    """
    if not os.path.exists(dir_path):
        return None
    
    try:
        if algorithm == 'md5':
            hasher = hashlib.md5()
        elif algorithm == 'sha256':
            hasher = hashlib.sha256()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å“ˆå¸Œç®—æ³•: {algorithm}")
        
        # æŒ‰æ–‡ä»¶åæ’åºä»¥ä¿è¯ä¸€è‡´æ€§
        file_paths = []
        for root, dirs, files in os.walk(dir_path):
            for filename in sorted(files):
                file_path = os.path.join(root, filename)
                file_paths.append(file_path)
        
        # è®¡ç®—æ¯ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå¹¶åˆå¹¶
        for file_path in sorted(file_paths):
            try:
                with open(file_path, 'rb') as f:
                    for chunk in iter(lambda: f.read(4096), b''):
                        hasher.update(chunk)
            except Exception as e:
                print(f"  âš ï¸  è·³è¿‡æ–‡ä»¶ {file_path}: {e}")
                continue
        
        return hasher.hexdigest()
    except Exception as e:
        print(f"  âš ï¸  è®¡ç®—ç›®å½•å“ˆå¸Œå¤±è´¥ ({dir_path}): {e}")
        return None


def get_dataset_info(dataset_id: str, dataset_config) -> Dict[str, any]:
    """
    è·å–æ•°æ®é›†è¯¦ç»†ä¿¡æ¯
    
    Args:
        dataset_id: æ•°æ®é›†ID
        dataset_config: æ•°æ®é›†é…ç½®å¯¹è±¡
    
    Returns:
        dict: æ•°æ®é›†ä¿¡æ¯
    """
    info = {
        'dataset_id': dataset_id,
        'name': dataset_config.name,
        'enabled': dataset_config.enabled,
        'description': dataset_config.description,
        'version': dataset_config.version or 'N/A',
        'path': dataset_config.path or 'N/A',
        'loader': dataset_config.loader or 'N/A',
        'num_samples': dataset_config.num_samples or 0,
        'path_exists': False,
        'path_type': 'N/A',
        'size_bytes': 0,
        'size_human': 'N/A',
        'md5_checksum': 'N/A',
        'sha256_checksum': 'N/A',
        'files_count': 0,
        'last_modified': 'N/A',
    }
    
    # æ£€æŸ¥è·¯å¾„
    if dataset_config.path and os.path.exists(dataset_config.path):
        info['path_exists'] = True
        path_obj = Path(dataset_config.path)
        
        if path_obj.is_file():
            # å•ä¸ªæ–‡ä»¶
            info['path_type'] = 'file'
            info['size_bytes'] = path_obj.stat().st_size
            info['size_human'] = format_bytes(info['size_bytes'])
            info['files_count'] = 1
            info['last_modified'] = datetime.fromtimestamp(
                path_obj.stat().st_mtime
            ).strftime('%Y-%m-%d %H:%M:%S')
            
            # è®¡ç®—æ ¡éªŒå’Œï¼ˆæ–‡ä»¶è¾ƒå°æ—¶ï¼‰
            if info['size_bytes'] < 100 * 1024 * 1024:  # < 100MB
                print(f"  è®¡ç®—æ ¡éªŒå’Œ: {dataset_id}")
                info['md5_checksum'] = calculate_file_hash(dataset_config.path, 'md5')
                info['sha256_checksum'] = calculate_file_hash(dataset_config.path, 'sha256')
            else:
                print(f"  è·³è¿‡æ ¡éªŒå’Œï¼ˆæ–‡ä»¶è¿‡å¤§ï¼‰: {dataset_id}")
        
        elif path_obj.is_dir():
            # ç›®å½•
            info['path_type'] = 'directory'
            
            # è®¡ç®—ç›®å½•å¤§å°å’Œæ–‡ä»¶æ•°
            total_size = 0
            file_count = 0
            latest_mtime = 0
            
            for root, dirs, files in os.walk(dataset_config.path):
                for filename in files:
                    file_path = os.path.join(root, filename)
                    try:
                        stat = os.stat(file_path)
                        total_size += stat.st_size
                        file_count += 1
                        latest_mtime = max(latest_mtime, stat.st_mtime)
                    except:
                        pass
            
            info['size_bytes'] = total_size
            info['size_human'] = format_bytes(total_size)
            info['files_count'] = file_count
            info['last_modified'] = datetime.fromtimestamp(
                latest_mtime
            ).strftime('%Y-%m-%d %H:%M:%S') if latest_mtime > 0 else 'N/A'
            
            # è®¡ç®—ç›®å½•æ ¡éªŒå’Œï¼ˆç›®å½•è¾ƒå°æ—¶ï¼‰
            if total_size < 50 * 1024 * 1024:  # < 50MB
                print(f"  è®¡ç®—ç›®å½•æ ¡éªŒå’Œ: {dataset_id}")
                info['md5_checksum'] = calculate_dir_hash(dataset_config.path, 'md5')
            else:
                print(f"  è·³è¿‡ç›®å½•æ ¡éªŒå’Œï¼ˆç›®å½•è¿‡å¤§ï¼‰: {dataset_id}")
    
    return info


def format_bytes(bytes_size: int) -> str:
    """æ ¼å¼åŒ–å­—èŠ‚å¤§å°ä¸ºäººç±»å¯è¯»æ ¼å¼"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if bytes_size < 1024.0:
            return f"{bytes_size:.2f} {unit}"
        bytes_size /= 1024.0
    return f"{bytes_size:.2f} PB"


def export_to_csv(dataset_infos: List[Dict], output_path: str):
    """
    å¯¼å‡ºåˆ°CSVæ–‡ä»¶
    
    Args:
        dataset_infos: æ•°æ®é›†ä¿¡æ¯åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    fieldnames = [
        'dataset_id',
        'name',
        'enabled',
        'version',
        'num_samples',
        'description',
        'path',
        'path_exists',
        'path_type',
        'size_bytes',
        'size_human',
        'files_count',
        'md5_checksum',
        'sha256_checksum',
        'loader',
        'last_modified',
    ]
    
    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        for info in dataset_infos:
            # åªå†™å…¥æŒ‡å®šçš„å­—æ®µ
            row = {k: info.get(k, 'N/A') for k in fieldnames}
            writer.writerow(row)
    
    print(f"\nâœ… CSVæ–‡ä»¶å·²å¯¼å‡º: {output_path}")


def export_to_json(dataset_infos: List[Dict], output_path: str):
    """
    å¯¼å‡ºåˆ°JSONæ–‡ä»¶
    
    Args:
        dataset_infos: æ•°æ®é›†ä¿¡æ¯åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    output = {
        'export_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_datasets': len(dataset_infos),
        'enabled_datasets': sum(1 for d in dataset_infos if d['enabled']),
        'datasets': dataset_infos
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… JSONæ–‡ä»¶å·²å¯¼å‡º: {output_path}")


def print_summary(dataset_infos: List[Dict]):
    """æ‰“å°æ±‡æ€»ä¿¡æ¯"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æ•°æ®é›†æ¸…å•æ±‡æ€»")
    print("=" * 80)
    
    enabled = [d for d in dataset_infos if d['enabled']]
    disabled = [d for d in dataset_infos if not d['enabled']]
    
    print(f"\næ€»æ•°æ®é›†æ•°: {len(dataset_infos)}")
    print(f"  âœ… å¯ç”¨: {len(enabled)}")
    print(f"  âŒ ç¦ç”¨: {len(disabled)}")
    
    print(f"\nå¯ç”¨çš„æ•°æ®é›†:")
    total_samples = 0
    total_size = 0
    
    for d in enabled:
        status = "âœ…" if d['path_exists'] else "âš ï¸ "
        print(f"  {status} {d['name']:20s} - {d['num_samples']:6d} samples, {d['size_human']:>10s}")
        total_samples += d['num_samples']
        total_size += d['size_bytes']
    
    print(f"\næ€»æ ·æœ¬æ•°: {total_samples:,}")
    print(f"æ€»å¤§å°: {format_bytes(total_size)}")
    
    if disabled:
        print(f"\nç¦ç”¨çš„æ•°æ®é›†:")
        for d in disabled:
            print(f"  âŒ {d['name']:20s} - {d['description']}")
    
    print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ“‹ æ•°æ®é›†æ¸…å•å¯¼å‡ºå·¥å…·")
    print("=" * 80)
    
    # è·å–æ•°æ®é›†æ³¨å†Œè¡¨
    registry = get_dataset_registry()
    
    # è·å–æ‰€æœ‰æ•°æ®é›†é…ç½®
    all_datasets = {**registry.get_enabled_datasets(), **registry.get_disabled_datasets()}
    
    print(f"\nå‘ç° {len(all_datasets)} ä¸ªæ•°æ®é›†")
    print("æ­£åœ¨æ”¶é›†è¯¦ç»†ä¿¡æ¯...")
    print()
    
    # æ”¶é›†æ•°æ®é›†ä¿¡æ¯
    dataset_infos = []
    for dataset_id, config in all_datasets.items():
        print(f"å¤„ç†: {config.name} ({dataset_id})")
        info = get_dataset_info(dataset_id, config)
        dataset_infos.append(info)
    
    # æŒ‰å¯ç”¨çŠ¶æ€å’Œåç§°æ’åº
    dataset_infos.sort(key=lambda x: (not x['enabled'], x['name']))
    
    # å¯¼å‡ºåˆ°CSV
    csv_path = '/root/autodl-tmp/FlashRAG/datasets_manifest.csv'
    export_to_csv(dataset_infos, csv_path)
    
    # å¯¼å‡ºåˆ°JSONï¼ˆé™„åŠ è¯¦ç»†ä¿¡æ¯ï¼‰
    json_path = '/root/autodl-tmp/FlashRAG/datasets_manifest.json'
    export_to_json(dataset_infos, json_path)
    
    # æ‰“å°æ±‡æ€»
    print_summary(dataset_infos)
    
    print(f"\nâœ… å¯¼å‡ºå®Œæˆï¼")
    print(f"  - CSV: {csv_path}")
    print(f"  - JSON: {json_path}")
    print()


if __name__ == '__main__':
    main()

