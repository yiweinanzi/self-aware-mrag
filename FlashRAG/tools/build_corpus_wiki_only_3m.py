#!/usr/bin/env python3
"""
æ„å»ºçº¯Wikipedia 3Mè¯­æ–™åº“
ä» psgs_w100.tsv ä¸­é€‰å–å‰300ä¸‡æ¡Wikipediaæ–‡æ¡£
"""

import json
import csv
from pathlib import Path
from tqdm import tqdm
import sys

def load_wikipedia(tsv_path, max_docs=3000000):
    """ä»TSVæ–‡ä»¶åŠ è½½Wikipediaæ–‡æ¡£"""
    docs = []
    print(f"ğŸ“– åŠ è½½Wikipediaæ•°æ®: {tsv_path}")
    print(f"   ç›®æ ‡æ•°é‡: {max_docs:,} æ¡")
    
    with open(tsv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter='\t')
        for idx, row in enumerate(tqdm(reader, desc="åŠ è½½Wikipedia", total=max_docs)):
            if idx >= max_docs:
                break
            
            title = row.get('title', '').strip()
            text = row.get('text', '').strip()
            
            if not title or not text:
                continue
            
            doc = {
                'id': idx,
                'source': 'wikipedia',
                'title': title,
                'contents': f"{title}\n{text}",
                'image_url': ""  # ä¿æŒä¸€è‡´çš„schemaï¼ˆè™½ç„¶æ˜¯çº¯æ–‡æœ¬ï¼‰
            }
            docs.append(doc)
    
    print(f"âœ… WikipediaåŠ è½½å®Œæˆ: {len(docs):,} æ¡")
    return docs

def save_corpus(docs, output_path):
    """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
    print(f"\nğŸ’¾ ä¿å­˜è¯­æ–™åº“åˆ°: {output_path}")
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for doc in tqdm(docs, desc="å†™å…¥æ–‡ä»¶"):
            f.write(json.dumps(doc, ensure_ascii=False) + '\n')
    
    print(f"âœ… è¯­æ–™åº“ä¿å­˜æˆåŠŸï¼")
    print(f"   æ–‡ä»¶: {output_path}")
    print(f"   å¤§å°: {output_path.stat().st_size / (1024**3):.2f} GB")
    print(f"   æ¡ç›®æ•°: {len(docs):,}")

def main():
    # è·¯å¾„é…ç½®
    wiki_tsv = Path("/root/autodl-tmp/data/wikipedia/psgs_w100.tsv")
    output_file = Path("/root/autodl-tmp/FlashRAG/corpus/corpus_wiki_3m.jsonl")
    
    print("=" * 80)
    print("æ„å»ºçº¯Wikipedia 3Mè¯­æ–™åº“")
    print("=" * 80)
    print()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not wiki_tsv.exists():
        print(f"âŒ é”™è¯¯: Wikipedia TSVæ–‡ä»¶ä¸å­˜åœ¨: {wiki_tsv}")
        sys.exit(1)
    
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   Wikipedia TSV: {wiki_tsv}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"   ç›®æ ‡æ•°é‡: 3,000,000 æ¡")
    print()
    
    # åŠ è½½Wikipediaæ•°æ®
    docs = load_wikipedia(wiki_tsv, max_docs=3000000)
    
    # ä¿å­˜è¯­æ–™åº“
    save_corpus(docs, output_file)
    
    print()
    print("=" * 80)
    print("âœ… è¯­æ–™åº“æ„å»ºå®Œæˆï¼")
    print("=" * 80)
    print()
    print("ğŸ“‹ åç»­æ­¥éª¤:")
    print("1. é‡å»ºBGEç´¢å¼•:")
    print("   python tools/rebuild_index_wiki_3m.py")
    print()
    print("2. æ›´æ–°å®éªŒé…ç½®ä¸­çš„corpus_pathå’Œindex_path")
    print()

if __name__ == '__main__':
    main()
