# Baselineä¿®å¤è®¡åˆ’ - è¯¦ç»†å®æ–½æ–¹æ¡ˆ

**åˆ¶å®šæ—¶é—´**: 2025-10-30 17:45  
**ç›®æ ‡**: æ­£ç¡®å®ç°æ‰€æœ‰baselineæ–¹æ³•ï¼Œç¡®ä¿å®éªŒç»“æœå¯ä¿¡  
**é¢„è®¡æ€»è€—æ—¶**: 1-2å‘¨

---

## ğŸ¯ æ€»ä½“ç­–ç•¥

**åˆ†é˜¶æ®µå®æ–½**:
1. **é˜¶æ®µ1**: ä¿®å¤æŒ‡æ ‡é—®é¢˜ + å®ç°2ä¸ªæ ¸å¿ƒbaselineï¼ˆSelf-RAGã€mRÂ²AGï¼‰
2. **é˜¶æ®µ2**: è¿è¡Œ100æ ·æœ¬éªŒè¯ï¼Œç¡®è®¤å®ç°æ­£ç¡®
3. **é˜¶æ®µ3**: æ ¹æ®ç»“æœå†³å®šæ˜¯å¦å®ç°å…¶ä»–baseline
4. **é˜¶æ®µ4**: è¿è¡Œå®Œæ•´æ•°æ®é›†å®éªŒ

---

## ğŸ“Š é˜¶æ®µ1: æ ¸å¿ƒä¿®å¤ï¼ˆé¢„è®¡3-5å¤©ï¼‰

### Task 1: ä¿®å¤3ä¸ªæŒ‡æ ‡ä¸º0çš„é—®é¢˜ âš¡ [é¢„è®¡: 2å°æ—¶]

**é—®é¢˜**: Pipelineæœªæä¾›å¿…éœ€çš„æ•°æ®å­—æ®µ

**éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**:
- `FlashRAG/experiments/run_all_baselines_100samples.py`
- `FlashRAG/flashrag/pipeline/self_aware_pipeline_qwen3vl.py`

**å…·ä½“ä»»åŠ¡**:
```python
# 1. åœ¨pipelineè¿”å›ç»“æœæ—¶æ·»åŠ å­—æ®µ

class SelfAwarePipelineQwen3VL:
    def forward(self, sample):
        # ... ç°æœ‰ä»£ç  ...
        
        # âœ… æ·»åŠ å¿…éœ€å­—æ®µ
        result = {
            'answer': final_answer,
            'retrieval_result': [  # â† æ–°å¢
                {
                    'retrieved_docs': retrieved_docs,
                    'retrieval_scores': scores
                }
            ],
            'attributions': {  # â† æ–°å¢ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
                'visual': [...],
                'text': [...]
            },
            # position_biaséœ€è¦ç‰¹æ®Šæµ‹è¯•ï¼Œæš‚æ—¶å¯ä»¥è·³è¿‡
        }
        return result
```

**éªŒè¯æ–¹æ³•**:
```bash
# è¿è¡Œ10ä¸ªæ ·æœ¬ï¼Œæ£€æŸ¥3ä¸ªæŒ‡æ ‡æ˜¯å¦ä¸å†æ˜¯0
python FlashRAG/experiments/run_all_baselines_100samples.py --max_samples 10
```

**ä¼˜å…ˆçº§**: ğŸ”´ P0 - å¿…é¡»å®Œæˆ  
**è´Ÿè´£äºº**: AI + ç”¨æˆ·éªŒè¯

---

### Task 2: æ­£ç¡®å®ç°Self-RAG [é¢„è®¡: 1-2å¤©]

**å‚è€ƒèµ„æ–™**:
- åŸå§‹è®ºæ–‡: Self-RAG (ICLR 2024)
- ä»£ç ä»“åº“: `open_resource/self-rag-main/`
- æ ¸å¿ƒæ–‡ä»¶: `retrieval_lm/run_long_form_static.py`

#### 2.1 Adaptive Retrievalå®ç°

**æ ¸å¿ƒæ€æƒ³**: ä½¿ç”¨ä¸ç¡®å®šæ€§åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢

```python
class SelfRAGPipeline(BaselinePipeline):
    """Self-RAG: è‡ªé€‚åº”æ£€ç´¢ + åæ€æœºåˆ¶"""
    
    def __init__(self, qwen3_vl, retriever, config):
        super().__init__(qwen3_vl, retriever, config)
        # æ·»åŠ ç‰¹æ®Štokenï¼ˆå¦‚æœéœ€è¦ï¼‰
        self.special_tokens = {
            'retrieval': '[Retrieval]',
            'no_retrieval': '[No Retrieval]',
            'relevant': '[Relevant]',
            'irrelevant': '[Irrelevant]',
            'utility_high': '[Utility:5]',
            'utility_low': '[Utility:1]'
        }
    
    def should_retrieve(self, question, image):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢ï¼ˆåŸºäºä¸ç¡®å®šæ€§ï¼‰"""
        
        # æ–¹æ³•1: ä½¿ç”¨ç®€åŒ–çš„å¯å‘å¼è§„åˆ™
        # æ£€æŸ¥é—®é¢˜ä¸­æ˜¯å¦åŒ…å«éœ€è¦å¤–éƒ¨çŸ¥è¯†çš„å…³é”®è¯
        knowledge_keywords = [
            'when', 'where', 'who', 'what year', 'which country',
            'how many', 'name of', 'capital', 'population'
        ]
        question_lower = question.lower()
        needs_knowledge = any(kw in question_lower for kw in knowledge_keywords)
        
        # æ–¹æ³•2: åŸºäºæ¨¡å‹çš„åˆå§‹ç½®ä¿¡åº¦
        # å…ˆè®©æ¨¡å‹å°è¯•å›ç­”ï¼Œå¦‚æœç½®ä¿¡åº¦ä½åˆ™æ£€ç´¢
        try:
            initial_answer, confidence = self._generate_with_confidence(
                question, image, max_tokens=5
            )
            # å¦‚æœç½®ä¿¡åº¦ < é˜ˆå€¼ï¼Œåˆ™éœ€è¦æ£€ç´¢
            needs_retrieval = confidence < 0.7
        except:
            needs_retrieval = needs_knowledge
        
        return needs_retrieval
    
    def _generate_with_confidence(self, question, image, max_tokens):
        """ç”Ÿæˆç­”æ¡ˆå¹¶è¿”å›ç½®ä¿¡åº¦"""
        # TODO: å®ç°åŸºäºlogitsçš„ç½®ä¿¡åº¦ä¼°è®¡
        # ç±»ä¼¼æˆ‘ä»¬çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œä½†æ›´ç®€åŒ–
        pass
    
    def evaluate_relevance(self, question, image, retrieved_doc):
        """è¯„ä¼°æ£€ç´¢æ–‡æ¡£çš„ç›¸å…³æ€§ï¼ˆreflectionï¼‰"""
        
        # æ„å»ºrelevanceåˆ¤æ–­prompt
        relevance_prompt = f"""Given the question and the retrieved passage, 
is this passage relevant to answering the question?

Question: {question}
Passage: {retrieved_doc[:500]}

Answer with: [Relevant] or [Irrelevant]"""
        
        # ä½¿ç”¨æ¨¡å‹åˆ¤æ–­ï¼ˆç”Ÿæˆ1ä¸ªtokenï¼‰
        response = self.qwen3_vl.generate(
            text=relevance_prompt,
            image=image,
            max_new_tokens=5,
            temperature=0.01
        )
        
        # è§£æåˆ¤æ–­ç»“æœ
        is_relevant = '[Relevant]' in response or 'relevant' in response.lower()
        
        return is_relevant
    
    def run_single(self, sample):
        """è¿è¡Œå•ä¸ªæ ·æœ¬"""
        
        # Step 1: åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢
        should_retrieve = self.should_retrieve(sample['question'], sample['image'])
        
        if not should_retrieve:
            # ç›´æ¥ç”Ÿæˆç­”æ¡ˆï¼ˆæ— æ£€ç´¢ï¼‰
            options = {
                'A': sample['A'], 'B': sample['B'],
                'C': sample['C'], 'D': sample['D']
            }
            prompt = self._construct_prompt(sample['question'], options, context=None)
            prediction = self._generate(prompt, sample['image'])
            
            return {
                'answer': self._map_letter_to_answer(prediction, sample),
                'raw_prediction': prediction,
                'retrieved_docs': [],
                'used_retrieval': False,
                'retrieval_decision': 'No Retrieval'
            }
        
        # Step 2: æ£€ç´¢æ–‡æ¡£
        results = self.retriever.search(sample['question'], num=10)
        
        # Step 3: è¯„ä¼°ç›¸å…³æ€§ï¼ˆreflectionï¼‰
        relevant_docs = []
        for doc in results[:5]:  # åªè¯„ä¼°top-5
            doc_text = doc.get('contents', '')
            if self.evaluate_relevance(sample['question'], sample['image'], doc_text):
                relevant_docs.append(doc_text)
        
        # Step 4: ä½¿ç”¨ç›¸å…³æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
        if relevant_docs:
            context = "\n\n".join(relevant_docs)
        else:
            # å¦‚æœæ²¡æœ‰ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨top-1
            context = results[0].get('contents', '') if results else ""
        
        options = {
            'A': sample['A'], 'B': sample['B'],
            'C': sample['C'], 'D': sample['D']
        }
        prompt = self._construct_prompt(sample['question'], options, context)
        prediction = self._generate(prompt, sample['image'])
        
        return {
            'answer': self._map_letter_to_answer(prediction, sample),
            'raw_prediction': prediction,
            'retrieved_docs': relevant_docs,
            'used_retrieval': True,
            'retrieval_decision': 'Retrieval',
            'num_relevant_docs': len(relevant_docs)
        }
```

**å…³é”®æ”¹è¿›**:
1. âœ… `should_retrieve()`: è‡ªé€‚åº”åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢
2. âœ… `evaluate_relevance()`: åæ€æœºåˆ¶ï¼Œè¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§
3. âœ… åªä½¿ç”¨ç›¸å…³æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
4. âœ… è®°å½•æ£€ç´¢å†³ç­–å’Œç›¸å…³æ–‡æ¡£æ•°é‡

**æµ‹è¯•ä»£ç **:
```python
# æµ‹è¯•Self-RAG
pipeline = SelfRAGPipeline(qwen3_vl, retriever, config)

# æµ‹è¯•æ ·æœ¬1: éœ€è¦å¤–éƒ¨çŸ¥è¯†
sample1 = {
    'question': 'When was this building constructed?',
    'image': ...,
    'A': '1887', 'B': '1900', 'C': '1920', 'D': '1950'
}
result1 = pipeline.run_single(sample1)
print(f"Decision: {result1['retrieval_decision']}")
print(f"Used retrieval: {result1['used_retrieval']}")

# æµ‹è¯•æ ·æœ¬2: ä¸éœ€è¦å¤–éƒ¨çŸ¥è¯†
sample2 = {
    'question': 'What color is the sky in this image?',
    'image': ...,
    'A': 'blue', 'B': 'red', 'C': 'green', 'D': 'yellow'
}
result2 = pipeline.run_single(sample2)
print(f"Decision: {result2['retrieval_decision']}")
print(f"Used retrieval: {result2['used_retrieval']}")
```

**æœŸæœ›ç»“æœ**:
- Sample1åº”è¯¥è§¦å‘æ£€ç´¢ï¼ˆ`used_retrieval=True`ï¼‰
- Sample2åº”è¯¥ä¸æ£€ç´¢ï¼ˆ`used_retrieval=False`ï¼‰
- æ£€ç´¢ç‡åº”è¯¥åœ¨30-60%ä¹‹é—´ï¼ˆæ ¹æ®æ•°æ®é›†ç‰¹æ€§ï¼‰

**ä¼˜å…ˆçº§**: ğŸ”´ P0 - æ ¸å¿ƒbaseline  
**é¢„è®¡æ—¶é—´**: 1-2å¤©

---

### Task 3: æ­£ç¡®å®ç°mRÂ²AG [é¢„è®¡: 2-3å¤©]

**å‚è€ƒèµ„æ–™**:
- å¤ç°æŒ‡å—: `open_resource/m_r_ag_å¤ç°æŒ‡å—ï¼ˆé¢å‘_cursorï¼‰.md`
- è®ºæ–‡: mRÂ²AG (arXiv:2411.15041)

#### 3.1 ä¸‰é˜¶æ®µæ¨ç†å®ç°

**ä¼ªä»£ç **ï¼ˆæ¥è‡ªå¤ç°æŒ‡å—ç¬¬227-253è¡Œï¼‰:
```python
# 1) Retrieval-Reflection
ret = model.generate_one_token(img, q, prompt=RET_PROMPT)
if ret == "[No Retrieval]":
    return model.generate_answer(img, q, prompt=ANS_PROMPT_SINGLE)

# 2) Entry Retrievalï¼ˆTop-K æ¡ç›®ï¼‰
entries = clip_retriever.topk(img, K=5)

cands = []
for e in entries:
    for para in e.paragraphs:
        rel_tok, rel_prob = model.generate_one_token_with_prob(
            img, q, para, prompt=REL_PROMPT
        )
        if rel_tok == "[Relevant]":
            ans, ans_prob_geom = model.generate_answer_with_prob(
                img, q, para, prompt=ANS_PROMPT
            )
            cands.append({
                'ans': ans,
                'S_ret': e.score,
                'S_rel': rel_prob,
                'S_ans': ans_prob_geom
            })

# 3) å±‚çº§åå¤„ç†ï¼ˆä¹˜ç§¯æ‰“åˆ†ï¼‰
best = max(cands, key=lambda d: d['S_ret']*d['S_rel']*d['S_ans'])
return normalize(best['ans'])
```

**å®é™…å®ç°**:
```python
class MR2AGPipeline(BaselinePipeline):
    """mRÂ²AG: å¤šè½®æ£€ç´¢-åæ€-ç”Ÿæˆ"""
    
    def __init__(self, qwen3_vl, retriever, config):
        super().__init__(qwen3_vl, retriever, config)
        self.special_tokens = {
            'retrieval': '[Retrieval]',
            'no_retrieval': '[No Retrieval]',
            'relevant': '[Relevant]',
            'irrelevant': '[Irrelevant]'
        }
    
    def stage1_retrieval_reflection(self, question, image):
        """é˜¶æ®µ1: Retrieval-Reflectionåˆ¤æ–­"""
        
        prompt = f"""Only output one token: [Retrieval] or [No Retrieval].

Question: {question}

Does this question require external knowledge?"""
        
        response = self.qwen3_vl.generate(
            text=prompt,
            image=image,
            max_new_tokens=5,
            temperature=0.01
        )
        
        needs_retrieval = '[Retrieval]' in response or 'retrieval' in response.lower()
        return needs_retrieval
    
    def stage2_relevance_reflection(self, question, image, paragraph):
        """é˜¶æ®µ2: Relevance-Reflectionæ®µè½åˆ¤æ–­"""
        
        prompt = f"""Only output one token: [Relevant] or [Irrelevant].

Question: {question}
Paragraph: {paragraph[:300]}

Is this paragraph relevant to answering the question?"""
        
        response = self.qwen3_vl.generate(
            text=prompt,
            image=image,
            max_new_tokens=5,
            temperature=0.01
        )
        
        is_relevant = '[Relevant]' in response or 'relevant' in response.lower()
        
        # ç®€åŒ–ç‰ˆ: æ— æ³•ç›´æ¥è·å–tokenæ¦‚ç‡ï¼Œä½¿ç”¨å¯å‘å¼
        # å®Œæ•´ç‰ˆéœ€è¦ä¿®æ”¹æ¨¡å‹æ¥å£ä»¥è¿”å›logits
        relevance_score = 0.9 if is_relevant else 0.1
        
        return is_relevant, relevance_score
    
    def stage3_answer_generation(self, question, image, paragraph, options):
        """é˜¶æ®µ3: åŸºäºç›¸å…³æ®µè½ç”Ÿæˆç­”æ¡ˆ"""
        
        prompt = self._construct_prompt(question, options, context=paragraph)
        
        answer = self.qwen3_vl.generate(
            text=prompt,
            image=image,
            max_new_tokens=self.config['max_new_tokens'],
            temperature=self.config['temperature']
        )
        
        # ç®€åŒ–ç‰ˆ: ç­”æ¡ˆç½®ä¿¡åº¦ä½¿ç”¨å¯å‘å¼
        # å®Œæ•´ç‰ˆéœ€è¦è®¡ç®—token-levelæ¦‚ç‡çš„å‡ ä½•å¹³å‡
        answer_score = 0.8 if len(answer.strip()) > 0 else 0.1
        
        return answer, answer_score
    
    def split_into_paragraphs(self, doc_text, max_para_len=200):
        """å°†æ–‡æ¡£åˆ‡åˆ†æˆæ®µè½"""
        
        # ç®€å•æŒ‰å¥å·åˆ‡åˆ†
        sentences = doc_text.split('. ')
        paragraphs = []
        current_para = ""
        
        for sent in sentences:
            if len(current_para) + len(sent) < max_para_len:
                current_para += sent + '. '
            else:
                if current_para:
                    paragraphs.append(current_para.strip())
                current_para = sent + '. '
        
        if current_para:
            paragraphs.append(current_para.strip())
        
        return paragraphs[:5]  # æœ€å¤š5ä¸ªæ®µè½
    
    def run_single(self, sample):
        """è¿è¡Œå•ä¸ªæ ·æœ¬ - å®Œæ•´çš„ä¸‰é˜¶æ®µæµç¨‹"""
        
        # ===== é˜¶æ®µ1: Retrieval-Reflection =====
        needs_retrieval = self.stage1_retrieval_reflection(
            sample['question'], 
            sample['image']
        )
        
        if not needs_retrieval:
            # ç›´æ¥ç”Ÿæˆç­”æ¡ˆ
            options = {
                'A': sample['A'], 'B': sample['B'],
                'C': sample['C'], 'D': sample['D']
            }
            prompt = self._construct_prompt(sample['question'], options, context=None)
            prediction = self._generate(prompt, sample['image'])
            
            return {
                'answer': self._map_letter_to_answer(prediction, sample),
                'raw_prediction': prediction,
                'retrieved_docs': [],
                'used_retrieval': False,
                'stage': 'No Retrieval'
            }
        
        # ===== é˜¶æ®µ2: Entry Retrieval + Relevance-Reflection =====
        # æ£€ç´¢Top-Kæ¡ç›®
        entries = self.retriever.search(sample['question'], num=10)
        
        candidates = []
        options = {
            'A': sample['A'], 'B': sample['B'],
            'C': sample['C'], 'D': sample['D']
        }
        
        for entry in entries[:5]:  # Top-5æ¡ç›®
            entry_score = entry.get('score', 1.0)
            doc_text = entry.get('contents', '')
            
            # å°†æ¡ç›®åˆ‡åˆ†æˆæ®µè½
            paragraphs = self.split_into_paragraphs(doc_text)
            
            for para in paragraphs:
                # æ®µè½çº§ç›¸å…³æ€§åˆ¤æ–­
                is_relevant, relevance_score = self.stage2_relevance_reflection(
                    sample['question'],
                    sample['image'],
                    para
                )
                
                if is_relevant:
                    # ===== é˜¶æ®µ3: Answer Generation =====
                    answer, answer_score = self.stage3_answer_generation(
                        sample['question'],
                        sample['image'],
                        para,
                        options
                    )
                    
                    # å±‚çº§åå¤„ç†: ä¹˜ç§¯æ‰“åˆ†
                    combined_score = entry_score * relevance_score * answer_score
                    
                    candidates.append({
                        'answer': answer,
                        'paragraph': para,
                        'entry_score': entry_score,
                        'relevance_score': relevance_score,
                        'answer_score': answer_score,
                        'combined_score': combined_score
                    })
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å€™é€‰ç­”æ¡ˆ
        if candidates:
            best_candidate = max(candidates, key=lambda x: x['combined_score'])
            final_answer = best_candidate['answer']
            used_paragraphs = [best_candidate['paragraph']]
        else:
            # å¦‚æœæ²¡æœ‰ç›¸å…³æ®µè½ï¼Œä½¿ç”¨Top-1æ¡ç›®
            context = entries[0].get('contents', '')[:500] if entries else ""
            prompt = self._construct_prompt(sample['question'], options, context)
            final_answer = self._generate(prompt, sample['image'])
            used_paragraphs = [context]
        
        return {
            'answer': self._map_letter_to_answer(final_answer, sample),
            'raw_prediction': final_answer,
            'retrieved_docs': used_paragraphs,
            'used_retrieval': True,
            'stage': 'Three-Stage Reflection',
            'num_candidates': len(candidates),
            'best_score': best_candidate['combined_score'] if candidates else 0.0
        }
```

**å…³é”®ç‰¹æ€§**:
1. âœ… ä¸‰é˜¶æ®µæ¨ç†æµç¨‹
2. âœ… Retrieval-Reflectionåˆ¤æ–­
3. âœ… æ®µè½çº§Relevance-Reflection
4. âœ… å±‚çº§åå¤„ç†ï¼ˆS_ret Ã— S_rel Ã— S_ansï¼‰
5. âœ… åªåœ¨ç›¸å…³æ®µè½ä¸Šç”Ÿæˆç­”æ¡ˆ

**æ³¨æ„äº‹é¡¹**:
- âš ï¸ ç®€åŒ–ç‰ˆ: æ— æ³•è·å–token-levelæ¦‚ç‡ï¼Œä½¿ç”¨å¯å‘å¼åˆ†æ•°
- âš ï¸ å®Œæ•´ç‰ˆéœ€è¦ä¿®æ”¹æ¨¡å‹æ¥å£ä»¥æ”¯æŒ`output_scores=True`
- âš ï¸ ç†æƒ³æƒ…å†µä¸‹åº”è¯¥åœ¨mRÂ²AG-ITæ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œä½†å¯ä»¥å…ˆç”¨zero-shotæµ‹è¯•

**ä¼˜å…ˆçº§**: ğŸ”´ P0 - æ ¸å¿ƒbaseline  
**é¢„è®¡æ—¶é—´**: 2-3å¤©

---

### Task 4: æµ‹è¯•Self-RAGå’ŒmRÂ²AG [é¢„è®¡: 0.5å¤©]

**æµ‹è¯•è„šæœ¬**:
```bash
# åˆ›å»ºæµ‹è¯•è„šæœ¬
cat > FlashRAG/experiments/test_corrected_baselines.py << 'EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""æµ‹è¯•ä¿®æ­£åçš„baselineå®ç°"""

import sys
sys.path.insert(0, '/root/autodl-tmp/FlashRAG')

from run_all_baselines_100samples import *

# æµ‹è¯•10ä¸ªæ ·æœ¬
config = CONFIG.copy()
config['max_samples'] = 10

print("=" * 60)
print("æµ‹è¯•ä¿®æ­£åçš„Baselineå®ç°")
print("=" * 60)

# åŠ è½½æ•°æ®
samples = load_dataset(config['dataset_path'], max_samples=10)
print(f"\nâœ… åŠ è½½ {len(samples)} ä¸ªæµ‹è¯•æ ·æœ¬")

# åˆå§‹åŒ–
qwen3_vl = initialize_qwen3_vl(config)
retriever = initialize_retriever(config, use_multimodal=False)

# æµ‹è¯•Self-RAG
print("\n" + "=" * 60)
print("1. æµ‹è¯•Self-RAG")
print("=" * 60)
self_rag = SelfRAGPipeline(qwen3_vl, retriever, config)

retrieval_count = 0
no_retrieval_count = 0

for i, sample in enumerate(samples[:10]):
    result = self_rag.run_single(sample)
    if result['used_retrieval']:
        retrieval_count += 1
    else:
        no_retrieval_count += 1
    
    print(f"\nSample {i+1}:")
    print(f"  Question: {sample['question'][:60]}...")
    print(f"  Decision: {result['retrieval_decision']}")
    print(f"  Used Retrieval: {result['used_retrieval']}")
    if result['used_retrieval']:
        print(f"  Relevant Docs: {result.get('num_relevant_docs', 0)}")

print(f"\næ£€ç´¢ç»Ÿè®¡:")
print(f"  ä½¿ç”¨æ£€ç´¢: {retrieval_count}/10 ({retrieval_count*10}%)")
print(f"  ä¸æ£€ç´¢: {no_retrieval_count}/10 ({no_retrieval_count*10}%)")

# æµ‹è¯•mRÂ²AG
print("\n" + "=" * 60)
print("2. æµ‹è¯•mRÂ²AG")
print("=" * 60)
mr2ag = MR2AGPipeline(qwen3_vl, retriever, config)

for i, sample in enumerate(samples[:5]):  # åªæµ‹è¯•5ä¸ªï¼ˆè¾ƒæ…¢ï¼‰
    result = mr2ag.run_single(sample)
    
    print(f"\nSample {i+1}:")
    print(f"  Question: {sample['question'][:60]}...")
    print(f"  Stage: {result['stage']}")
    print(f"  Used Retrieval: {result['used_retrieval']}")
    if result['used_retrieval']:
        print(f"  Candidates: {result['num_candidates']}")
        print(f"  Best Score: {result['best_score']:.4f}")

print("\n" + "=" * 60)
print("âœ… æµ‹è¯•å®Œæˆï¼")
print("=" * 60)
EOF

chmod +x FlashRAG/experiments/test_corrected_baselines.py

# è¿è¡Œæµ‹è¯•
cd /root/autodl-tmp
python FlashRAG/experiments/test_corrected_baselines.py
```

**éªŒè¯checklist**:
- [ ] Self-RAGçš„æ£€ç´¢ç‡åœ¨30-60%ä¹‹é—´ï¼ˆä¸æ˜¯100%ï¼‰
- [ ] Self-RAGèƒ½æ­£ç¡®è¯†åˆ«éœ€è¦å¤–éƒ¨çŸ¥è¯†çš„é—®é¢˜
- [ ] mRÂ²AGæ‰§è¡Œäº†ä¸‰é˜¶æ®µæµç¨‹
- [ ] mRÂ²AGç”Ÿæˆäº†å¤šä¸ªå€™é€‰ç­”æ¡ˆå¹¶é€‰æ‹©æœ€ä½³
- [ ] ä¸¤ä¸ªæ–¹æ³•çš„ç­”æ¡ˆè´¨é‡åˆç†

---

## ğŸ“Š é˜¶æ®µ2: 100æ ·æœ¬éªŒè¯ï¼ˆé¢„è®¡1å¤©ï¼‰

### Task 5: è¿è¡Œ100æ ·æœ¬å¯¹æ¯”å®éªŒ

**ä¿®æ”¹å®éªŒè„šæœ¬**:
```python
# åªè¿è¡Œä¿®æ­£åçš„baseline
METHODS_TO_RUN = [
    'self_aware_mrag',  # æˆ‘ä»¬çš„æ–¹æ³•
    'self_rag',         # ä¿®æ­£åçš„Self-RAG
    'mr2ag',            # ä¿®æ­£åçš„mRÂ²AG
    # æš‚æ—¶ä¸è¿è¡Œå…¶ä»–baseline
]
```

**è¿è¡Œå‘½ä»¤**:
```bash
cd /root/autodl-tmp/FlashRAG
nohup python experiments/run_all_baselines_100samples.py \
    --methods self_aware_mrag self_rag mr2ag \
    --max_samples 100 \
    > ../run_corrected_baselines_100.log 2>&1 &

# ç›‘æ§è¿›åº¦
tail -f ../run_corrected_baselines_100.log
```

**æœŸæœ›ç»“æœ**:
- Self-RAGçš„Recall@5åº”è¯¥ > 9.0ï¼ˆå› ä¸ºæœ‰è‡ªé€‚åº”æ£€ç´¢ï¼‰
- mRÂ²AGçš„Recall@5åº”è¯¥ > 9.0ï¼ˆå› ä¸ºæœ‰æ®µè½çº§åˆ¤æ–­ï¼‰
- EMå’ŒF1çš„å·®å¼‚åº”è¯¥æ›´åˆç†ï¼ˆä¸ä¼šå®Œå…¨ä¸€æ ·ï¼‰
- æˆ‘ä»¬çš„æ–¹æ³•åº”è¯¥ä»ç„¶æ˜¯æœ€å¥½çš„ï¼Œä½†å·®è·å¯èƒ½ä¼šç¼©å°

---

## ğŸ“Š é˜¶æ®µ3: å…¶ä»–Baselineï¼ˆæ ¹æ®éœ€è¦ï¼‰

### Task 6-8: VisRAGå®ç° [é¢„è®¡: 2-3å¤©]

**ç‰¹ç‚¹**:
- éœ€è¦ä¸“é—¨çš„è§†è§‰ç¼–ç å™¨
- è·¨æ¨¡æ€èåˆæœºåˆ¶
- è§†è§‰å¼•å¯¼æ£€ç´¢

**å‚è€ƒ**: `open_resource/VisRAG-master/`

### Task 9-11: REVEALå®ç° [é¢„è®¡: 2-3å¤©]

**ç‰¹ç‚¹**:
- è·¨æ¨¡æ€æ¨ç†
- çŸ¥è¯†å›¾è°±å¢å¼º

### Task 12-14: RagVLå®ç° [é¢„è®¡: 1-2å¤©]

**ç‰¹ç‚¹**:
- è§†è§‰è¯­è¨€è”åˆç¼–ç 

### Task 15-17: MuRAGå®ç° [é¢„è®¡: 2-3å¤©]

**ç‰¹ç‚¹**:
- å¤šæ¨¡æ€è®°å¿†
- è”åˆå¯¹æ¯”å­¦ä¹ 
- éœ€è¦é¢„è®­ç»ƒ

**å‚è€ƒ**: `open_resource/muragå¤ç°æ–‡æ¡£ï¼ˆåŸºäºè®ºæ–‡å’Œèµ„æ–™ï¼‰.md`

---

## ğŸ“Š é˜¶æ®µ4: å®Œæ•´å®éªŒï¼ˆé¢„è®¡1-2å¤©ï¼‰

### Task 18: è¿è¡Œå®Œæ•´æ•°æ®é›†

```bash
# æ‰€æœ‰baseline + å…¨éƒ¨æ ·æœ¬
python experiments/run_all_baselines_fullset.py \
    --max_samples 2000 \
    --methods all
```

### Task 19: ç»“æœåˆ†æå’Œè®ºæ–‡æ’°å†™

---

## ğŸ“‹ ä¼˜å…ˆçº§æ€»ç»“

| é˜¶æ®µ | ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„è®¡æ—¶é—´ | å¿…è¦æ€§ |
|------|------|--------|---------|--------|
| **é˜¶æ®µ1** | ä¿®å¤æŒ‡æ ‡é—®é¢˜ | ğŸ”´ P0 | 2å°æ—¶ | â­â­â­â­â­ |
| | Self-RAGå®ç° | ğŸ”´ P0 | 1-2å¤© | â­â­â­â­â­ |
| | mRÂ²AGå®ç° | ğŸ”´ P0 | 2-3å¤© | â­â­â­â­â­ |
| | æµ‹è¯•éªŒè¯ | ğŸ”´ P0 | 0.5å¤© | â­â­â­â­â­ |
| **é˜¶æ®µ2** | 100æ ·æœ¬å®éªŒ | ğŸ”´ P0 | 1å¤© | â­â­â­â­â­ |
| | ç»“æœåˆ†æ | ğŸ”´ P0 | 0.5å¤© | â­â­â­â­â­ |
| **é˜¶æ®µ3** | VisRAGå®ç° | ğŸŸ¡ P1 | 2-3å¤© | â­â­â­ |
| | REVEALå®ç° | ğŸŸ¢ P2 | 2-3å¤© | â­â­ |
| | RagVLå®ç° | ğŸŸ¢ P2 | 1-2å¤© | â­â­ |
| | MuRAGå®ç° | ğŸŸ¢ P2 | 2-3å¤© | â­ |
| **é˜¶æ®µ4** | å®Œæ•´å®éªŒ | ğŸ”´ P0 | 1-2å¤© | â­â­â­â­â­ |

**æ€»è®¡**:
- **é˜¶æ®µ1+2ï¼ˆæ ¸å¿ƒï¼‰**: 5-7å¤©
- **é˜¶æ®µ3ï¼ˆå¯é€‰ï¼‰**: 7-11å¤©
- **é˜¶æ®µ4ï¼ˆå¿…é¡»ï¼‰**: 1-2å¤©

---

## âœ… æˆåŠŸæ ‡å‡†

### é˜¶æ®µ1å®Œæˆæ ‡å‡†:
- [ ] 3ä¸ªæŒ‡æ ‡ä¸å†æ˜¯0
- [ ] Self-RAGçš„æ£€ç´¢ç‡ < 100%ï¼ˆæœ‰è‡ªé€‚åº”æ€§ï¼‰
- [ ] mRÂ²AGæ‰§è¡Œä¸‰é˜¶æ®µæµç¨‹
- [ ] 10æ ·æœ¬æµ‹è¯•é€šè¿‡

### é˜¶æ®µ2å®Œæˆæ ‡å‡†:
- [ ] 100æ ·æœ¬å®éªŒå®Œæˆ
- [ ] å„baselineçš„Recall@5ä¸å®Œå…¨ç›¸åŒ
- [ ] EM/F1åˆ†å¸ƒåˆç†
- [ ] æˆ‘ä»¬çš„æ–¹æ³•ä»ç„¶æœ€ä¼˜æˆ–æ¥è¿‘æœ€ä¼˜

### æœ€ç»ˆå®Œæˆæ ‡å‡†:
- [ ] æ‰€æœ‰æ ¸å¿ƒbaselineæ­£ç¡®å®ç°
- [ ] å®Œæ•´æ•°æ®é›†å®éªŒå®Œæˆ
- [ ] ç»“æœå¯ä¿¡ã€å¯å¤ç°
- [ ] å¯ä»¥ç”¨äºè®ºæ–‡æŠ•ç¨¿

---

**ä¸‹ä¸€æ­¥**: å¼€å§‹Task 1 - ä¿®å¤3ä¸ªæŒ‡æ ‡ä¸º0çš„é—®é¢˜

éœ€è¦æˆ‘ç«‹å³å¼€å§‹å—ï¼Ÿ

