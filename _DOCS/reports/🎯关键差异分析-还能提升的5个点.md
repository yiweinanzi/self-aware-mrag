# 🎯 Self-Aware-MRAG vs Baseline 关键差异分析

**日期**: 2025-10-31  
**当前性能**: EM 60.0%, F1 65.72%  
**目标**: 超越baseline (EM 61.0%, F1 68.89%)  
**差距**: -1.0% EM, -3.17% F1

---

## 🔍 发现的5个关键差异

### ⭐⭐⭐ 差异1: 缺少do_sample=False参数

**Self-Aware-MRAG** (当前):
```python
answer = self.qwen3_vl.generate(
    text=prompt,
    image=image,
    max_new_tokens=10,
    temperature=0.01
    # ❌ 缺少 do_sample 参数
)
```

**Baseline** (Self-RAG):
```python
answer = self.qwen3vl.generate(
    text=prompt,
    image=sample.get('image'),
    max_new_tokens=10,
    temperature=self.temperature,
    do_sample=False  # ✅ 明确设置
)
```

**影响**:
- temperature=0.01时，理论上应该是贪婪搜索
- 但没有明确设置do_sample=False可能导致行为不确定
- 在某些框架中，不设置do_sample可能默认为True

**修复**:
```python
answer = self.qwen3_vl.generate(
    text=prompt,
    image=image,
    max_new_tokens=10,
    temperature=0.01,
    do_sample=False  # ✅ 添加这行
)
```

**预期提升**: +0.5-1.0% ⭐⭐⭐  
**风险**: 极低  
**优先级**: 最高

---

### ⭐⭐⭐ 差异2: 文档长度截断

**Self-Aware-MRAG**:
```python
doc_text = doc[:512] if len(doc) > 512 else doc
```

**Baseline**:
```python
context = "\n\n".join(relevant_docs)  # 完整文档，不截断
```

**问题**:
- Self-Aware-MRAG截断到512字符
- Baseline使用完整文档
- 512字符可能截断关键信息（如长答案、详细说明）

**测试数据**:
| 配置 | EM | F1 | 说明 |
|------|-----|-----|------|
| 300字符 | 58.0% | 64.06% | 太短，信息丢失 ❌ |
| 512字符 | 60.0% | 65.72% | 当前配置 |
| 完整文档 | ?.?% | ?.?% | 与baseline一致 ✅ |

**修复选项**:

**选项A**: 完全移除截断（推荐）
```python
doc_text = doc  # 不截断，使用完整文档
```

**选项B**: 增加到1000字符
```python
doc_text = doc[:1000] if len(doc) > 1000 else doc
```

**选项C**: 增加到800字符
```python
doc_text = doc[:800] if len(doc) > 800 else doc
```

**预期提升**: +0.5-1.5% ⭐⭐⭐  
**风险**: 低（可能增加计算时间）  
**优先级**: 最高

---

### ⭐⭐ 差异3: Query Reformulation

**Self-Aware-MRAG**:
```python
enhanced_query = self.query_reformulator.reformulate(
    query=question,
    uncertainty_scores=uncertainty_info,
    modality=modality
)
# 使用enhanced_query进行检索
```

**Baseline**:
```python
# 直接使用原始question，不做任何改写
```

**已测试**:
- 禁用reformulation → EM 58.0% (下降2%) ❌
- 启用reformulation → EM 60.0% (当前) ✅

**结论**: 
- Query Reformulation是有益的
- 虽然它是Self-Aware-MRAG的独有特性
- **保持启用** ✅

**预期提升**: 0% (已是最优)  
**优先级**: 无需修改

---

### ⭐ 差异4: 答案映射位置

**Self-Aware-MRAG**:
```python
# 在forward_step()的最后做映射
if has_choices:
    answer_letter = final_answer.strip().upper()
    if answer_letter and answer_letter[0] in ['A', 'B', 'C', 'D']:
        choice_letter = answer_letter[0]
        mapped_answer = sample.get(choice_letter, final_answer)
        final_answer = mapped_answer
```

**Baseline**:
```python
# 在_generate_with_context()内部做映射
if all(k in sample for k in ['A', 'B', 'C', 'D']):
    return self._map_mc_answer(answer, sample)
```

**分析**:
- 逻辑基本相同
- 只是执行位置不同
- Self-Aware-MRAG的逻辑更清晰（在统一位置处理）

**结论**: 不太可能是性能差异的原因

**预期提升**: 0% (逻辑正确)  
**优先级**: 低

---

### ⭐ 差异5: 不检索时的Prompt

**Self-Aware-MRAG** (不检索时):
```python
# context="" 时
if has_choices:
    prompt = f"""Question: {core_question}

Choices:
A. {sample['A']}
B. {sample['B']}
C. {sample['C']}
D. {sample['D']}

Answer with the letter only (A/B/C/D):"""
else:
    prompt = f"""Question: {question}

Answer:"""
```

**Baseline** (不检索时):
```python
def _direct_answer(self, sample: Dict) -> str:
    if all(k in sample for k in ['A', 'B', 'C', 'D']):
        prompt = f"""Answer this question based on the image.

Question: {sample['question']}

Choices:
A. {sample['A']}
B. {sample['B']}
C. {sample['C']}
D. {sample['D']}

Answer with the letter only (A/B/C/D):"""
```

**差异**:
- Self-Aware-MRAG: 没有"based on the image"提示
- Baseline: 明确指出"based on the image"

**影响范围**: 
- 仅影响不检索的样本（~30-40%）
- 影响较小

**修复**:
```python
# 在不检索分支添加"based on the image"提示
if has_choices:
    prompt = f"""Answer this question based on the image.

Question: {core_question}

Choices:
A. {sample['A']}
B. {sample['B']}
C. {sample['C']}
D. {sample['D']}

Answer with the letter only (A/B/C/D):"""
```

**预期提升**: +0.2-0.5%  
**风险**: 极低  
**优先级**: 中等

---

## 🎯 推荐优化方案

### 方案A: 最保守（立即应用）⭐⭐⭐

**修改内容**:
1. ✅ 添加`do_sample=False`
2. ✅ 移除文档截断（使用完整文档）

**预期效果**:
- EM: 60.0% → 61.5-62.0% (+1.5-2.0%)
- F1: 65.72% → 67.5-69.0% (+1.8-3.3%)

**风险**: 极低  
**时间成本**: 修改2行代码，重跑6小时

**实施**:
```python
# 修改1: Line 563 附近
answer = self.qwen3_vl.generate(
    text=prompt,
    image=image,
    max_new_tokens=10,
    temperature=0.01,
    do_sample=False  # ✅ 添加
)

# 修改2: Line 489 附近  
doc_text = doc  # ✅ 移除截断，使用完整文档
```

---

### 方案B: 激进（组合所有优化）⭐⭐

**修改内容**:
1. ✅ 添加`do_sample=False`
2. ✅ 移除文档截断
3. ✅ 优化不检索时的prompt

**预期效果**:
- EM: 60.0% → 62.0-63.0% (+2.0-3.0%)
- F1: 65.72% → 68.0-70.0% (+2.3-4.3%)

**风险**: 低  
**时间成本**: 修改5行代码，重跑6小时

---

### 方案C: 单独测试（逐个验证）⭐

**步骤**:
1. 只添加`do_sample=False`，测试 → 如果提升，继续
2. 移除文档截断，测试 → 如果提升，继续
3. 优化不检索prompt，测试

**优点**: 能确定每个优化的独立贡献  
**缺点**: 需要3次实验，18小时

---

## 💡 我的建议

### 🎯 推荐：**方案A**（最保守）

**理由**:
1. **do_sample=False是对齐baseline的必要修改**
   - 这是明确的差异
   - 风险极低，收益稳定

2. **移除文档截断是合理的**
   - Baseline使用完整文档
   - 我们的512字符可能太保守
   - 信息完整性 > 计算速度

3. **组合效果预期达到61-62%**
   - 足以匹配或超越baseline
   - 证明方法有效性

4. **如果还不够，再应用方案B的其他优化**

---

## 📊 优化效果预测

| 版本 | 修改内容 | EM | F1 | 说明 |
|------|---------|-----|-----|------|
| **当前** | Prompt Fix | 60.0% | 65.72% | 运行中 |
| **+do_sample** | +方案A第1项 | 60.5-61.0% | 66.5-67.5% | 低风险 |
| **+完整文档** | +方案A第2项 | 61.5-62.0% | 67.5-69.0% | ✅ 目标达成 |
| **+优化prompt** | +方案B第3项 | 62.0-63.0% | 68.0-70.0% | 超越baseline |

---

## ⏰ 时间安排

**当前实验** (Prompt Fix):
- 启动时间: 10:23
- 预计完成: 16:00-17:00
- 等待结果，然后决定下一步

**如果EM<61%**:
- 立即应用方案A
- 重跑实验
- 预计明天早上有结果

**如果EM≥61%**:
- ✅ 成功！可以考虑是否继续优化

---

## 📝 总结

发现的关键问题：
1. ❌ **缺少do_sample=False** - 明确的对齐问题
2. ❌ **512字符截断** - 可能丢失关键信息
3. ✅ Query Reformulation - 已验证有益
4. ✅ 答案映射 - 逻辑正确
5. ⚠️ 不检索prompt - 影响较小但可优化

**最有把握的提升点**：do_sample=False + 完整文档

**预期最终性能**：EM 61-63%, F1 67-70%

---

**报告生成时间**: 2025-10-31 10:45  
**状态**: 当前实验运行中，待结果后应用优化

