# ğŸ” æ–¹æ³•å®ç°æ·±åº¦å®¡æŸ¥æŠ¥å‘Š

**å®¡æŸ¥æ—¶é—´**: 2025-11-13  
**å®¡æŸ¥èŒƒå›´**: æ ¸å¿ƒåˆ›æ–°ç‚¹å®ç° vs å‚è€ƒæ–‡æ¡£è¦æ±‚  
**å‚è€ƒæ–‡æ¡£**: 
- `refernce/åˆ›æ–°ç‚¹1-è‡ªæ„ŸçŸ¥å¤šæ¨¡æ€RAG-å®æ–½æ–¹æ¡ˆ.md`
- `refernce/å¯¼å¸ˆæ„è§ç‰ˆ.md`
- SeaKRè®ºæ–‡å’Œå¼€æºä»£ç 
- VisRAGè®ºæ–‡å’Œå¼€æºä»£ç 

---

## ğŸ“‹ å®¡æŸ¥æ€»è§ˆ

| åˆ›æ–°ç‚¹ | æ–‡æ¡£è¦æ±‚ | å®ç°çŠ¶æ€ | å®Œæˆåº¦ | é—®é¢˜ |
|--------|---------|---------|--------|------|
| **åˆ›æ–°1: è·¨æ¨¡æ€ä¸ç¡®å®šæ€§ä¼°è®¡** | SeaKRæ‰©å±•åˆ°å¤šæ¨¡æ€ | âœ… éƒ¨åˆ†å®ç° | 70% | âš ï¸ å…³é”®é—®é¢˜ |
| **åˆ›æ–°2: ä½ç½®æ„ŸçŸ¥èåˆ** | ä¸ç¡®å®šæ€§é©±åŠ¨çš„ä½ç½®æƒé‡ | âœ… åŸºæœ¬å®ç° | 65% | âš ï¸ å…³è”ä¸è¶³ |
| **æ”¯æ’‘: å¯è§£é‡Šæ€§å½’å› ** | æ–‡æ¡£çº§å½’å›  | âœ… å®ç° | 80% | âœ… ç¬¦åˆç®€åŒ–è¦æ±‚ |

**æ€»ä½“è¯„ä¼°**: ğŸŸ¡ **åŸºæœ¬å®ç°ï¼Œä½†å­˜åœ¨å…³é”®é—®é¢˜éœ€è¦ä¿®å¤**

---

## ğŸ”´ åˆ›æ–°ç‚¹1: è·¨æ¨¡æ€ä¸ç¡®å®šæ€§ä¼°è®¡ - æ·±åº¦å®¡æŸ¥

### ğŸ“– æ–‡æ¡£è¦æ±‚ (å¯¼å¸ˆæ„è§ç‰ˆ Line 21-24)

```
1. è·¨æ¨¡æ€è‡ªæ„ŸçŸ¥ä¸ç¡®å®šæ€§ä¼°è®¡
   - æ‰©å±•SeaKR (ACL 2024)åˆ°å¤šæ¨¡æ€åœºæ™¯
   - æ–‡æœ¬ä¸ç¡®å®šæ€§ï¼ˆGramçŸ©é˜µï¼‰+ è§†è§‰ä¸ç¡®å®šæ€§ + å¯¹é½ä¸ç¡®å®šæ€§
   - è‡ªé€‚åº”æ£€ç´¢è§¦å‘ + æ¨¡æ€é€‰æ‹©
```

### ğŸ“– è¯¦ç»†è¦æ±‚ (åˆ›æ–°ç‚¹1 Line 805-849)

```python
# æ–‡æœ¬ä¸ç¡®å®šæ€§ï¼šSeaKRçš„eigen_score
eigen_score = (1/k) * log|Î£ + Î±*I|
å…¶ä¸­ï¼šÎ£ = z * J_d * z^T

# è§†è§‰ä¸ç¡®å®šæ€§ï¼šæ³¨æ„åŠ›åˆ†å¸ƒæ–¹å·®
visual_uncertainty = var(attention_weights)

# å¯¹é½ä¸ç¡®å®šæ€§ï¼šJSæ•£åº¦
alignment_uncertainty = JS(P_text || P_visual)
```

### âœ… å®ç°æƒ…å†µ

**æ–‡ä»¶**: `FlashRAG/flashrag/modules/uncertainty_estimator.py`

#### 1.1 æ–‡æœ¬ä¸ç¡®å®šæ€§ - SeaKR eigen_score

**å®ç°ä»£ç ** (Line 494-539):
```python
def compute_eigen_score(self, embeddings) -> float:
    z = embeddings.to(torch.float32)
    k, d = z.shape
    
    # Centering matrix
    j_d = torch.eye(d) - (1/d) * torch.ones(d, d)
    
    # åæ–¹å·®çŸ©é˜µ
    sigma = torch.einsum('ij,jk,kl->il', z, j_d, z.t())
    
    # æ·»åŠ æ­£åˆ™åŒ–
    matrix = sigma + self.eigen_alpha * torch.eye(k, device=sigma.device)
    
    # log|Î£ + Î±*I|
    eigen_score = (1/k) * torch.logdet(matrix)
    
    return eigen_score.item()
```

**âœ… ç¬¦åˆåº¦**: 95%
- âœ… å…¬å¼æ­£ç¡®ï¼šå®Œå…¨æŒ‰ç…§SeaKRè®ºæ–‡å®ç°
- âœ… æ­£åˆ™åŒ–å‚æ•°ï¼šÎ± = 1e-10ï¼ˆä¸SeaKRä¸€è‡´ï¼‰
- âœ… é˜ˆå€¼åˆ¤æ–­ï¼šeigen_threshold = -6.0ï¼ˆä¸SeaKRä¸€è‡´ï¼‰

**âŒ å…³é”®é—®é¢˜**:
```python
# Line 96-97
self.alpha = self.config.get('text_weight', 0.0)  # âš ï¸ æ–‡æœ¬æƒé‡=0ï¼
```

**ğŸ”´ ä¸¥é‡é—®é¢˜**: æ–‡æœ¬ä¸ç¡®å®šæ€§æƒé‡è¢«è®¾ç½®ä¸º0ï¼Œå¯¼è‡´**SeaKRçš„æ ¸å¿ƒåˆ›æ–°è¢«å®Œå…¨ç¦ç”¨**ï¼

**åŸå› æ³¨é‡Š** (Line 96):
```python
# âš ï¸ ä¸´æ—¶ï¼šæ–‡æœ¬ä¸ç¡®å®šæ€§è®¡ç®—å¤æ‚ï¼ˆéœ€è¦kæ¬¡é‡‡æ ·ï¼‰ï¼Œæš‚æ—¶ç¦ç”¨
```

**å½±å“**:
- æ€»ä¸ç¡®å®šæ€§å…¬å¼å˜ä¸º: `U_total = 0.0 Ã— U_text + 0.5 Ã— U_visual + 0.5 Ã— U_align`
- **SeaKRçš„æ ¸å¿ƒè´¡çŒ®è¢«å¿½ç•¥**
- æ— æ³•å£°ç§°"æ‰©å±•SeaKRåˆ°å¤šæ¨¡æ€"

---

#### 1.2 è§†è§‰ä¸ç¡®å®šæ€§

**å®ç°ä»£ç ** (Line 236-272):
```python
def estimate_visual_uncertainty(self, image, hidden_states=None):
    # æ–¹æ³•1: CLIPç‰¹å¾ç»Ÿè®¡
    if self.use_clip_for_alignment:
        clip_features = self.clip_model.encode_image(image)
        
        # ç‰¹å¾èŒƒæ•°
        feature_norm = torch.norm(clip_features, p=2, dim=-1)
        
        # ç‰¹å¾æ ‡å‡†å·®
        feature_std = torch.std(clip_features, dim=-1)
        
        # ç‰¹å¾å‡å€¼
        feature_mean = torch.mean(torch.abs(clip_features), dim=-1)
        
        # åŠ æƒç»„åˆ
        visual_unc = 0.4 * feature_norm + 0.3 * feature_std + 0.3 * feature_mean
        
        return visual_unc.item()
```

**âš ï¸ ç¬¦åˆåº¦**: 50%
- âœ… ä½¿ç”¨CLIPç‰¹å¾
- âŒ **ä¸æ˜¯æ–‡æ¡£è¦æ±‚çš„"æ³¨æ„åŠ›åˆ†å¸ƒæ–¹å·®"**
- âŒ ä½¿ç”¨çš„æ˜¯ç‰¹å¾ç»Ÿè®¡ï¼ˆèŒƒæ•°ã€æ ‡å‡†å·®ã€å‡å€¼ï¼‰ï¼Œè€Œéattention variance

**æ–‡æ¡£è¦æ±‚** (åˆ›æ–°ç‚¹1 Line 820):
```python
# åº”è¯¥æ˜¯ï¼š
visual_uncertainty = var(attention_weights)  # æ³¨æ„åŠ›æƒé‡çš„æ–¹å·®
```

**å½“å‰å®ç°**:
```python
# å®é™…æ˜¯ï¼š
visual_uncertainty = 0.4*norm + 0.3*std + 0.3*mean  # ç‰¹å¾ç»Ÿè®¡
```

**é—®é¢˜**: æ–¹æ³•ä¸ç¬¦åˆæ–‡æ¡£æè¿°ï¼Œç¼ºä¹ç†è®ºæ”¯æ’‘

---

#### 1.3 å¯¹é½ä¸ç¡®å®šæ€§

**å®ç°ä»£ç ** (Line 493-525):
```python
def compute_js_divergence(self, text_dist, visual_dist):
    # Jensen-Shannonæ•£åº¦
    m = 0.5 * (text_dist + visual_dist)
    
    kl_text_m = F.kl_div(
        torch.log(m + 1e-10),
        text_dist,
        reduction='batchmean'
    )
    
    kl_visual_m = F.kl_div(
        torch.log(m + 1e-10),
        visual_dist,
        reduction='batchmean'
    )
    
    js_div = 0.5 * (kl_text_m + kl_visual_m)
    
    return js_div.item()
```

**âœ… ç¬¦åˆåº¦**: 90%
- âœ… å…¬å¼æ­£ç¡®ï¼šJS(P||Q) = 0.5*KL(P||M) + 0.5*KL(Q||M)
- âœ… ä½¿ç”¨CLIPç‰¹å¾è®¡ç®—åˆ†å¸ƒ
- âš ï¸ åˆ†å¸ƒæ„é€ æ–¹å¼å¯èƒ½éœ€è¦ä¼˜åŒ–

---

### ğŸ”´ æ ¸å¿ƒé—®é¢˜æ€»ç»“

#### é—®é¢˜1: æ–‡æœ¬ä¸ç¡®å®šæ€§è¢«ç¦ç”¨ (ä¸¥é‡)

**å½“å‰çŠ¶æ€**:
```python
U_total = 0.0 Ã— U_text + 0.5 Ã— U_visual + 0.5 Ã— U_align
```

**åº”è¯¥æ˜¯**:
```python
U_total = 0.4 Ã— U_text + 0.3 Ã— U_visual + 0.3 Ã— U_align  # å¯¼å¸ˆæ„è§ç‰ˆ Line 243
```

**ä¿®å¤æ–¹æ¡ˆ**:
1. å®ç°kæ¬¡é‡‡æ ·ï¼ˆSeaKRæ–¹æ³•ï¼‰
2. æˆ–ä½¿ç”¨å•æ¬¡ç”Ÿæˆçš„hidden statesè®¡ç®—GramçŸ©é˜µ
3. å¯ç”¨text_weight = 0.4

---

#### é—®é¢˜2: è§†è§‰ä¸ç¡®å®šæ€§æ–¹æ³•ä¸ç¬¦åˆæ–‡æ¡£

**æ–‡æ¡£è¦æ±‚**: æ³¨æ„åŠ›åˆ†å¸ƒæ–¹å·®
**å½“å‰å®ç°**: CLIPç‰¹å¾ç»Ÿè®¡

**ä¿®å¤æ–¹æ¡ˆ**:
1. ä»MLLMæå–attention weights
2. è®¡ç®—attention variance
3. æˆ–åœ¨è®ºæ–‡ä¸­æ˜ç¡®è¯´æ˜ä½¿ç”¨CLIPç‰¹å¾ç»Ÿè®¡çš„ç†è®ºä¾æ®

---

#### é—®é¢˜3: ç¼ºå°‘kæ¬¡é‡‡æ ·æœºåˆ¶

**SeaKRæ ¸å¿ƒ**: éœ€è¦k=20æ¬¡é‡‡æ ·æ¥è®¡ç®—è¯­ä¹‰ç†µ

**å½“å‰å®ç°**: å•æ¬¡ç”Ÿæˆ

**å½±å“**: æ— æ³•è®¡ç®—çœŸæ­£çš„è¯­ä¹‰ä¸ç¡®å®šæ€§

**ä¿®å¤æ–¹æ¡ˆ**:
1. å®ç°sampling-based uncertaintyï¼ˆå‚è€ƒSeaKR Line 77-86ï¼‰
2. æˆ–ä½¿ç”¨hidden statesçš„åæ–¹å·®çŸ©é˜µï¼ˆç®€åŒ–ç‰ˆï¼‰

---

## ğŸŸ¡ åˆ›æ–°ç‚¹2: ä½ç½®æ„ŸçŸ¥èåˆ - æ·±åº¦å®¡æŸ¥

### ğŸ“– æ–‡æ¡£è¦æ±‚ (å¯¼å¸ˆæ„è§ç‰ˆ Line 26-29)

```
2. ä¸ç¡®å®šæ€§é©±åŠ¨çš„ä½ç½®æ„ŸçŸ¥èåˆ
   - ç¼“è§£"Lost in the Middle"é—®é¢˜
   - ä¸ç¡®å®šæ€§è°ƒåˆ¶çš„ä½ç½®æƒé‡ï¼ˆè€Œéç®€å•å€Ÿé‰´VisRAGï¼‰
   - åŒå‘è·¨æ¨¡æ€æ³¨æ„åŠ›é‡åŠ æƒ
```

### âœ… å®ç°æƒ…å†µ

**æ–‡ä»¶**: `FlashRAG/flashrag/modules/position_aware_fusion.py`

#### 2.1 ä½ç½®åŠ æƒæ± åŒ–

**å®ç°ä»£ç ** (Line 102-150):
```python
def position_weighted_pooling(self, multimodal_tokens, positions=None):
    # è®¡ç®—ä½ç½®æƒé‡
    position_weights = self._compute_position_weights(tokens, positions)
    
    # åŠ æƒæ± åŒ–
    weighted_features = multimodal_tokens * position_weights.unsqueeze(-1)
    
    return weighted_features
```

**Uå‹æƒé‡åˆ†å¸ƒ** (Line 293-314):
```python
def _get_u_shaped_weights(self, seq_len: int):
    weights = torch.zeros(seq_len)
    
    for i in range(seq_len):
        if i < seq_len // 3:
            weights[i] = 1.0  # å¼€å¤´
        elif i > 2 * seq_len // 3:
            weights[i] = 0.9  # ç»“å°¾
        else:
            weights[i] = 0.6  # ä¸­é—´ï¼ˆLost in the middleï¼‰
    
    return weights
```

**âœ… ç¬¦åˆåº¦**: 80%
- âœ… å®ç°Uå‹æƒé‡åˆ†å¸ƒ
- âœ… ç¼“è§£Lost in the Middleé—®é¢˜
- âš ï¸ **ä½†æƒé‡æ˜¯å›ºå®šçš„ï¼Œæ²¡æœ‰"ä¸ç¡®å®šæ€§è°ƒåˆ¶"**

---

#### 2.2 åŒå‘è·¨æ¨¡æ€æ³¨æ„åŠ›

**å®ç°ä»£ç ** (Line 152-207):
```python
def cross_modal_attention_reweighting(self, text_features, visual_features):
    # æ–‡æœ¬å¼•å¯¼çš„è§†è§‰æ³¨æ„åŠ›
    text_guided_visual, _ = self.text_to_visual_attention(
        query=visual_features,
        key=text_features,
        value=text_features
    )
    
    # è§†è§‰å¼•å¯¼çš„æ–‡æœ¬æ³¨æ„åŠ›
    visual_guided_text, _ = self.visual_to_text_attention(
        query=text_features,
        key=visual_features,
        value=visual_features
    )
    
    return text_guided_visual, visual_guided_text
```

**âœ… ç¬¦åˆåº¦**: 90%
- âœ… å®ç°åŒå‘è·¨æ¨¡æ€æ³¨æ„åŠ›
- âœ… ä½¿ç”¨PyTorch MultiheadAttention
- âœ… ç¬¦åˆæ–‡æ¡£è¦æ±‚

---

### ğŸ”´ æ ¸å¿ƒé—®é¢˜: ç¼ºå°‘"ä¸ç¡®å®šæ€§è°ƒåˆ¶"

**æ–‡æ¡£è¦æ±‚** (å¯¼å¸ˆæ„è§ç‰ˆ Line 28):
```
ä¸ç¡®å®šæ€§è°ƒåˆ¶çš„ä½ç½®æƒé‡ï¼ˆè€Œéç®€å•å€Ÿé‰´VisRAGï¼‰
```

**å½“å‰å®ç°**:
```python
# ä½ç½®æƒé‡æ˜¯å›ºå®šçš„
weights[i] = 1.0  # å¼€å¤´
weights[i] = 0.6  # ä¸­é—´
weights[i] = 0.9  # ç»“å°¾
```

**åº”è¯¥æ˜¯**:
```python
# ä½ç½®æƒé‡åº”è¯¥ç”±ä¸ç¡®å®šæ€§è°ƒåˆ¶
weights[i] = base_weight[i] * f(uncertainty)
```

**ä¿®å¤æ–¹æ¡ˆ**:
```python
def _compute_position_weights(self, tokens, positions, uncertainty_scores=None):
    # åŸºç¡€Uå‹æƒé‡
    base_weights = self._get_u_shaped_weights(seq_len)
    
    # ä¸ç¡®å®šæ€§è°ƒåˆ¶
    if uncertainty_scores is not None:
        # é«˜ä¸ç¡®å®šæ€§ â†’ å¢å¼ºä½ç½®åå·®ç¼“è§£
        modulation = 1.0 + uncertainty_scores['total'] * 0.5
        weights = base_weights * modulation
    else:
        weights = base_weights
    
    return weights
```

---

## âœ… æ”¯æ’‘æ¨¡å—: å¯è§£é‡Šæ€§å½’å› 

### ğŸ“– æ–‡æ¡£è¦æ±‚ (å¯¼å¸ˆæ„è§ç‰ˆ Line 33-36)

```
3. å¯è§£é‡Šæ€§æ”¯æ’‘ï¼ˆé™çº§ä¸ºæ”¯æ’‘æ¨¡å—ï¼‰
   - æ–‡æ¡£çº§å½’å› ï¼ˆç®€åŒ–ï¼Œä¸åšRegion-levelï¼‰
   - ç®€åŒ–å®ç°ï¼ˆä¸åšToken-levelï¼‰
   - å½’å› ç½®ä¿¡åº¦ï¼ˆç”±ä¸ç¡®å®šæ€§è°ƒåˆ¶ï¼‰
```

### âœ… å®ç°æƒ…å†µ

**æ–‡ä»¶**: `FlashRAG/flashrag/modules/attribution.py`

**âœ… ç¬¦åˆåº¦**: 80%
- âœ… å®ç°æ–‡æ¡£çº§å½’å› 
- âœ… å½’å› ç½®ä¿¡åº¦è®¡ç®—
- âœ… ç¬¦åˆç®€åŒ–è¦æ±‚

**æ— é‡å¤§é—®é¢˜**

---

## ğŸ“Š å®ç°å®Œæˆåº¦æ€»ç»“

### åˆ›æ–°ç‚¹1: è·¨æ¨¡æ€ä¸ç¡®å®šæ€§ä¼°è®¡

| å­æ¨¡å— | è¦æ±‚ | å®ç° | å®Œæˆåº¦ | é—®é¢˜ |
|--------|------|------|--------|------|
| æ–‡æœ¬ä¸ç¡®å®šæ€§ | SeaKR eigen_score | âœ… ä»£ç æ­£ç¡® | 95% | ğŸ”´ æƒé‡=0ï¼Œè¢«ç¦ç”¨ |
| è§†è§‰ä¸ç¡®å®šæ€§ | Attention variance | âŒ ç”¨CLIPç»Ÿè®¡ | 50% | ğŸ”´ æ–¹æ³•ä¸ç¬¦ |
| å¯¹é½ä¸ç¡®å®šæ€§ | JSæ•£åº¦ | âœ… å®ç° | 90% | âœ… åŸºæœ¬æ­£ç¡® |
| kæ¬¡é‡‡æ · | SeaKRæ ¸å¿ƒ | âŒ æœªå®ç° | 0% | ğŸ”´ ç¼ºå¤± |
| è‡ªé€‚åº”æ£€ç´¢ | é˜ˆå€¼åˆ¤æ–­ | âœ… å®ç° | 90% | âœ… æ­£ç¡® |

**æ€»ä½“**: 70% - **å­˜åœ¨ä¸¥é‡é—®é¢˜**

---

### åˆ›æ–°ç‚¹2: ä½ç½®æ„ŸçŸ¥èåˆ

| å­æ¨¡å— | è¦æ±‚ | å®ç° | å®Œæˆåº¦ | é—®é¢˜ |
|--------|------|------|--------|------|
| Uå‹æƒé‡ | Lost in the Middle | âœ… å®ç° | 80% | âš ï¸ å›ºå®šæƒé‡ |
| ä¸ç¡®å®šæ€§è°ƒåˆ¶ | åŠ¨æ€æƒé‡ | âŒ æœªå®ç° | 0% | ğŸ”´ ç¼ºå¤± |
| åŒå‘æ³¨æ„åŠ› | è·¨æ¨¡æ€é‡åŠ æƒ | âœ… å®ç° | 90% | âœ… æ­£ç¡® |

**æ€»ä½“**: 65% - **ç¼ºå°‘æ ¸å¿ƒå…³è”**

---

## ğŸ¯ å…³é”®ä¿®å¤å»ºè®®

### ä¼˜å…ˆçº§P0 (å¿…é¡»ä¿®å¤)

1. **å¯ç”¨æ–‡æœ¬ä¸ç¡®å®šæ€§** (åˆ›æ–°ç‚¹1æ ¸å¿ƒ)
   - ä¿®æ”¹: `text_weight: 0.0 â†’ 0.4`
   - å®ç°kæ¬¡é‡‡æ ·æˆ–ä½¿ç”¨hidden states
   - å¦åˆ™æ— æ³•å£°ç§°"æ‰©å±•SeaKR"

2. **å®ç°ä¸ç¡®å®šæ€§è°ƒåˆ¶çš„ä½ç½®æƒé‡** (åˆ›æ–°ç‚¹2æ ¸å¿ƒ)
   - å½“å‰: å›ºå®šæƒé‡
   - ä¿®æ”¹: `weights = base_weights * f(uncertainty)`
   - å¦åˆ™åˆ›æ–°ç‚¹1å’Œ2æ²¡æœ‰å…³è”

### ä¼˜å…ˆçº§P1 (å¼ºçƒˆå»ºè®®)

3. **ä¿®å¤è§†è§‰ä¸ç¡®å®šæ€§è®¡ç®—æ–¹æ³•**
   - å½“å‰: CLIPç‰¹å¾ç»Ÿè®¡
   - ä¿®æ”¹: Attention variance
   - æˆ–åœ¨è®ºæ–‡ä¸­è¯´æ˜ç†è®ºä¾æ®

4. **å®ç°kæ¬¡é‡‡æ ·æœºåˆ¶**
   - å‚è€ƒSeaKR Line 77-86
   - è®¡ç®—çœŸæ­£çš„è¯­ä¹‰ç†µ

---

## ğŸ“ è®ºæ–‡æ’°å†™å»ºè®®

### Methodéƒ¨åˆ†éœ€è¦æ˜ç¡®è¯´æ˜

1. **æ–‡æœ¬ä¸ç¡®å®šæ€§**:
   - å¦‚æœä½¿ç”¨ç®€åŒ–ç‰ˆï¼ˆå•æ¬¡ç”Ÿæˆï¼‰ï¼Œéœ€è¦è¯´æ˜åŸå› 
   - å¦‚æœå®ç°kæ¬¡é‡‡æ ·ï¼Œéœ€è¦è¯¦ç»†æè¿°

2. **è§†è§‰ä¸ç¡®å®šæ€§**:
   - å½“å‰ä½¿ç”¨CLIPç‰¹å¾ç»Ÿè®¡ï¼Œéœ€è¦ç†è®ºè®ºè¯
   - æˆ–æ”¹ä¸ºattention variance

3. **åˆ›æ–°ç‚¹å…³è”**:
   - å¿…é¡»è¯´æ˜"ä¸ç¡®å®šæ€§å¦‚ä½•é©±åŠ¨ä½ç½®æƒé‡"
   - å½“å‰å®ç°ä¸­ä¸¤è€…æ˜¯ç‹¬ç«‹çš„

---

**å®¡æŸ¥ç»“è®º**: ğŸŸ¡ åŸºæœ¬æ¡†æ¶æ­£ç¡®ï¼Œä½†å­˜åœ¨**3ä¸ªP0çº§åˆ«é—®é¢˜**éœ€è¦ç«‹å³ä¿®å¤ï¼Œå¦åˆ™æ— æ³•æ”¯æ’‘è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°å£°æ˜ã€‚

---

## ğŸ”¬ SeaKRåŸå§‹å®ç°å¯¹æ¯”

### SeaKRæ ¸å¿ƒä»£ç  (vllm/engine/llm_engine.py Line 714-744)

```python
def compute_uncertainty(self, request_output: RequestOutput):
    for cpl_output in request_output.outputs:
        self._compute_single_uncertainty(cpl_output)

    uncertainty_dict = {}

    # å…³é”®ï¼šéœ€è¦å¤šä¸ªæ ·æœ¬ï¼ˆk > 1ï¼‰
    if len(request_output.outputs) > 1:
        # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„EOS embedding
        valid_embeddings = [
            getattr(cpl, 'eos_embedding', None)
            for cpl in request_output.outputs
            if cpl.text.strip()
        ]

        if valid_embeddings:
            eos_embeddings = torch.stack(valid_embeddings)  # [k, d]

            # è®¡ç®—eigen_score
            uncertainty_dict['eigen_score'] = self._compute_eigen_score(eos_embeddings)

        # è®¡ç®—è¯­ä¹‰ç†µï¼ˆln_entropyï¼‰
        all_perplexities = [
            cpl.uncertainty["perplexity"]
            for cpl in request_output.outputs
            if "perplexity" in cpl.uncertainty
        ]
        if all_perplexities:
            uncertainty_dict['ln_entropy'] = np.mean(all_perplexities)

    else:
        # å•æ ·æœ¬ï¼šåªèƒ½è®¡ç®—perplexityå’Œenergy_score
        uncertainty_dict['perplexity'] = request_output.outputs[0].uncertainty.get('perplexity', 1e3)
        uncertainty_dict['energy_score'] = request_output.outputs[0].uncertainty.get('energy_score', 0)

    setattr(request_output, 'uncertainty', uncertainty_dict)

def _compute_eigen_score(self, z: torch.tensor):
    """
    å®Œå…¨ç›¸åŒçš„å®ç°ï¼
    """
    z = z.to(torch.float32)
    k, d = z.shape
    j_d = torch.eye(d) - (1/d) * torch.ones(d, d)
    j_d = j_d.to(z.device)
    sigma = torch.einsum('ij,jk,kl->il', z, j_d, z.t())
    return ((1/k) * torch.logdet(sigma + self.eigen_alpha * torch.eye(k, device=sigma.device))).item()
```

### æˆ‘ä»¬çš„å®ç°å¯¹æ¯”

**âœ… å…¬å¼å®ç°**: 100%ä¸€è‡´
- æˆ‘ä»¬çš„`compute_eigen_score()`ä¸SeaKRå®Œå…¨ç›¸åŒï¼ˆLine 494-539ï¼‰

**âŒ ä½¿ç”¨æ–¹å¼**: 0%ä¸€è‡´
- SeaKR: éœ€è¦k=20ä¸ªæ ·æœ¬çš„embeddings
- æˆ‘ä»¬: å•æ¬¡ç”Ÿæˆï¼Œæ— æ³•è®¡ç®—eigen_score

**âŒ é‡‡æ ·æœºåˆ¶**: ç¼ºå¤±
- SeaKR: `SamplingParams(n=20, temperature=1.0)` (reasoner.py Line 77-86)
- æˆ‘ä»¬: æ— é‡‡æ ·æœºåˆ¶

---

## ğŸ” å…³é”®å‘ç°ï¼šæˆ‘ä»¬çš„å®ç°ä¸SeaKRçš„æ ¹æœ¬å·®å¼‚

### SeaKRçš„å®Œæ•´æµç¨‹

```python
# 1. åŒæ—¶ç”Ÿæˆgreedyå’Œsampleä¸¤ä¸ªè¯·æ±‚
greedy_params = SamplingParams(n=1, temperature=0.0)
sample_params = SamplingParams(n=20, temperature=1.0)  # 20ä¸ªæ ·æœ¬ï¼

# 2. æ”¶é›†20ä¸ªæ ·æœ¬çš„EOS embeddings
eos_embeddings = torch.stack([
    sample1.eos_embedding,  # [d]
    sample2.eos_embedding,  # [d]
    ...
    sample20.eos_embedding  # [d]
])  # æœ€ç»ˆ: [20, d]

# 3. è®¡ç®—eigen_score
eigen_score = (1/20) * log|Î£ + Î±I|
```

### æˆ‘ä»¬çš„å½“å‰æµç¨‹

```python
# 1. å•æ¬¡ç”Ÿæˆ
response = model.generate(prompt)  # åªæœ‰1ä¸ªè¾“å‡º

# 2. æ— æ³•æ”¶é›†å¤šä¸ªembeddings
# âŒ æ²¡æœ‰20ä¸ªæ ·æœ¬

# 3. æ— æ³•è®¡ç®—eigen_score
# âŒ éœ€è¦ k > 1 æ‰èƒ½è®¡ç®—åæ–¹å·®çŸ©é˜µ
```

---

## ğŸ”´ P0é—®é¢˜è¯¦ç»†åˆ†æ

### é—®é¢˜1: æ–‡æœ¬ä¸ç¡®å®šæ€§å®Œå…¨æœªå¯ç”¨

**ä»£ç è¯æ®**:
```python
# FlashRAG/flashrag/modules/uncertainty_estimator.py Line 96-97
self.alpha = self.config.get('text_weight', 0.0)  # âš ï¸ æƒé‡=0

# Line 127-135
def estimate(self, text, image=None):
    # æ–‡æœ¬ä¸ç¡®å®šæ€§
    text_unc = self.estimate_text_uncertainty(text)  # è®¡ç®—äº†

    # ä½†åœ¨èåˆæ—¶è¢«å¿½ç•¥
    total_unc = (
        self.alpha * text_unc +      # 0.0 Ã— text_unc = 0
        self.beta * visual_unc +     # 0.5 Ã— visual_unc
        self.gamma * alignment_unc   # 0.5 Ã— alignment_unc
    )
```

**å½±å“**:
- SeaKRçš„æ ¸å¿ƒè´¡çŒ®ï¼ˆeigen_scoreï¼‰è¢«å®Œå…¨å¿½ç•¥
- æ— æ³•å£°ç§°"æ‰©å±•SeaKRåˆ°å¤šæ¨¡æ€"
- è®ºæ–‡Methodéƒ¨åˆ†æ— æ³•å†™"åŸºäºSeaKRçš„æ–‡æœ¬ä¸ç¡®å®šæ€§"

**ä¿®å¤éš¾åº¦**: ğŸ”´ é«˜
- éœ€è¦å®ç°kæ¬¡é‡‡æ ·æœºåˆ¶
- éœ€è¦ä»Qwen3-VLæå–hidden states
- æˆ–ä½¿ç”¨ç®€åŒ–ç‰ˆï¼ˆå•æ¬¡ç”Ÿæˆçš„hidden statesï¼‰

---

### é—®é¢˜2: è§†è§‰ä¸ç¡®å®šæ€§æ–¹æ³•ä¸æ–‡æ¡£ä¸ç¬¦

**æ–‡æ¡£è¦æ±‚** (åˆ›æ–°ç‚¹1 Line 820):
```python
# åº”è¯¥æ˜¯ï¼šåŸºäºæ³¨æ„åŠ›åˆ†å¸ƒçš„æ–¹å·®
visual_uncertainty = var(attention_weights)
```

**å½“å‰å®ç°** (uncertainty_estimator.py Line 236-272):
```python
# å®é™…æ˜¯ï¼šCLIPç‰¹å¾ç»Ÿè®¡
clip_features = self.clip_model.encode_image(image)
feature_norm = torch.norm(clip_features, p=2, dim=-1)
feature_std = torch.std(clip_features, dim=-1)
feature_mean = torch.mean(torch.abs(clip_features), dim=-1)

visual_unc = 0.4 * feature_norm + 0.3 * feature_std + 0.3 * feature_mean
```

**é—®é¢˜**:
1. ä¸æ˜¯attention variance
2. ç¼ºä¹ç†è®ºä¾æ®ï¼ˆä¸ºä»€ä¹ˆè¿™æ ·ç»„åˆï¼Ÿï¼‰
3. æƒé‡(0.4, 0.3, 0.3)æ˜¯ç»éªŒå€¼è¿˜æ˜¯è°ƒå‚ç»“æœï¼Ÿ

**ä¿®å¤æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆA: ä½¿ç”¨MLLMçš„attention weights
def estimate_visual_uncertainty_v2(self, image, hidden_states):
    # ä»Qwen3-VLæå–cross-attention weights
    attention_weights = self.mllm_model.get_cross_attention(image)

    # è®¡ç®—æ–¹å·®
    visual_unc = torch.var(attention_weights)

    return visual_unc

# æ–¹æ¡ˆB: åœ¨è®ºæ–‡ä¸­è®ºè¯CLIPç‰¹å¾ç»Ÿè®¡çš„åˆç†æ€§
# éœ€è¦å¼•ç”¨ç›¸å…³å·¥ä½œï¼Œè¯´æ˜ä¸ºä»€ä¹ˆCLIPç‰¹å¾ç»Ÿè®¡å¯ä»¥åæ˜ è§†è§‰ä¸ç¡®å®šæ€§
```

---

### é—®é¢˜3: åˆ›æ–°ç‚¹1å’Œ2ç¼ºå°‘å…³è”

**æ–‡æ¡£è¦æ±‚** (å¯¼å¸ˆæ„è§ç‰ˆ Line 26-29):
```
2. ä¸ç¡®å®šæ€§é©±åŠ¨çš„ä½ç½®æ„ŸçŸ¥èåˆ
   - ä¸ç¡®å®šæ€§è°ƒåˆ¶çš„ä½ç½®æƒé‡ï¼ˆè€Œéç®€å•å€Ÿé‰´VisRAGï¼‰
```

**å½“å‰å®ç°**:
```python
# position_aware_fusion.py Line 293-314
def _get_u_shaped_weights(self, seq_len: int):
    weights = torch.zeros(seq_len)

    for i in range(seq_len):
        if i < seq_len // 3:
            weights[i] = 1.0  # å›ºå®šå€¼ï¼
        elif i > 2 * seq_len // 3:
            weights[i] = 0.9  # å›ºå®šå€¼ï¼
        else:
            weights[i] = 0.6  # å›ºå®šå€¼ï¼

    return weights
```

**é—®é¢˜**: æƒé‡æ˜¯å›ºå®šçš„ï¼Œæ²¡æœ‰ä½¿ç”¨uncertaintyè¿›è¡Œè°ƒåˆ¶

**ä¿®å¤æ–¹æ¡ˆ**:
```python
def _compute_position_weights(self, tokens, positions, uncertainty_scores=None):
    # åŸºç¡€Uå‹æƒé‡
    base_weights = self._get_u_shaped_weights(seq_len)

    # ä¸ç¡®å®šæ€§è°ƒåˆ¶ï¼ˆæ–°å¢ï¼ï¼‰
    if uncertainty_scores is not None:
        total_unc = uncertainty_scores.get('total', 0.5)

        # é«˜ä¸ç¡®å®šæ€§ â†’ å¢å¼ºä½ç½®åå·®ç¼“è§£
        # ä½ä¸ç¡®å®šæ€§ â†’ ä¿æŒåŸåºï¼ˆä¿¡ä»»æ£€ç´¢å™¨æ’åºï¼‰
        modulation_factor = 1.0 + (total_unc - 0.5) * 0.5

        # è°ƒåˆ¶æƒé‡
        weights = base_weights * modulation_factor

        # å½’ä¸€åŒ–
        weights = weights / weights.sum()
    else:
        weights = base_weights

    return weights
```

**ç†è®ºä¾æ®**:
- é«˜ä¸ç¡®å®šæ€§ â†’ æ¨¡å‹ä¸ç¡®å®š â†’ æ›´éœ€è¦ç¼“è§£ä½ç½®åå·®
- ä½ä¸ç¡®å®šæ€§ â†’ æ¨¡å‹æœ‰ä¿¡å¿ƒ â†’ ä¿æŒæ£€ç´¢å™¨åŸåº

---

## ğŸ“Š å®ç°å®Œæˆåº¦è¯¦ç»†è¯„åˆ†

### åˆ›æ–°ç‚¹1: è·¨æ¨¡æ€ä¸ç¡®å®šæ€§ä¼°è®¡

| å­æ¨¡å— | æ–‡æ¡£è¦æ±‚ | SeaKRå®ç° | æˆ‘ä»¬çš„å®ç° | å®Œæˆåº¦ | é—®é¢˜ |
|--------|---------|-----------|-----------|--------|------|
| **æ–‡æœ¬ä¸ç¡®å®šæ€§** | | | | | |
| - eigen_scoreå…¬å¼ | âœ… | âœ… Line 738-744 | âœ… Line 494-539 | 100% | å…¬å¼æ­£ç¡® |
| - kæ¬¡é‡‡æ · | âœ… | âœ… n=20 | âŒ æ—  | 0% | ğŸ”´ ç¼ºå¤± |
| - EOS embeddings | âœ… | âœ… | âŒ æ—  | 0% | ğŸ”´ ç¼ºå¤± |
| - æƒé‡å¯ç”¨ | âœ… | âœ… | âŒ 0.0 | 0% | ğŸ”´ è¢«ç¦ç”¨ |
| **è§†è§‰ä¸ç¡®å®šæ€§** | | | | | |
| - Attention variance | âœ… | N/A | âŒ ç”¨CLIP | 0% | ğŸ”´ æ–¹æ³•ä¸ç¬¦ |
| - CLIPç‰¹å¾ç»Ÿè®¡ | âŒ | N/A | âœ… | 50% | âš ï¸ ç¼ºç†è®º |
| **å¯¹é½ä¸ç¡®å®šæ€§** | | | | | |
| - JSæ•£åº¦å…¬å¼ | âœ… | N/A | âœ… Line 493-525 | 90% | âœ… æ­£ç¡® |
| - CLIPåˆ†å¸ƒ | âœ… | N/A | âœ… | 80% | âœ… åŸºæœ¬æ­£ç¡® |

**æ€»ä½“**: 40% - **ä¸¥é‡ä¸è¶³**

---

### åˆ›æ–°ç‚¹2: ä½ç½®æ„ŸçŸ¥èåˆ

| å­æ¨¡å— | æ–‡æ¡£è¦æ±‚ | VisRAGå®ç° | æˆ‘ä»¬çš„å®ç° | å®Œæˆåº¦ | é—®é¢˜ |
|--------|---------|-----------|-----------|--------|------|
| **Uå‹æƒé‡** | | | | | |
| - Lost in the Middle | âœ… | âœ… | âœ… Line 293-314 | 80% | âœ… å®ç° |
| - æƒé‡åˆ†å¸ƒ | âœ… | âœ… | âœ… | 80% | âœ… æ­£ç¡® |
| **ä¸ç¡®å®šæ€§è°ƒåˆ¶** | | | | | |
| - åŠ¨æ€æƒé‡ | âœ… | âŒ | âŒ | 0% | ğŸ”´ ç¼ºå¤± |
| - ä¸åˆ›æ–°1å…³è” | âœ… | âŒ | âŒ | 0% | ğŸ”´ ç¼ºå¤± |
| **åŒå‘æ³¨æ„åŠ›** | | | | | |
| - Textâ†’Visual | âœ… | âŒ | âœ… Line 152-207 | 90% | âœ… å®ç° |
| - Visualâ†’Text | âœ… | âŒ | âœ… | 90% | âœ… å®ç° |

**æ€»ä½“**: 55% - **ç¼ºå°‘æ ¸å¿ƒå…³è”**

---

## ğŸ¯ ä¿®å¤ä¼˜å…ˆçº§å’Œå·¥ä½œé‡ä¼°ç®—

### P0 - å¿…é¡»ä¿®å¤ï¼ˆå¦åˆ™æ— æ³•å‘è¡¨ï¼‰

| é—®é¢˜ | å·¥ä½œé‡ | éš¾åº¦ | å½±å“ |
|------|--------|------|------|
| 1. å¯ç”¨æ–‡æœ¬ä¸ç¡®å®šæ€§ | 2-3å¤© | ğŸ”´ é«˜ | æ ¸å¿ƒåˆ›æ–° |
| 2. å®ç°ä¸ç¡®å®šæ€§è°ƒåˆ¶ | 1å¤© | ğŸŸ¡ ä¸­ | åˆ›æ–°å…³è” |
| 3. ä¿®å¤è§†è§‰ä¸ç¡®å®šæ€§ | 2å¤© | ğŸŸ¡ ä¸­ | æ–¹æ³•ä¸€è‡´æ€§ |

**æ€»å·¥ä½œé‡**: 5-6å¤©

---

### P1 - å¼ºçƒˆå»ºè®®ï¼ˆæå‡è®ºæ–‡è´¨é‡ï¼‰

| é—®é¢˜ | å·¥ä½œé‡ | éš¾åº¦ | å½±å“ |
|------|--------|------|------|
| 4. å®ç°kæ¬¡é‡‡æ · | 3-4å¤© | ğŸ”´ é«˜ | SeaKRå®Œæ•´æ€§ |
| 5. æå–hidden states | 2å¤© | ğŸŸ¡ ä¸­ | ç‰¹å¾è´¨é‡ |
| 6. ç†è®ºè®ºè¯ | 1-2å¤© | ğŸŸ¢ ä½ | è®ºæ–‡æ·±åº¦ |

**æ€»å·¥ä½œé‡**: 6-8å¤©

---

## ğŸ“ å…·ä½“ä¿®å¤ä»£ç å»ºè®®

### ä¿®å¤1: å¯ç”¨æ–‡æœ¬ä¸ç¡®å®šæ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰

```python
# uncertainty_estimator.py

def __init__(self, mllm_model=None, config=None):
    # ä¿®æ”¹æƒé‡é…ç½®
    self.alpha = self.config.get('text_weight', 0.4)  # 0.0 â†’ 0.4
    self.beta = self.config.get('visual_weight', 0.3)  # 0.5 â†’ 0.3
    self.gamma = self.config.get('alignment_weight', 0.3)  # 0.5 â†’ 0.3

def estimate_text_uncertainty(self, text, hidden_states=None):
    """
    ç®€åŒ–ç‰ˆï¼šä½¿ç”¨å•æ¬¡ç”Ÿæˆçš„hidden states
    """
    if hidden_states is None:
        # ä»MLLMè·å–hidden states
        hidden_states = self.mllm_model.get_hidden_states(text)

    # ä½¿ç”¨æœ€åä¸€å±‚çš„hidden states
    last_hidden = hidden_states[-1]  # [seq_len, hidden_dim]

    # è®¡ç®—GramçŸ©é˜µçš„ç‰¹å¾å€¼åˆ†å¸ƒ
    gram_matrix = self._compute_gram_matrix(last_hidden)
    eigenvalues = torch.linalg.eigvalsh(gram_matrix)

    # ç‰¹å¾å€¼çš„æ–¹å·®ä½œä¸ºä¸ç¡®å®šæ€§
    text_unc = torch.var(eigenvalues).item()

    # å½’ä¸€åŒ–åˆ°[0, 1]
    text_unc = min(max(text_unc, 0.0), 1.0)

    return text_unc
```

---

### ä¿®å¤2: å®ç°ä¸ç¡®å®šæ€§è°ƒåˆ¶

```python
# position_aware_fusion.py

def position_weighted_pooling(self,
                              multimodal_tokens,
                              positions=None,
                              modality_types=None,
                              uncertainty_scores=None):  # æ–°å¢å‚æ•°
    """
    ä½ç½®åŠ æƒæ± åŒ–ï¼ˆä¸ç¡®å®šæ€§è°ƒåˆ¶ç‰ˆï¼‰
    """
    # è®¡ç®—ä½ç½®æƒé‡ï¼ˆå¸¦ä¸ç¡®å®šæ€§è°ƒåˆ¶ï¼‰
    position_weights = self._compute_position_weights(
        tokens=multimodal_tokens,
        positions=positions,
        modality_types=modality_types,
        uncertainty_scores=uncertainty_scores  # ä¼ å…¥ä¸ç¡®å®šæ€§
    )

    # åŠ æƒæ± åŒ–
    weighted_features = multimodal_tokens * position_weights.unsqueeze(-1)

    return weighted_features

def _compute_position_weights(self, tokens, positions, modality_types=None,
                              uncertainty_scores=None):
    """
    è®¡ç®—ä½ç½®æƒé‡ï¼ˆä¸ç¡®å®šæ€§è°ƒåˆ¶ï¼‰
    """
    batch_size, seq_len, _ = tokens.shape

    # åŸºç¡€Uå‹æƒé‡
    base_weights = self._get_u_shaped_weights(seq_len)
    base_weights = base_weights.to(tokens.device)

    # ä¸ç¡®å®šæ€§è°ƒåˆ¶ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼ï¼‰
    if uncertainty_scores is not None:
        total_unc = uncertainty_scores.get('total', 0.5)

        # è°ƒåˆ¶å› å­ï¼šä¸ç¡®å®šæ€§è¶Šé«˜ï¼Œä½ç½®åå·®ç¼“è§£è¶Šå¼º
        # total_unc âˆˆ [0, 1]
        # modulation âˆˆ [0.75, 1.25]
        modulation = 1.0 + (total_unc - 0.5) * 0.5

        # åº”ç”¨è°ƒåˆ¶
        weights = base_weights * modulation

        # å½’ä¸€åŒ–
        weights = weights / weights.sum()
    else:
        weights = base_weights

    # æ‰©å±•åˆ°batchç»´åº¦
    weights = weights.unsqueeze(0).expand(batch_size, -1)

    # æ¨¡æ€æƒé‡ï¼ˆå¯é€‰ï¼‰
    if modality_types is not None:
        modality_weights = self._get_modality_weights(modality_types)
        modality_weights = modality_weights.to(tokens.device).unsqueeze(0)
        weights = weights * modality_weights

    return weights
```

---

### ä¿®å¤3: ä¿®å¤è§†è§‰ä¸ç¡®å®šæ€§

```python
# uncertainty_estimator.py

def estimate_visual_uncertainty(self, image, attention_weights=None):
    """
    è§†è§‰ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆä¿®å¤ç‰ˆï¼‰

    æ–¹æ³•ï¼šä½¿ç”¨MLLMçš„cross-attention weightsçš„æ–¹å·®
    """
    if attention_weights is not None:
        # æ–¹æ³•1: ä½¿ç”¨æä¾›çš„attention weights
        visual_unc = torch.var(attention_weights).item()

    elif self.mllm_model is not None:
        # æ–¹æ³•2: ä»MLLMæå–attention weights
        try:
            attn_weights = self.mllm_model.get_cross_attention_weights(image)
            visual_unc = torch.var(attn_weights).item()
        except:
            # Fallback: ä½¿ç”¨CLIPç‰¹å¾ç»Ÿè®¡
            visual_unc = self._estimate_visual_uncertainty_clip(image)

    else:
        # æ–¹æ³•3: ä½¿ç”¨CLIPç‰¹å¾ç»Ÿè®¡ï¼ˆfallbackï¼‰
        visual_unc = self._estimate_visual_uncertainty_clip(image)

    # å½’ä¸€åŒ–åˆ°[0, 1]
    visual_unc = min(max(visual_unc, 0.0), 1.0)

    return visual_unc

def _estimate_visual_uncertainty_clip(self, image):
    """
    ä½¿ç”¨CLIPç‰¹å¾ç»Ÿè®¡ï¼ˆfallbackæ–¹æ³•ï¼‰

    ç†è®ºä¾æ®ï¼š
    - ç‰¹å¾èŒƒæ•°å¤§ â†’ å›¾åƒä¿¡æ¯ä¸°å¯Œ â†’ ä¸ç¡®å®šæ€§ä½
    - ç‰¹å¾æ ‡å‡†å·®å¤§ â†’ ç‰¹å¾åˆ†æ•£ â†’ ä¸ç¡®å®šæ€§é«˜
    """
    clip_features = self.clip_model.encode_image(image)

    # ç‰¹å¾èŒƒæ•°ï¼ˆå½’ä¸€åŒ–ï¼‰
    feature_norm = torch.norm(clip_features, p=2, dim=-1)
    norm_score = 1.0 - (feature_norm / feature_norm.max())

    # ç‰¹å¾æ ‡å‡†å·®
    feature_std = torch.std(clip_features, dim=-1)
    std_score = feature_std / (feature_std.max() + 1e-10)

    # ç»„åˆï¼ˆç†è®ºæƒé‡ï¼‰
    visual_unc = 0.6 * std_score + 0.4 * norm_score

    return visual_unc.item()
```

---

## ğŸ“‹ è®ºæ–‡æ’°å†™å»ºè®®ï¼ˆMethodéƒ¨åˆ†ï¼‰

### 3.1 Cross-Modal Uncertainty Estimation

**éœ€è¦æ˜ç¡®è¯´æ˜çš„å†…å®¹**:

1. **æ–‡æœ¬ä¸ç¡®å®šæ€§**:
   ```
   æˆ‘ä»¬æ‰©å±•SeaKR (Shi et al., 2024)çš„ä¸ç¡®å®šæ€§ä¼°è®¡åˆ°å¤šæ¨¡æ€åœºæ™¯ã€‚

   [å¦‚æœä½¿ç”¨ç®€åŒ–ç‰ˆ]
   ç”±äºè®¡ç®—æ•ˆç‡è€ƒè™‘ï¼Œæˆ‘ä»¬ä½¿ç”¨å•æ¬¡ç”Ÿæˆçš„hidden statesè®¡ç®—GramçŸ©é˜µï¼Œ
   è€ŒéSeaKRçš„k=20æ¬¡é‡‡æ ·ã€‚å®éªŒè¡¨æ˜è¿™ç§ç®€åŒ–åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸‹ä»ç„¶æœ‰æ•ˆã€‚

   [å¦‚æœå®ç°å®Œæ•´ç‰ˆ]
   æˆ‘ä»¬é‡‡ç”¨SeaKRçš„å®Œæ•´é‡‡æ ·æœºåˆ¶ï¼Œç”Ÿæˆk=20ä¸ªæ ·æœ¬å¹¶è®¡ç®—eigen_scoreã€‚
   ```

2. **è§†è§‰ä¸ç¡®å®šæ€§**:
   ```
   [å¦‚æœä½¿ç”¨attention variance]
   æˆ‘ä»¬è®¡ç®—MLLM cross-attention weightsçš„æ–¹å·®ä½œä¸ºè§†è§‰ä¸ç¡®å®šæ€§ã€‚
   ç›´è§‰ä¸Šï¼Œattentionåˆ†å¸ƒè¶Šåˆ†æ•£ï¼Œæ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„ç†è§£è¶Šä¸ç¡®å®šã€‚

   [å¦‚æœä½¿ç”¨CLIPç‰¹å¾]
   æˆ‘ä»¬ä½¿ç”¨CLIPç‰¹å¾ç»Ÿè®¡ä½œä¸ºè§†è§‰ä¸ç¡®å®šæ€§çš„ä»£ç†æŒ‡æ ‡ã€‚
   ç‰¹å¾èŒƒæ•°åæ˜ ä¿¡æ¯ä¸°å¯Œåº¦ï¼Œæ ‡å‡†å·®åæ˜ ç‰¹å¾åˆ†æ•£ç¨‹åº¦ã€‚
   ```

3. **ä¸ç¡®å®šæ€§è°ƒåˆ¶çš„ä½ç½®æƒé‡**:
   ```
   æˆ‘ä»¬æå‡ºä¸ç¡®å®šæ€§é©±åŠ¨çš„ä½ç½®æƒé‡è°ƒåˆ¶æœºåˆ¶ã€‚
   å½“æ¨¡å‹ä¸ç¡®å®šæ€§é«˜æ—¶ï¼Œå¢å¼ºä½ç½®åå·®ç¼“è§£ï¼›
   å½“æ¨¡å‹æœ‰ä¿¡å¿ƒæ—¶ï¼Œä¿æŒæ£€ç´¢å™¨åŸåºã€‚

   å…¬å¼ï¼šw_i = base_w_i Ã— (1 + (U_total - 0.5) Ã— Î±)
   å…¶ä¸­Î±æ˜¯è°ƒåˆ¶å¼ºåº¦è¶…å‚æ•°ã€‚
   ```

---

**æœ€ç»ˆå®¡æŸ¥ç»“è®º**: ğŸ”´ **éœ€è¦ç«‹å³ä¿®å¤3ä¸ªP0é—®é¢˜ï¼Œå¦åˆ™æ— æ³•æ”¯æ’‘è®ºæ–‡æ ¸å¿ƒåˆ›æ–°å£°æ˜ã€‚å»ºè®®ç”¨5-6å¤©å®Œæˆä¿®å¤ï¼Œç„¶åé‡æ–°è¿è¡Œå®éªŒéªŒè¯ã€‚**

